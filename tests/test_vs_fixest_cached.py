"""
Refactored test_vs_fixest using cached R results for improved performance.

This module tests pyfixest against pre-computed R fixest results stored in CSV files.
Results are generated by the R script in tests/scripts/generate_r_results.R.
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd
import pytest

# Add config directory to path
config_dir = Path(__file__).parent / "config"
sys.path.insert(0, str(config_dir))

from config_loader import (
    CachedResultsLoader,
    TestConfigLoader,
    inference_to_string,
    weights_to_string,
)

import pyfixest as pf
from pyfixest.utils.utils import get_data, ssc

# Global config and cache loaders
config_loader = TestConfigLoader()
cache_loader = CachedResultsLoader()


def check_absolute_diff(x1, x2, tol, msg=None):
    """Check for absolute differences between Python and R results."""
    if isinstance(x1, (int, float)):
        x1 = np.array([x1])
    if isinstance(x2, (int, float)):
        x2 = np.array([x2])

    msg = "" if msg is None else msg

    # Handle nan values
    nan_mask_x1 = np.isnan(x1)
    nan_mask_x2 = np.isnan(x2)

    # Check if NaN patterns match
    assert np.array_equal(nan_mask_x1, nan_mask_x2), f"NaN patterns don't match: {msg}"

    # For non-NaN values, check absolute difference
    non_nan_mask = ~(nan_mask_x1 | nan_mask_x2)
    if np.any(non_nan_mask):
        diff = np.abs(x1[non_nan_mask] - x2[non_nan_mask])
        max_diff = np.max(diff)
        assert max_diff <= tol, f"Max difference {max_diff} > tolerance {tol}: {msg}"


def get_python_results(
    formula, data, inference, weights, dropna, test_type, family=None
):
    """Run pyfixest and extract results in standardized format."""
    # Prepare data
    if dropna:
        data = data.dropna()

    # Get estimation settings
    est_settings = config_loader.get_estimation_settings()
    ssc_settings = est_settings["ssc_settings"]
    ssc_ = ssc(adj=ssc_settings["adj"], cluster_adj=ssc_settings["cluster_adj"])

    try:
        # Fit model based on test type
        if test_type == "feols":
            mod = pf.feols(
                fml=formula, data=data, vcov=inference, weights=weights, ssc=ssc_
            )
        elif test_type == "iv":
            mod = pf.feols(  # IV is also handled by feols
                fml=formula, data=data, vcov=inference, weights=weights, ssc=ssc_
            )
        elif test_type == "glm":
            mod = pf.feglm(fml=formula, data=data, family=family, vcov=inference)
        elif test_type == "fepois":
            mod = pf.fepois(
                fml=formula, data=data, vcov=inference, weights=weights, ssc=ssc_
            )
        else:
            raise ValueError(f"Unknown test type: {test_type}")

        # Extract results - focus on X1 coefficient
        try:
            py_coef = mod.coef().xs("X1")
            py_se = mod.se().xs("X1")
            py_pval = mod.pvalue().xs("X1")
            py_tstat = mod.tstat().xs("X1")
            py_confint = mod.confint().xs("X1").values
            py_vcov = mod._vcov[0, 0]
        except KeyError:
            # X1 not found in results - return NaN
            return {
                "coef": np.nan,
                "se": np.nan,
                "pvalue": np.nan,
                "tstat": np.nan,
                "confint_low": np.nan,
                "confint_high": np.nan,
                "vcov_00": np.nan,
                "nobs": mod._N if hasattr(mod, "_N") else np.nan,
                "n_coefs": np.nan,
                "dof_k": mod._dof_k if hasattr(mod, "_dof_k") else np.nan,
                "df_t": mod._df_t if hasattr(mod, "_df_t") else np.nan,
                "resid_1": np.nan,
                "resid_2": np.nan,
                "resid_3": np.nan,
                "resid_4": np.nan,
                "resid_5": np.nan,
                "predict_1": np.nan,
                "predict_2": np.nan,
                "predict_3": np.nan,
                "predict_4": np.nan,
                "predict_5": np.nan,
            }

        # Get additional results
        py_nobs = mod._N
        py_n_coefs = mod.coef().values.size
        py_dof_k = mod._dof_k
        py_df_t = mod._df_t

        # Get residuals and predictions (first 5 values)
        py_resid = mod.resid()
        py_predict = mod.predict()

        resid_first_5 = (
            py_resid[:5]
            if len(py_resid) >= 5
            else np.concatenate([py_resid, [np.nan] * (5 - len(py_resid))])
        )
        predict_first_5 = (
            py_predict[:5]
            if len(py_predict) >= 5
            else np.concatenate([py_predict, [np.nan] * (5 - len(py_predict))])
        )

        return {
            "coef": py_coef,
            "se": py_se,
            "pvalue": py_pval,
            "tstat": py_tstat,
            "confint_low": py_confint[0],
            "confint_high": py_confint[1],
            "vcov_00": py_vcov,
            "nobs": py_nobs,
            "n_coefs": py_n_coefs,
            "dof_k": py_dof_k,
            "df_t": py_df_t,
            "resid_1": resid_first_5[0],
            "resid_2": resid_first_5[1],
            "resid_3": resid_first_5[2],
            "resid_4": resid_first_5[3],
            "resid_5": resid_first_5[4],
            "predict_1": predict_first_5[0],
            "predict_2": predict_first_5[1],
            "predict_3": predict_first_5[2],
            "predict_4": predict_first_5[3],
            "predict_5": predict_first_5[4],
        }

    except Exception:
        # Return NaN results if fitting fails
        return {
            "coef": np.nan,
            "se": np.nan,
            "pvalue": np.nan,
            "tstat": np.nan,
            "confint_low": np.nan,
            "confint_high": np.nan,
            "vcov_00": np.nan,
            "nobs": np.nan,
            "n_coefs": np.nan,
            "dof_k": np.nan,
            "df_t": np.nan,
            "resid_1": np.nan,
            "resid_2": np.nan,
            "resid_3": np.nan,
            "resid_4": np.nan,
            "resid_5": np.nan,
            "predict_1": np.nan,
            "predict_2": np.nan,
            "predict_3": np.nan,
            "predict_4": np.nan,
            "predict_5": np.nan,
        }


# Test fixtures
@pytest.fixture(scope="module")
def data_feols():
    """Generate data for FEOLS tests."""
    params = config_loader.get_data_params("feols")
    return get_data(N=params["N"], seed=params["seed"], model=params["model"])


@pytest.fixture(scope="module")
def data_fepois():
    """Generate data for FEPOIS tests."""
    params = config_loader.get_data_params("fepois")
    return get_data(N=params["N"], seed=params["seed"], model=params["model"])


@pytest.fixture(scope="module")
def data_glm():
    """Generate data for GLM tests."""
    params = config_loader.get_data_params("glm")
    data = get_data(N=params["N"], seed=params["seed"])
    # Convert Y to binary for GLM
    data["Y"] = np.where(data["Y"] > 0, 1, 0)
    return data


# FEOLS Tests
@pytest.mark.against_r_core
@pytest.mark.parametrize("dropna", [False, True])
@pytest.mark.parametrize("inference", ["iid", "hetero", {"CRV1": "group_id"}])
@pytest.mark.parametrize("weights", [None, "weights"])
def test_feols_cached(data_feols, dropna, inference, weights, formula="Y~X1"):
    """Test FEOLS against cached R results."""
    # Get tolerance settings
    tolerance = config_loader.get_tolerance("default")

    # Get Python results
    py_results = get_python_results(
        formula, data_feols, inference, weights, dropna, "feols"
    )

    # Get cached R results
    inference_str = inference_to_string(inference)
    weights_str = weights_to_string(weights)

    try:
        r_results = cache_loader.get_result(
            "feols", formula, inference_str, weights_str, dropna
        )
    except KeyError:
        pytest.skip(
            f"No cached result found for: {formula}, {inference_str}, {weights_str}, {dropna}"
        )

    # Compare core results
    core_fields = [
        "coef",
        "se",
        "pvalue",
        "tstat",
        "confint_low",
        "confint_high",
        "vcov_00",
        "nobs",
        "n_coefs",
        "dof_k",
        "df_t",
    ]

    for field in core_fields:
        if field in r_results.index and not pd.isna(r_results[field]):
            check_absolute_diff(
                py_results[field],
                r_results[field],
                tolerance["atol"],
                f"py_{field} != r_{field}",
            )

    # Compare residuals and predictions for basic cases
    if inference == "iid" and weights is None and not dropna:
        resid_fields = ["resid_1", "resid_2", "resid_3", "resid_4", "resid_5"]
        predict_fields = [
            "predict_1",
            "predict_2",
            "predict_3",
            "predict_4",
            "predict_5",
        ]

        for field in resid_fields + predict_fields:
            if field in r_results.index and not pd.isna(r_results[field]):
                check_absolute_diff(
                    py_results[field],
                    r_results[field],
                    tolerance["atol"],
                    f"py_{field} != r_{field}",
                )


# Parameterized tests for all formulas
def create_feols_tests():
    """Create parameterized tests for all FEOLS formulas."""
    formulas = config_loader.get_formulas("feols")
    inference_types = config_loader.get_inference_types("feols")
    weights_options = config_loader.get_weights_options("feols")
    dropna_options = config_loader.get_dropna_options("feols")

    # Create test parameters
    test_params = []
    for formula in formulas:
        for inference in inference_types:
            for weights in weights_options:
                for dropna in dropna_options:
                    test_params.append((formula, inference, weights, dropna))

    return test_params


# Generate parameterized test for all FEOLS formulas
@pytest.mark.against_r_core
@pytest.mark.parametrize("formula,inference,weights,dropna", create_feols_tests())
def test_all_feols_formulas(data_feols, formula, inference, weights, dropna):
    """Test all FEOLS formulas against cached R results."""
    test_feols_cached(data_feols, dropna, inference, weights, formula)


# IV Tests
def create_iv_tests():
    """Create parameterized tests for all IV formulas."""
    formulas = config_loader.get_formulas("iv")
    inference_types = config_loader.get_inference_types("iv")
    weights_options = config_loader.get_weights_options("iv")
    dropna_options = config_loader.get_dropna_options("iv")

    test_params = []
    for formula in formulas:
        for inference in inference_types:
            for weights in weights_options:
                for dropna in dropna_options:
                    test_params.append((formula, inference, weights, dropna))

    return test_params


@pytest.mark.against_r_core
@pytest.mark.parametrize("formula,inference,weights,dropna", create_iv_tests())
def test_all_iv_formulas(data_feols, formula, inference, weights, dropna):
    """Test all IV formulas against cached R results."""
    # Get tolerance settings
    tolerance = config_loader.get_tolerance("default")

    # Get Python results
    py_results = get_python_results(
        formula, data_feols, inference, weights, dropna, "iv"
    )

    # Get cached R results
    inference_str = inference_to_string(inference)
    weights_str = weights_to_string(weights)

    try:
        r_results = cache_loader.get_result(
            "iv", formula, inference_str, weights_str, dropna
        )
    except KeyError:
        pytest.skip(
            f"No cached result found for: {formula}, {inference_str}, {weights_str}, {dropna}"
        )

    # Compare results
    core_fields = ["coef", "se", "pvalue", "tstat", "confint_low", "confint_high"]

    for field in core_fields:
        if field in r_results.index and not pd.isna(r_results[field]):
            check_absolute_diff(
                py_results[field],
                r_results[field],
                tolerance["atol"],
                f"py_{field} != r_{field}",
            )


# GLM Tests
def create_glm_tests():
    """Create parameterized tests for all GLM formulas."""
    formulas = config_loader.get_formulas("glm")
    families = config_loader.get_test_config("glm")["families"]
    inference_types = config_loader.get_inference_types("glm")
    dropna_options = config_loader.get_dropna_options("glm")

    test_params = []
    for formula in formulas:
        for family in families:
            for inference in inference_types:
                for dropna in dropna_options:
                    test_params.append((formula, family, inference, dropna))

    return test_params


@pytest.mark.against_r_core
@pytest.mark.parametrize("formula,family,inference,dropna", create_glm_tests())
def test_all_glm_formulas(data_glm, formula, family, inference, dropna):
    """Test all GLM formulas against cached R results."""
    # Get tolerance settings
    tolerance = config_loader.get_tolerance("glm")

    # Get Python results
    py_results = get_python_results(
        formula, data_glm, inference, None, dropna, "glm", family
    )

    # Get cached R results
    inference_str = inference_to_string(inference)

    try:
        r_results = cache_loader.get_result(
            "glm", formula, inference_str, "none", dropna, family
        )
    except KeyError:
        pytest.skip(
            f"No cached result found for: {formula}, {family}, {inference_str}, dropna={dropna}"
        )

    # Compare results (only coefficients for GLM tests as in original)
    if inference == "iid":
        check_absolute_diff(
            py_results["coef"],
            r_results["coef"],
            tolerance["atol"],
            f"py_coef != r_coef for {formula} {family}",
        )


# FEPOIS Tests
def create_fepois_tests():
    """Create parameterized tests for all FEPOIS formulas."""
    formulas = config_loader.get_formulas("fepois")
    inference_types = config_loader.get_inference_types("fepois")
    weights_options = config_loader.get_weights_options("fepois")
    dropna_options = config_loader.get_dropna_options("fepois")

    test_params = []
    for formula in formulas:
        for inference in inference_types:
            for weights in weights_options:
                for dropna in dropna_options:
                    test_params.append((formula, inference, weights, dropna))

    return test_params


@pytest.mark.against_r_core
@pytest.mark.parametrize("formula,inference,weights,dropna", create_fepois_tests())
def test_all_fepois_formulas(data_fepois, formula, inference, weights, dropna):
    """Test all FEPOIS formulas against cached R results."""
    # Get tolerance settings (use fepois-specific tolerances)
    tolerance = config_loader.get_tolerance("fepois")
    rtol = tolerance["crv_rtol"] if isinstance(inference, dict) else tolerance["rtol"]
    atol = tolerance["crv_atol"] if isinstance(inference, dict) else tolerance["atol"]

    # Get Python results
    py_results = get_python_results(
        formula, data_fepois, inference, weights, dropna, "fepois"
    )

    # Get cached R results
    inference_str = inference_to_string(inference)
    weights_str = weights_to_string(weights)

    try:
        r_results = cache_loader.get_result(
            "fepois", formula, inference_str, weights_str, dropna
        )
    except KeyError:
        pytest.skip(
            f"No cached result found for: {formula}, {inference_str}, {weights_str}, {dropna}"
        )

    # Compare results
    core_fields = ["coef", "se", "pvalue", "tstat"]

    for field in core_fields:
        if field in r_results.index and not pd.isna(r_results[field]):
            check_absolute_diff(
                py_results[field], r_results[field], atol, f"py_{field} != r_{field}"
            )


# Cache validation test
def test_cache_validity():
    """Test that cached results are available and valid."""
    assert cache_loader.is_cache_valid(), (
        "Cached results not found. Run tests/scripts/generate_r_results.R first."
    )

    available_types = cache_loader.list_available_test_types()
    expected_types = {"feols", "iv", "glm", "fepois"}

    missing_types = expected_types - set(available_types)
    assert not missing_types, f"Missing cached result types: {missing_types}"

    # Check metadata
    metadata = cache_loader.get_metadata()
    assert len(metadata) > 0, "Metadata is empty"

    print(f"âœ“ Cache validation passed. Available types: {available_types}")
