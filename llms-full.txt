# acknowledgements.html

# Acknowledgements

Like many open-source software projects, PyFixest builds on work and ideas first developed in other packages. In this section, we want to acknowledge and express our appreciation for the authors of these packages and their creativity and hard work.

# Software

> Unless explicitly stated otherwise, all PyFixest code is written independently from scratch. The packages listed below have influenced PyFixest's API design or algorithmic choices, or are used for testing PyFixest as reference implementations, but no source code has been copied except where explicitly stated (with license and permission details provided inline).

## fixest (R)

If open source software is made by "standing on the shoulders of giants", in case of `PyFixest`, there is mostly one very big giant - [Laurent Bergé's](https://sites.google.com/site/laurentrberge/) formidable [fixest](https://github.com/lrberge/fixest/) R package. `fixest` is so good we decided to stick to its API and conventions as closely as Python allows when starting to work on a fixed effects regression package in Python. Without `fixest`, PyFixest likely wouldn't exist - or at the very least, it would look very different. Most importantly, `fixest` has shaped our understanding of how a user-friendly regression package should look like and what functionality it should offer.

More concretely, we have borrowed the following API conventions and ideas directly from fixest:

| Feature | What PyFixest borrows |
|---|---|
| **Formula syntax** | `feols()`, `fepois()`, `feglm()` function and argument names; the `i()` interaction operator; multiple estimation syntax via `sw()`, `sw0()`, `csw()`, `csw0()`; fixed effects interactions via `fe1^fe2` |
| **Multiple Estimation Optimizations** | The core idea that most of the work of fixed effects regression can be pooled / cached when estimating multiple models with the same fixed effects structure|
| **Demeaning / FWL** | The alternating-projections algorithm in PyFixest is a standalone implementation in numba/rust, but uses the same convergence criteria and default parameters |
| **Small-sample corrections** | The `ssc()` function to control small sample adjustments, and all of its defaults - `adj`, `fixef_K`, `cluster_adj`, `cluster_df` - mirror fixest exactly (see fixest's [standard errors vignette](https://cran.r-project.org/web/packages/fixest/vignettes/standard_errors.html)) |
| **Collinearity detection** | The algorithm is a Rust/Numba re-implementation of Laurent Berge's [C++ routine in fixest](https://github.com/lrberge/fixest/blob/a4d1a9bea20aa7ab7ab0e0f1d2047d8097971ad7/src/lm_related.cpp#L130), re-licensed under MIT with Laurent's permission |
| **Post-estimation** | `etable()`, `coefplot()`, `iplot()`, `coef()` etc mirror fixest's output and plotting functionality |
| **On the fly variance covariance adjustments** | As in `fixest`, you can adjust the vcov post estimation by calling a `vcov()` method on the results object (`Feols` in pyfixest and `fixest` in `fixest`) |
| **Predict method for fixed effects** | The `predict()`  and `fixef()` methods in PyFixest mirrors fixest's functionality for obtaining fitted values, fixed effects, and linear predictions |

You can learn more about fixest [on github](https://github.com/lrberge/fixest), via its [documentation](https://lrberge.github.io/fixest/), or by reading the [associated paper](https://arxiv.org/abs/2601.21749).

PyFixest is tested against fixest via **rpy2** to ensure numerical equivalence
(usually `rtol = 1e-08`, `atol = 1e-08`) for coefficients,
standard errors, t-statistics, p-values, confidence intervals, etc for OLS, IV, Poisson, and GLM models.

---

## By functionality

### Poisson regression

| Package | Language | Role |
|---|---|---|
| [**ppmlhdfe**](https://github.com/sergiocorreia/ppmlhdfe) | Stata |  PyFixest's Poisson estimator (`fepois`) implements the ppmlhdfe algorithm as described in Correia, Guimaraes & Zylkin (2020), including its separation detection and acceleration strategies. Test datasets for separation examples are taken from the ppmlhdfe repository (MIT license) |

### Instrumental variables

| Package | Language | Role |
|---|---|---|
| [**ivDiag**](https://yiqingxu.org/packages/ivDiag/) | R | The IV diagnostics implementations are validated against ivDiag by Lal et al. |

### Quantile regression

| Package | Language | Role |
|---|---|---|
| [**quantreg**](https://cran.r-project.org/package=quantreg) | R | PyFixest's `quantreg()` implementation is tested against R's quantreg package (by Roger Koenker) for coefficient and NID standard error equivalence. |
| [**qreg2**](https://ideas.repec.org/c/boc/bocode/s457369.html) | Stata | PyFixest's cluster-robust standard errors for quantile regression are tested against Stata's qreg2 output, which is based on work by Parente & Santos Silva (2016). |

### Difference-in-Differences

| Package | Language | Role |
|---|---|---|
| [**did2s**](https://github.com/kylebutts/did2s) | R | PyFixest's DID2S estimator's API is strongly inspired by Kyle Butts' R package (MIT license) and we have relied on Kyle's writeup of the method for our own implementation. Tests compare coefficients and standard errors against the R implementation. Additionally, PyFixest's `event_study()` function is inspired by the [`event_study()` function](https://kylebutts.github.io/did2s/articles/event_study.html) in `did2s`.  |
| [**lpdid**](https://github.com/alexCardazzi/lpdid) | R | PyFixest's local-projections DID estimator is highly influenced by Alex Cardazzi's R code (published under MIT) for the lpdid package. We also test against Alex' package. |
| [**lpdid**](https://github.com/danielegirardi/lpdid) | Stata | We also test our implementation against Daniel Busch and Daniele Girardi's Stata implementation of the local-projections estimator. |

### Panel data visualization

| Package | Language | Role |
|---|---|---|
| [**panelView**](https://yiqingxu.org/packages/panelView/) | R | PyFixest's `panelview()` function for visualizing treatment patterns and outcomes in panel data is inspired by the panelView R package by Mou, Liu and Xu.|

### Randomization inference

| Package | Language | Role |
|---|---|---|
| [**ritest**](https://github.com/grantmcdermott/ritest) | R | PyFixest's `ritest()` method's API heavily borrows from Grant McDermott's R package and is tested against it. |
| [**ritest**](https://github.com/simonheb/ritest) | Stata | Grant's `ritest` is itself inspired by Simon Heß `ritest` Stata package.|

### Wild cluster bootstrap

| Package | Language | Role |
|---|---|---|
| [**wildboottest**](https://github.com/py-econometrics/wildboottest) | Python | PyFixest loads classes from `wildboottest` to run wild bootstrap inference. `wildboottest` is a Python port of `fwildclusterboot`. |
| [**fwildclusterboot**](https://github.com/s3alfisc/fwildclusterboot) | R | An R implementation of the "fast and wild" algorithm by Roodman et al.  |
| [**boottest**](https://github.com/droodman/boottest) | Stata | Roodman et al. (2019). The fast wild cluster bootstrap methodology traces back to Roodman's Stata boottest package |

### Multiple hypothesis testing (Romano-Wolf)

| Package | Language | Role |
|---|---|---|
| [**wildrwolf**](https://github.com/s3alfisc/wildrwolf) | R | PyFixest's `rwolf()` Romano-Wolf correction is tested against the wildrwolf R package for both HC and CRV inference. |
| [**wildwyoung**](https://github.com/s3alfisc/wildwyoung) | R | An R implementation of the Westfall-Young correction using the wild bootstrap. |
| [**rwolf**](https://github.com/damiancclarke/rwolf) | Stata | A Stata implementation of the Romano-Wolf stepdown procedure that inspired development of `rwolf`. |
| [**wyoung**](https://github.com/reifjulian/wyoung) | Stata | A Stata implementation of the Westfall-Young stepdown procedure by Jones, Molitor & Reif.|

### Causal cluster variance

| Package | Language | Role |
|---|---|---|
| [**TSCB-CCV**](https://github.com/Daniel-Pailanir/TSCB-CCV) | Stata | Pailanir & Clarke. PyFixest's CCV implementation (Abadie et al., QJE 2023) is tested against Daniel Pailanir and Damian Clarke's Stata implementation.|

### Gelbach decomposition

| Package / Author | Language | Role |
|---|---|---|
| [**b1x2**](https://ideas.repec.org/c/boc/bocode/s457814.html)| Stata |PyFixest's `decompose()` method is tested against hardcoded results from Gelbach's `b1x2` Stata package.|
| [Apoorva's Linear Mediation Gist](https://gist.github.com/apoorvalal/e7dc9f3e52dcd9d51854b28b3e8a7ba4) | Python | The initial implementation of Gelbach's decomposition in `PyFixest` was based on Apoorva's gist |

### Demeaning and fixed effects recovery

| Package | Language | Role |
|---|---|---|
| [**lfe**](https://cran.r-project.org/web/packages/lfe/vignettes/lfehow.pdf) | R | We based our first implementation of the MAP algorithm on the description in the "how lfe works" vignette. |
| [**pyhdfe**](https://github.com/jeffgortmaker/pyhdfe) | Python | PyFixest's demeaning results are tested against Jeff Gortmaker's `pyhdfe`. `pyfixest`'s first MVP was built using `pyhdfe` it ran its demeaning algorithm via `pyhdfe` MAP algo. |

---

## Test infrastructure

The following packages are used in PyFixest's test suite to bridge between
Python and R:

| Package | Language | Role |
|---|---|---|
| [**rpy2**](https://rpy2.github.io/) | Python | The bridge between Python and R that powers all cross-language test comparisons. |

## Other Software

Here we list other foundational software without which a project like `PyFixest` would not be possible:

- `formulaic`
- `numpy`
- `numba`
- `pandas`
- `scipy`
- `matplotlib`
- `great-tables` / `maketables`
- `pyo3`

# Papers and Algorithms

- Bergé, L. R., Butts, K., & McDermott, G. (2026). "Fast and user-friendly econometrics estimations: The R package fixest." [arXiv:2601.21749](https://arxiv.org/abs/2601.21749).
- Correia, S., Guimarães, P., & Zylkin, T. (2020). "ppmlhdfe: Fast Poisson estimation with high-dimensional fixed effects." *The Stata Journal*, 20(1). [arXiv:1903.01690](https://arxiv.org/abs/1903.01690).
- Gaure, S. (2013). "OLS with multiple high dimensional category variables." *Computational Statistics & Data Analysis*, 66, 8-18. [Vignette: How lfe works](https://cran.r-project.org/web/packages/lfe/vignettes/lfehow.pdf).
- Guimarães, P. & Portugal, P. (2010). "A simple feasible procedure to fit models with high-dimensional fixed effects." *The Stata Journal*, 10(4), 628-649. [DOI:10.1177/1536867X1101000406](https://doi.org/10.1177/1536867X1101000406).
- Koenker, R. & Ng, P. (2005). "A Frisch-Newton Algorithm for Sparse Quantile Regression." *Acta Mathematicae Applicatae Sinica*, 21(2), 225-236. [DOI:10.1007/s10255-005-0231-1](https://doi.org/10.1007/s10255-005-0231-1).

---

# brave_true.html

## Causal Inference for the Brave and True


```python
import pandas as pd

import pyfixest as pf
```

### Chapter 14: Panel Data and Fixed Effects

In this example we replicate the results of the great (freely available reference!) Causal Inference for the Brave and True - Chapter 14. Please refer to the original text for a detailed explanation of the data.


```python
data_path = "https://raw.githubusercontent.com/bashtage/linearmodels/main/linearmodels/datasets/wage_panel/wage_panel.csv.bz2"
data_df = pd.read_csv(data_path)

data_df.head()
```

```text
   nr  year  black  exper  hisp  hours  married  educ  union     lwage  \
0  13  1980      0      1     0   2672        0    14      0  1.197540   
1  13  1981      0      2     0   2320        0    14      1  1.853060   
2  13  1982      0      3     0   2940        0    14      0  1.344462   
3  13  1983      0      4     0   2960        0    14      0  1.433213   
4  13  1984      0      5     0   3071        0    14      0  1.568125   

   expersq  occupation  
0        1           9  
1        4           9  
2        9           9  
3       16           9  
4       25           5  
```

We have a classical panel data set with units (nr) and time (year).

We are interested in estimating the effect of marriage status on log wage, using a set of controls (union, hours) and individual (nr) and year fixed effects. 

```python
panel_fit = pf.feols(
    fml="lwage ~ married + expersq + union + hours | nr + year",
    data=data_df,
    vcov={"CRV1": "nr + year"},
    demeaner_backend="rust",
)
```

```python
pf.etable(panel_fit)
```

```text
GT(_tbl_data=  level_0               level_1                       0
0    coef               married     0.048* <br> (0.018)
1    coef               expersq  -0.006*** <br> (0.001)
2    coef                 union     0.073* <br> (0.023)
3    coef                 hours   -0.000** <br> (0.000)
4      fe                    nr                       x
5      fe                  year                       x
6   stats          Observations                    4360
7   stats             S.E. type             by: nr+year
8   stats         R<sup>2</sup>                   0.631
9   stats  R<sup>2</sup> Within                   0.047, _body=<great_tables._gt_data.Body object at 0x000001E546E22C30>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x000001E546115DF0>, _spanners=Spanners([SpannerInfo(spanner_id='lwage', spanner_level=1, spanner_label='lwage', spanner_units=None, spanner_pattern=None, vars=['0'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x000001E546EB2420>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo(scss=True, category='table', type='value', value='solid'), table_border_top_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_top_color=OptionsInfo(scss=True, category='table', type='value', value='#A8A8A8'), table_border_right_style=OptionsInfo(scss=True, category='table', type='value', value='none'), table_border_right_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_right_color=OptionsInfo(scss=True, category='table', type='value', value='#D3D3D3'), table_border_bottom_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_bottom_style=OptionsInfo(scss=True, category='table', type='value', value='hidden'), table_border_bottom_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_bo
...[truncated]...
```

We obtain the same results as in the book!

---

# changelog.html

# Changelog

```{python}
import pyfixest as pf
from pyfixest.report.utils import rename_categoricals

df = pf.get_data()

fit1 = pf.feols("Y ~ X1", data = df)
fit2 = pf.feols("Y ~ X1 + X2", data = df)
fit3 = pf.feols("Y ~ X1 + X2 | f1", data = df)
```

## PyFixest 0.50.0 (In Development)

::: {.callout-tip}
You can install the latest pre-release to try out the new features:

```{.bash .code-copy}
pip install pyfixest==0.50.0a1
pip install --pre pyfixest
```
:::

### Reworked Formula Parsing and New `i()` Operator

The formula parsing module has been significantly reworked. The new implementation introduces a cleaner `Formula` class,
a rewritten `i()` operator that closely follows R's `fixest` syntax, and new multiple estimation operators.

#### New `i()` operator

The `i()` operator now follows the `fixest` naming convention, using `::` to separate variable names from levels. It further
provides two new arguments: `ref2` and `bin2` that allow to set
reference levels for interacted variables and to bin categoricals.

**Simple categorical encoding:**

```{python}
# Coefficient per level, first level dropped when intercept is present
fit = pf.feols("Y ~ i(f1, ref=1)", data=df)
fit.coef().head()
```

**Factor x Continuous interaction:**

```{python}
# Each level of f1 gets its own slope on X1
fit = pf.feols("Y ~ i(f1, X1, ref=1)", data=df)
fit.coef().head()
```

**Factor x Factor interaction** with `ref2`:

The `ref2` argument controls the reference level of the second variable in a factor-by-factor interaction.

```{python}
import numpy as np
df["group"] = np.where(df["f1"] < 15, "A", "B")

# Full interaction: f1 levels x group levels
# ref drops from f1, ref2 drops from group
fit = pf.feols("Y ~ i(f1, group, ref=1, ref2='A')", data=df)
fit.coef().head()
```

**Binning** with `bin` and `bin2`:

The `bin` parameter merges categorical levels before encoding. This is useful for collapsing sparse categories.
Values not in the mapping are kept unchanged, matching R `fixest` behavior.

```{python}
df["size"] = np.where(df["f1"] < 10, "small", np.where(df["f1"] < 20, "medium", "large"))

# Merge 'small' and 'medium' into 'not_large', then use as reference
fit = pf.feols("Y ~ i(size, bin={'not_large': ['small','medium']}, ref='not_large')", data=df)
fit.coef()
```

`bin2` applies binning to the second variable in a factor-by-factor interaction.

#### `mvsw()` and multiple estimation


`mvsw()` for **multiverse stepwise** — generates all $2^k$ combinations of the provided variables, including the intercept-only model.

```{python}
# mvsw: all combinations of X1 and X2
fits = pf.feols("Y ~ mvsw(X1, X2)", data=df)
pf.etable(fits)
```

Multiple estimation operators can be combined:

```{python}
# mvsw: all combinations of X1 and X2
fits = pf.feols("Y ~ sw(X1, X2) + csw(f1,f2)", data=df)
pf.etable(fits)
```

Last, you can run operations within the operators:

```{python}
# mvsw: all combinations of X1 and X2
fits = pf.feols("Y ~ sw(X1, f1 + X2)", data=df)
pf.etable(fits)
```

#### Deprecations

- `FixestFormulaParser` is deprecated in favor of `Formula.parse()`. A `FutureWarning` is emitted when the old class is used.
- `model_matrix_fixest()` is deprecated in favor of `create_model_matrix()`.


### Migration to maketables

The table functionality in pyfixest now uses [maketables](https://py-econometrics.github.io/maketables/). `maketables` is a spin-off of
pyfixest internal functions, but supports more packages in the Python eco-system (e.g. `statsmodels` and `linearmorels`). Due to it's close
connection to `pyfixest`, the API of `pf.etable()` remains unchanged.
**Changes:**

- `pf.etable()` now uses `maketables.ETable` internally. The API remains unchanged for backward compatibility.
- Because the function is not at the core of `pyixest` functionality, we will deprecate `pf.dtable()`.
  A `FutureWarning` is now emitted. The function has been moved to `maketables` and can be used by calling `maketables.DTable()` directly.
- The same applies for `pf.make_table()`, which has been an internal utility function to create tables. An equivalent function now lives in
  `maketables.MTable()`.
- The `great_tables` dependency has been replaced with `maketables`.

**Migration guide:**

```python
# dtable migration
# Before:
pf.dtable(df, vars=["Y", "X1"])
# After:
import maketables
maketables.DTable(df, vars=["Y", "X1"])

# make_table migration
# Before:
pf.make_table(df, type="gt", caption="My Table")
# After:
import maketables
maketables.MTable(df, caption="My Table").make(type="gt")
```

### Other Changes

- Adds the following statistics to the `Fepois` class: `_loglik`, `_loglik_null`, `_pseudo_r2` for regression without weights.
- Set the default parameters for the MAP algorithm to a tolerance of `1e-06` and maximum number of iterations of `10_000`.
- Adds support for weights for poisson regression via `feopis()`.

### GLMs with High-Dimensional Fixed Effects

`feglm()` now supports high-dimensional fixed effects for logit, probit, and gaussian families! Fixed effects are specified after the `|` symbol, just like in `feols()` and `fepois()`.

```{python}
import numpy as np

data_glm = pf.get_data()
data_glm["Y"] = np.where(data_glm["Y"] > 0, 1, 0)

# Logit with fixed effects
fit_logit = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="logit")
fit_logit.summary()
```

Fixed effects estimation via demeaning produces identical point estimates as one-hot encoding the fixed effects via `C()`:

```{python}
# Compare FE demeaning vs one-hot encoding
fit_logit_onehot = pf.feglm("Y ~ X1 + X2 + C(f1)", data=data_glm, family="logit")

pf.etable([fit_logit, fit_logit_onehot])
```

Note that standard errors may differ between the two approaches due to different degrees of freedom adjustments.

All three GLM families (gaussian, logit, probit) support fixed effects:

```{python}
fit_gaussian = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="gaussian")
fit_probit = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="probit")

pf.etable([fit_gaussian, fit_logit, fit_probit])
```

## PyFixest 0.40.1

### Breaking Changes for compatibility with `fixest` 0.13

`fixest` 0.13 has recently been released to CRAN, with a range of breaking changes. We are following these in pyfixet 0.40.0.

- The default inference method is now always "iid". Before, both `fixest` and `pyfixest` would default to cluster by the first fixest effect
  in the presence of fixed effect.
- The arguments of the `pf.ssc()` functions have been renamed: `adj` becomes `k_adj`, `fixef_k` becomes `k_fixef`, `cluster_df` becomes `G_df`, and `cluster_adj` becomes `G_adj`.
  Backwards compatibility is ensured.
- `fixest` no longer applies the `G/(G-1)` small sample correction for heteroskedastic errors. The argument is now only relevant for cluster robust errors.
- If the `k_adj` arg is set to `True` with heteroskedastic errors, the applied small sample correction now is `N / (N-k)` and no longer `(N-1) / (N-df_k)`,
  as was previously the case and is still the case for iid and cluster robust errors.
- The `k_fixef` option `"nested"` has been renamed to `"nonnested"`.
- The `fixef_rm` argument is changed from default `"none"` to `"singleton"` for all estimation functions. This has no impact on point estimates, but might change inference due to a smaller number of
  observations in the degree of freedom correction.
- The multicollinearity default tolerance has been reduced from 1e-10 to 1e-09.
- The attribute `_dof_k` has been renamed to `_df_k`.

Other related changes:
- The arguments to adjust the small samples for the `wildboottest` methods have been renamed to `k_adj` and `G_adj`.

### Moved to `pixi.toml` and install from conda-forge

We have been using pixi as our package manager for a while and are moving from a `pyproject.toml` to using a `pixi.toml` for specifying dependencies.
We also install all package by default from the free and open source conda-forge. Because of some environment resolution challenges around an old version of
`rpy2`that we need for testing `pyfixest` against `fixest`, we temporarily have dropped support for the dev and docs environments for windows. A fix is
work in progress.

## New Features

### CuPy and SciPy Backends via LSMR

We add CuPy and SciPy Backend to run the demeaning algorithm on the GPU via the sparse LSMR solver. For problems
where the standard demeaner struggles to converge, this strategy can lead to significant speedups if paired with a GPU.

![Complex Fixed Effects Benchmark](https://raw.githubusercontent.com/py-econometrics/pyfixest/master/benchmarks/complex_benchmarks.png)

### HAC Standard Errors

We now support HAC standard errors! Thanks for Daman (https://github.com/damandhaliwal) for all his work (and coping with me with the PR, which took forever to get through).
We now support:

- time series HAC
- panel HAC
- panel DK

variance-covariance matrices.

For now, we assume that the time series column is consecutive and each entry has "time delta 1". Relaxation of this requirement are work in progress.

### `cat_template` argument for plotting functions

For easier encoding of categorical variables in relation to the `iplot()` and `coefplot()` functions, we add a new function argument, cat_template:

```{python}
fit_c = pf.feols(fml = "Y ~ i(X1, f1)", data = df)
fit_c.iplot(cat_template = "{variable}::{value}")
```

This is particularly useful for Difference-in-Differences and event studies. You can find an example use case in the DiD [vignette](https://pyfixest.org/difference-in-differences.html#one-shot-adoption-static-and-dynamic-specifications).

Going forward, we will deprecate the `rename_event_study_coefs` function, which is no longer needed (and did not work anyways).

### Updates to Gelbach Decomposition

We have reworked the Gelbach Decomposition method, with some breaking changes: by default, calling `Feols.decompose()` now returns a `GelbachDecomposition` instance - before, we'd return a `pd.DataFrame`. The `param` argument has been renamed to `decomp_var`, but is not yet deprecated.

The class now comes with new `tidy()` and `etable()` methods.

```{python}
from pyfixest.utils.dgps import gelbach_data
import numpy as np
import pyfixest as pf

# Generate test data using gelbach_data
data = gelbach_data(nobs=500)
data["w"] = np.random.rand(500)
fit = pf.feols("y ~ x1 + x21 + x22 + x23", data=data)
gb = fit.decompose(decomp_var = "x1", x1_vars = ["x21"],reps = 10, nthreads = 1)
print(type(gb))
```

It is now also possible to add background variables that are included in both the long and short regressions via the `x1_vars` function argument.

We can inspect results as a `pd.DataFrame`

```{python}
gb.tidy()
```

or produce a `GT` table:

```{python}
gb.etable(
    stats = "all",
    caption = "Gelbach Decomposition"
)
```

As can be seen, we by default now return normalized (and not just absolute) effects.

We are now also supporting frequency weights for the decomposition (currently without inference).

Additionally, some house keeping and interal refactoring of the `GelbachDecomposition` class.

## PyFixest 0.30.0

### New Features

- We have created a **Rust** backend for all performance critical algorithms, with pretty great performance improvements! You can use the Rust backend by setting `demeaner_options = "rust"`.

We find pretty great performance improvements and want to make the Rust backend the default in PyFixest 0.31.0.

To back up the performance claim, here is a benchmark:

```{python}
import pyfixest as pf
import numpy as np
import pandas as pd
import time

rng = np.random.default_rng(737)

N = 10_000_000
benchmark_data = pd.DataFrame({
  "Y": rng.normal(0, 1, N),
  "X1": rng.normal(0, 1, N),
  "X2": rng.normal(0, 1, N),
  "X3": rng.normal(0, 1, N),
  "f1": rng.integers(0, 10_000, N),
  "f2": rng.integers(0, 1_000, N),
  "f3": rng.integers(0, 10, N),
})

# burn-in for numba
fit_nb_warmup = pf.feols(
  fml = "Y ~ X1 + X2 + X3 | f1 + f2 + f3", data = benchmark_data[:100_000]
)

# benchmark for numba backend
tic = time.time()
fit_nb = pf.feols(
  fml = "Y ~ X1 + X2 + X3 | f1 + f2 + f3", data = benchmark_data
)
toc = time.time()
print(f"Numba backend took {toc-tic}.")

# benchmark for rust backend
tic = time.time()
fit_rust = pf.feols(
  fml = "Y ~ X1 + X2 + X3 | f1 + f2 + f3", data = benchmark_data,
  demeaner_backend = "rust"
)
toc = time.time()
print(f"Rust backend took {toc-tic}.")
```

Results are also matching =)

```{python}
pf.etable([fit_nb, fit_rust], digits = 8)
```

- We now support **quantile regression**, including a Frisch-Newton Interior Point Solver with and without preprocessing, iid, heteroskedastic and cluster robust standard errors, fast algorithms for the entire quantile regression process, and some visualisations. In particular the algorithms for the quantile regression process show excellent performance. You can
learn more about all features and take a look at more systematic benchmarks in the [quantreg vignette](https://pyfixest.org/quantile-regression.html).

```{python}
#| eval: false

N_qr = 10_000
rng = np.random.default_rng(929)

df_qr = pd.DataFrame({
  "X1": rng.normal(0, 1, N_qr),
  "X2": rng.normal(0, 1, N_qr)
})
df_qr["Y"] = -0.5 + -2 * df_qr["X1"] + 1.9 * df_qr["X1"] ** 4 + df_qr["X2"] - 0.4 * df_qr["X2"] **7 + rng.normal(0, 1, N_qr)

fit_qr = pf.quantreg(
  fml = "Y ~ X1 + X2",
  data = df_qr,
  quantile = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
  method = "pfn",
  multi_method = "cfm2"
)

pf.qplot(fit_qr, figsize = [7,3])
```

- We have switched the **default solver** to `scipy.linalg.solve()`: [link](https://github.com/py-econometrics/pyfixest/pull/904)

- You can now set the **maximum number of iterations** for the demeaning algo via a `fixef_maxiter` argument: [link](https://github.com/py-econometrics/pyfixest/pull/944)

### Bug Fixes

- We fixed a bug in internal renaming of categoricals: [link](https://github.com/py-econometrics/pyfixest/pull/886)
- We fixed a bug in etable arguments [link](https://github.com/py-econometrics/pyfixest/pull/889)
- We stopped casting dependent variable to integer to void Information Loss in Poisson Regression: [link](https://github.com/py-econometrics/pyfixest/pull/900)


### Documentation

- We have added a guide on how to replicate Stata results with pyfixest: [link](https://github.com/py-econometrics/pyfixest/pull/897)
- We improved the documentation on how to relabel variable names in the plotting and etable functions: [link](https://github.com/py-econometrics/pyfixest/pull/895)

## Infrastructure

- We have reorganized our tests and rely more on conda environments for making R package test dependencies available: [link](https://github.com/py-econometrics/pyfixest/pull/906)

### Community

- We have added a Code of Conduct.
- We have opened our discord community. Please join us there to discuss pyfixest and other py-econometrics projects! Link [here](https://discord.com/invite/gBAydeDMVK).

## New Contributors
* @FuZhiyu made their first contribution in https://github.com/py-econometrics/pyfixest/pull/886
* @mortizm1988 made their first contribution in https://github.com/py-econometrics/pyfixest/pull/895
* @jestover made their first contribution in https://github.com/py-econometrics/pyfixest/pull/897
* @JaapCTJ made their first contribution in https://github.com/py-econometrics/pyfixest/pull/900
* @shapiromh made their first contribution in https://github.com/py-econometrics/pyfixest/pull/906
* @schroedk made their first contribution in https://github.com/py-econometrics/pyfixest/pull/905
* @WiktorTheScriptor made their first contribution in https://github.com/py-econometrics/pyfixest/pull/938
* @damandhaliwal made their first contribution in https://github.com/py-econometrics/pyfixest/pull/944

**Full Changelog**: https://github.com/py-econometrics/pyfixest/compare/v0.29.0...v0.30.0

## PyFixest 0.29.0

- We add options `fixef_k = "nested"` and `fixef_k = "full"` for computing small sample corrections via `pf.ssc()`. We set the defaults for `pf.feols()` and other estimation functions to `fixef_k = "nested"` to 100% mimic the defaults of `r-fixest`. This is a "breaking change" in the sense that it might (slightly) impact the standard errors of your estimations.
- We add support for fully saturated event study estimation via the `SaturatedEventStudy` class, which can be called via `pf.event_study()`.
- We add support for difference-in-differences specification tests following Lal (2025).
- We add R2-within values to the default `etable()` output.
- We fix a small bug in the Gelbach `decompose()` method, which would fail if a user selected `only_coef = True`.
- The `decompose()` method runs fully on sparse matrices, which leads to large performance improvements on big data sets.
- We fix a small bug in the `predict()` method with `newdata`, see [here](https://github.com/py-econometrics/pyfixest/issues/840) for details.
- We add a function argument `rename_models` to help rename model names in the `coefplot()` and `iplot()` functions and methods:
  ```{python}
  pf.coefplot(
      models = [fit1, fit2, fit3],
      rename_models = {
          fit1._model_name_plot: "Model 1",
          fit2._model_name_plot: "Model 2",
          fit3._model_name_plot: "Model 3"
      },
  )
  ```
- Made `lets-plot` an optional dependency. The package will now fall back to `matplotlib` for plotting if `lets-plot` is not installed. Users can install `lets-plot` with `pip install pyfixest[plots]`.
- PyFixest now supports R2, adjusted R2, and within-R2 values for WLS (it previously only did for OLS, if at all).
- We add support for standard error of predictions for OLS models without fixed effects. As a default, the predict model still returns a `np.ndarray`. If the argument `se_fit` is set to
  `True`, we report the prediction standard errors. If argument `interval = "prediction"`, we return a `pd.DataFrame` with predictions, their standard errors, and confidence intervals.

## PyFixest 0.28.0

### New features and bug fixes
- Adds a function argument `context`, that allows to pass information / context to the `formulaic.Formulaic.get_model_matrix()` call that creates the model matrix.
- Fix a bug that caused reindexing of `LPDID._coeftable` when calling `LPDID.iplot()`. As a result, a second call of `LPDID.iplot()` would fail.
- Bumps the required `formulaic` version to `1.1.0` and fixes errors that arose when a) the ref argument was used for i() syntax, which led to a silent failure under formulaic >= 1.1.0, and fixef() / predict() with fixed effects, which led to a loud error.

### New experimental Features
- Adds a `pf.feglm()` function that supports GLMs with normal and binomial families (gaussian, logit, probit) without fixed effects. Fixed effects support is work in progress.
- Adds options to run the demean function via JAX. This might speed up the model fit if GPU is available.


## PyFixest 0.27.0

- Adds support for Gelbach's (JoLe 2016) Regression Decomposition method using a `decompose()` method for `Feols`.
- Adds support for the multiple hypothesis correction by Westfall & Young via the `pf.wyoung()` function.
- Input data frames to `pf.feols()` and `pf.fepois()` are now converted to `pandas` via [narwhals](https://github.com/narwhals-dev/narwhals).
  As a result, users can not provide `duckdb` or `ibis` tables as inputs, as well as `pandas` and `polars` data frames. `polars` and `pyarrow`
  are dropped as a dependencies.
- Fixes a bug in the `wildboottest` method, which incorrectly used to run a regression on the demeaned dependend variable in case it was
  applied after a fixed effects regression. My apologies for that!
- Fixes a bug in the `ritest` method, which would use randomization inference coefficients instead of t-statistics, leading to incorrect results.
  This has consequences for the rwolf() function, which, in case of running ri-inference, would default to run the randomization-t. My apolgies!
- Adds a vignette on multiple testing corrections.
- Adds a vignette on Gelbach's regression decomposition.

## PyFixest 0.22.0 - 0.25.4

See the github changelog for details: [link](https://github.com/py-econometrics/pyfixest/releases).


## PyFixest 0.22.0

### Changes

- Fix bug in wildboottest method @s3alfisc (#506)
- docs: add sanskriti2005 as a contributor for infra @allcontributors (#503)
- Infra: added the release-drafter for automation of release notes @sanskriti2005 (#502)
- Fix broken link in contributing.md @s3alfisc (#499)
- docs: add leostimpfle as a contributor for bug @allcontributors (#495)
- Update justfile @leostimpfle (#494)
- docs: add baggiponte as a contributor for doc @allcontributors (#490)
- docs: improve installation section @baggiponte (#489)
- Bump tornado from 6.4 to 6.4.1 @dependabot (#487)
- docs: add leostimpfle as a contributor for code @allcontributors (#478)
- Feols: speed up the creation of interacted fixed effects via `fe1^fe2` syntax @leostimpfle (#475)
- rename resampling iterations to 'reps' in all methods @s3alfisc (#474)
- fix a lot of broken links throught the repo @s3alfisc (#472)
- Multiple readme fixes required after package was moved to py-econometrics project @s3alfisc (#450)

### Infrastructure

- infrastructure: fix minor release drafter bugs @s3alfisc (#504)

## PyFixest 0.21.0

- Add support for randomization inference via the `ritest()` method:

```{python}
#| eval: False
import pyfixest as pf
data = pf.get_data()

fit = pf.feols("Y ~ X1", data = data)
fit.ritest(resampvar="X1=0", reps = 1000)
```

## PyFixest 0.20.0

- This version introduces MyPy type checks to the entire pyfixest codebase. Thanks to @juanitorduz for nudging me to get started with this =). It also fixes a handful of smaller bugs.

## PyFixest 0.19.0

- Fixes multiple smaller and larger performance regressions. The NYC-Taxi example regression now takes approximately 22 seconds to run (... if my laptopt is connected to a power charger)!

```{python}
#| eval: False

%load_ext autoreload
%autoreload 2

import duckdb
import time
import numpy as np
import pyfixest as pf

# %%
nyc = duckdb.sql(
    '''
    FROM 'C:/Users/alexa/Documents/nyc-taxi/**/*.parquet'
    SELECT
        tip_amount, trip_distance, passenger_count,
        vendor_id, payment_type, dropoff_at,
        dayofweek(dropoff_at) AS dofw
    WHERE year = 2012 AND month <= 3
    '''
    ).df()

# convert dowf, vendor_id, payment_type to categorical
tic = time.time()
nyc["dofw"] = nyc["dofw"].astype(int)
nyc["vendor_id"] = nyc["vendor_id"].astype("category")
nyc["payment_type"] = nyc["payment_type"].astype("category")
print(f"""
    I am convering columns of type 'objects' to 'categories' and 'int'data types outside
    of the regression, hence I am cheating a bit. This saves {np.round(time.time() - tic)} seconds.
    """
)
#    I am convering columns of type 'objects' to 'categories' and 'int'data types outside
#    of the regression, hence I am cheating a bit. This saves 7.0 seconds.

run = True
if run:

    # mock regression for JIT compilation
    fit = pf.feols(
        fml = "tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw",
        data = nyc.iloc[1:10_000],
        copy_data = False,
        store_data = False
        )

    import time
    tic = time.time()
    fit = pf.feols(
        fml = "tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw",
        data = nyc,
        copy_data = False, # saves a few seconds
        store_data = False # saves a few second
        )
    passed = time.time() - tic
    print(f"Passed time is {np.round(passed)}.")
    # Passed time is 22.
```

- Adds three new function arguments to `feols()` and `fepois()`: `copy_data`, `store_data`, and `fixef_tol`.
- Adds support for frequency weights with the `weights_type` function argument.
```{python}
import pyfixest as pf

data = pf.get_data(N = 10000, model = "Fepois")
df_weighted = data[["Y", "X1", "f1"]].groupby(["Y", "X1", "f1"]).size().reset_index().rename(columns={0: "count"})
df_weighted["id"] = list(range(df_weighted.shape[0]))

print("Dimension of the aggregated df:", df_weighted.shape)
print(df_weighted.head())

fit = pf.feols(
    "Y ~ X1 | f1",
    data = data
)
fit_weighted = pf.feols(
    "Y ~ X1 | f1",
    data = df_weighted,
    weights = "count",
    weights_type = "fweights"
)
pf.etable([fit, fit_weighted], coef_fmt = "b(se) \n (t) \n (p)")
```

- Bugfix: Wild Cluster Bootstrap Inference with Weights would compute unweighted standard errors. Sorry about that! WLS is not supported for the WCB.
- Adds support for CRV3 inference with weights.


## PyFixest 0.18.0

- Large Refactoring of Interal Processing of Model Formulas, in particular `FixestFormulaParser` and `model_matrix_fixest`. As a results, the code should be cleaner and more robust.
- Thanks to the refactoring, we can now bump the required `formulaic` version to the stable `1.0.0` release.
- The `fml` argument of `model_matrix_fixest` is deprecated. Instead, `model_matrix_fixest`
  now asks for a `FixestFormula`, which is essentially a dictionary with information on model
  specifications like a first stage formula (if applicable), dependent variables, fixed effects,
  etc.
- Additionally, `model_matrix_fixest` now returns a dictionary instead of a tuple.
- Brings back fixed effects reference setting via `i(var1, var2, ref)` syntax. Deprecates the `i_ref1`, `i_ref2` function arguments. I.e. it is again possible to e.g. run

```{python}
#| eval: False

import pyfixest as pf
data = pf.get_data()

fit1 = pf.feols("Y ~ i(f1, X2)", data=data)
fit1.coef()[0:8]
```
Via the `ref` syntax, via can set the reference level:
```{python, eval=FALSE}
#| eval: False
fit2 = pf.feols("Y ~ i(f1, X2, ref = 1)", data=data)
fit2.coef()[0:8]
```

## PyFixest 0.17.0

- Restructures the codebase and reorganizes how users can interact with the `pyfixest` API. It is now recommended to use `pyfixest` in the following way:

  ```{python}
  import numpy as np
  import pyfixest as pf
  data = pf.get_data()
  data["D"] = data["X1"] > 0
  fit = pf.feols("Y ~ D + f1", data = data)
  fit.tidy()
  ```

  The update should not inroduce any breaking changes. Thanks to [@Wenzhi-Ding](https://github.com/Wenzhi-Ding) for the PR!

- Adds support for simultaneous confidence intervals via a multiplier bootstrap. Thanks to [@apoorvalal](https://github.com/apoorvalal) for the contribution!

  ```{python}
  fit.confint(joint = True)
  ```

- Adds support for the causal cluster variance estimator by [Abadie et al. (QJE, 2023)](https://academic.oup.com/qje/article/138/1/1/6750017)
  for OLS via the `.ccv()` method.

  ```{python}
  fit.ccv(treatment = "D", cluster = "group_id")
  ```



## PyFixest 0.16.0

- Adds multiple quality of life improvements for developers, thanks to [NKeleher](https://github.com/NKeleher).
- Adds more options to customize `etable()` output thanks to [Wenzhi-Ding](https://github.com/Wenzhi-Ding).
- Implements Romano-Wolf and Bonferroni corrections for multiple testing in the `multcomp` module.

## PyFixest 0.15.

- Adds support for weighted least squares for `feols()`.
- Reduces testing time drastically by running tests on fewer random data samples. Qualitatively,
  the set of test remains identical.
- Some updates for future `pandas` compatibility.

## PyFixest 0.14.0

- Moves the documentation to [quartodoc](https://github.com/machow/quartodoc).
- Changes all docstrings to `numpy` format.
- Difference-in-differences estimation functions now need to be imported via the `pyfixest.did.estimation` module:

```{python}
from pyfixest.did.estimation import did2s, lpdid, event_study
```
## PyFixest 0.13.5

- Fixes a bug that lead to incorrect results when the dependent variable and **all covariates** (excluding the fixed effects) where integers.

## PyFixest 0.13.4

- Fixes a bug in `etable()` with IV's that occurred because `feols()` does not report R2 statistics for IVs.

## PyFixest 0.13.2

- Fixes a bug in `etable()` and a warning in `fixest_model_matrix` that arose with higher `pandas` versions. Thanks to @aeturrell for reporting!

## PyFixest 0.13.0

### New Features

- Introduces a new `pyfixest.did` module which contains routines for Difference-in-Differences estimation.
- Introduces support for basic versions of the local projections DiD estimator following [Dube et al (2023)](https://www.nber.org/papers/w31184)
- Adds a new vignette for Difference-in-Differences estimation.
- Reports R2 values in `etable()`.


## PyFixest 0.12.0


### Enhancements:

- Good performance improvements for singleton fixed effects detection. Thanks to [@styfenschaer](https://github.com/styfenschaer) for the PR! See [#229](https://github.com/py-econometrics/pyfixest/issues/229).
- Uses the [r2u project](https://github.com/eddelbuettel/r2u) for installing R and R packages on github actions, with great performance improvements.
- Allows to pass `polars` data frames to `feols()`, `fepois()` and `predict()`. [#232](https://github.com/py-econometrics/pyfixest/issues/232). Thanks to [@vincentarelbundock](https://github.com/py-econometrics/pyfixest/issues/232) for the suggestion!

### Bug Fixes:

- Missing variables in features were not always handled correctly in `predict()` with `newdata` not `None` in the presence of missing data, which would lead to an error. See [#246](https://github.com/py-econometrics/pyfixest/issues/246) for details.
- Categorical variables were not always handled correctly in `predict()` with `newdata` not `None`, because the number of fixed effects levels in `newdata` might be smaller than in `data`. In consequence, some levels were not found, which lead to an error. See [#245](https://github.com/py-econometrics/pyfixest/issues/245) for details. Thanks to [@jiafengkevinchen](https://github.com/jiafengkevinchen) for the pointer!
- Multicollinearity checks for over-identified IV was not implemented correctly, which lead to a dimension error. See [#236](https://github.com/py-econometrics/pyfixest/issues/236) for details.  Thanks to [@jiafengkevinchen](https://github.com/jiafengkevinchen) for the pointer!
- The number of degrees of freedom `k` was computed incorrectly if columns were dropped from the design matrix `X` in the presence of multicollinearity. See [#235](https://github.com/py-econometrics/pyfixest/issues/235) for details.  Thanks to [@jiafengkevinchen](https://github.com/jiafengkevinchen) for the pointer!
- If all variables were dropped due to multicollinearity, an unclear and imprecise error message was produced. See [#228](https://github.com/py-econometrics/pyfixest/issues/228) for details. Thanks to [@manferdinig](https://github.com/manferdinig) for the pointer!
- If selection `fixef_rm = 'singleton'`, `feols()` and `fepois()` would fail, which has been fixed. [#192](https://github.com/py-econometrics/pyfixest/issues/192)

### Dependency Requirements

- For now, sets `formulaic` versions to be `0.6.6` or lower as version `1.0.0` seems to have introduced a problem with the `i()` operator, See [#244](https://github.com/py-econometrics/pyfixest/issues/244) for details.
- Drops dependency on `pyhdfe`.

## PyFixest 0.11.1

- Fixes some bugs around the computation of R-squared values (see [issue #103](https://github.com/py-econometrics/pyfixest/issues/103)).
- Reports R-squared values again when calling `.summary()`.

## PyFixest 0.11.0

- Significant speedups for CRV1 inference.

## PyFixest 0.10.12

Fixes a small bug with the separation check for poisson regression #138.

## PyFixest 0.10.11

Fixes bugs with i(var1, var2) syntax introduced with PyFixest 0.10.10.

## PyFixest 0.10.10

Fixes a bug with variable interactions via `i(var)` syntax. See [issue #221](https://github.com/py-econometrics/pyfixest/issues/211).

## PyFixest 0.10.9

Makes `etable()` prettier and more informative.

## PyFixest 0.10.8

### Breaking changes
Reference levels for the `i()` formula syntax can no longer be set within the formula, but need to be added via the `i_ref1` function argument to either `feols()` and `fepois()`.

### New feature

A `dids2()` function is added, which implements the 2-stage difference-in-differences procedure à la Gardner and follows the syntax of @kylebutts [did2s](https://github.com/kylebutts/did2s) R package.

```py
from pyfixest.did.did import did2s
from pyfixest.estimation import feols
from pyfixest.visualize import iplot
import pandas as pd
import numpy as np

df_het = pd.read_csv("https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv")

fit = did2s(
    df_het,
    yname = "dep_var",
    first_stage = "~ 0 | state + year",
    second_stage = "~i(rel_year)",
    treatment = "treat",
    cluster = "state",
    i_ref1 = [-1.0, np.inf],
)

fit_twfe = feols(
    "dep_var ~ i(rel_year) | state + year",
    df_het,
    i_ref1 = [-1.0, np.inf]
)

iplot([fit, fit_twfe], coord_flip=False, figsize = (900, 400), title = "TWFE vs DID2S")
```
![](figures/event_study.svg)



## PyFixest 0.10.7

- Adds basic support for event study estimation via two-way fixed effects and Gardner's two-stage "Did2s" approach.
  This is a beta version and experimental. Further updates (i.e. proper event studies vs "only" ATTs) and a more flexible
  did2s front end will follow in future releases.

```python
%load_ext autoreload
%autoreload 2

from pyfixest.did.did import event_study
import pyfixest as pf
import pandas as pd
df_het = pd.read_csv("pyfixest/did/data/df_het.csv")

fit_twfe = event_study(
    data = df_het,
    yname = "dep_var",
    idname= "state",
    tname = "year",
    gname = "g",
    estimator = "twfe"
)

fit_did2s = event_study(
    data = df_het,
    yname = "dep_var",
    idname= "state",
    tname = "year",
    gname = "g",
    estimator = "did2s"
)

pf.etable([fit_twfe, fit_did2s])
# | Coefficient   | est1             | est2             |
# |:--------------|:-----------------|:-----------------|
# | ATT           | 2.135*** (0.044) | 2.152*** (0.048) |
# Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001
```

## PyFixest 0.10.6

- Adds an `etable()` function that outputs markdown, latex or a pd.DataFrame.

## PyFixest 0.10.5

- Fixes a big in IV estimation that would trigger an error. See [here](https://github.com/py-econometrics/pyfixest/issues/197) for details. Thanks to @aeturrell for reporting!

## PyFixest 0.10.4

- Implements a custom function to drop singleton fixed effects.
- Additional small performance improvements.

## PyFixest 0.10.3

- Allows for white space in the multiway clustering formula.
- Adds documentation for multiway clustering.

## PyFixest 0.10.2

- Adds support for two-way clustering.
- Adds support for CRV3 inference for Poisson regression.

## PyFixest 0.10.1

- Adapts the internal fixed effects demeaning criteron to match `PyHDFE's default.
- Adds Styfen as coauthor.

## PyFixest 0.10

- Multiple performance improvements.
- Most importantly, implements a custom demeaning algorithm in `numba` - thanks to Styfen Schaer (@styfenschaer),
  which leads to performance improvements of 5x or more:

```python
%load_ext autoreload
%autoreload 2

import numpy as np
import time
import pyhdfe
from pyfixest.demean import demean

np.random.seed(1238)
N = 10_000_000
x = np.random.normal(0, 1, 10*N).reshape((N,10))
f1 = np.random.choice(list(range(1000)), N).reshape((N,1))
f2 = np.random.choice(list(range(1000)), N).reshape((N,1))

flist = np.concatenate((f1, f2), axis = 1)
weights = np.ones(N)

algorithm = pyhdfe.create(flist)

start_time = time.time()
res_pyhdfe = algorithm.residualize(x)
end_time = time.time()
print(end_time - start_time)
# 26.04527711868286


start_time = time.time()
res_pyfixest, success = demean(x, flist, weights, tol = 1e-10)
# Calculate the execution time
end_time = time.time()
print(end_time - start_time)
#4.334428071975708

np.allclose(res_pyhdfe , res_pyfixest)
# True
```



## PyFixest 0.9.11

- Bump required `formulaic` version to `0.6.5`.
- Stop copying the data frame in `fixef()`.

## PyFixest 0.9.10

- Fixes a big in the `wildboottest` method (see [#158](https://github.com/py-econometrics/pyfixest/issues/158)).
- Allows to run a wild bootstrap after fixed effect estimation.

## PyFixest 0.9.9

- Adds support for `wildboottest` for Python `3.11`.

## PyFixest 0.9.8

- Fixes a couple more bugs in the `predict()` and `fixef()` methods.
- The `predict()` argument `data` is renamed to `newdata`.

## PyFixest 0.9.7

Fixes a bug in `predict()` produced when multicollinear variables are dropped.

## PyFixest 0.9.6

Improved Collinearity handling. See [#145](https://github.com/py-econometrics/pyfixest/issues/145)

## PyFixest 0.9.5


- Moves plotting from `matplotlib` to `lets-plot`.
- Fixes a few minor bugs in plotting and the `fixef()` method.


## PyFixest 0.9.1

### Breaking API changes

It is no longer required to initiate an object of type `Fixest` prior to running [Feols](/reference/estimation.models.feols_.Feols.qmd) or `fepois`. Instead,
you can now simply use `feols()` and `fepois()` as functions, just as in `fixest`. Both function can be found in an
`estimation` module and need to obtain a `pd.DataFrame` as a function argument:

```py
from pyfixest.estimation import fixest, fepois
from pyfixest.utils import get_data

data = get_data()
fit = feols("Y ~ X1 | f1", data = data, vcov = "iid")
```

Calling `feols()` will return an instance of class [Feols](/reference/estimation.models.feols_.Feols.qmd), while calling `fepois()` will return an instance of class `Fepois`.
Multiple estimation syntax will return an instance of class `FixestMulti`.

Post processing works as before via `.summary()`, `.tidy()` and other methods.

### New Features

A summary function allows to compare multiple models:

```py
from pyfixest.summarize import summary
fit2 = feols("Y ~ X1 + X2| f1", data = data, vcov = "iid")
summary([fit, fit2])
```

Visualization is possible via custom methods (`.iplot()` & `.coefplot()`), but a new module allows to visualize
  a list of [Feols](/reference/estimation.models.feols_.Feols.qmd) and/or `Fepois` instances:

```py
from pyfixest.visualize import coefplot, iplot
coefplot([fit, fit2])
```

The documentation has been improved (though there is still room for progress), and the code has been cleaned up a
bit (also lots of room for improvements).

---

# compare-fixest-pyfixest.html

This vignette compares estimation results from `fixest` with `pyfixest` via the `rpy2` package.

## Setup


```{python}
import pandas as pd
import rpy2.robjects as ro
from rpy2.robjects import pandas2ri
from rpy2.robjects.packages import importr

import pyfixest as pf

# Activate pandas2ri
pandas2ri.activate()

# Import R packages
fixest = importr("fixest")
stats = importr("stats")
broom = importr("broom")

# IPython magic commands for autoreloading
%load_ext autoreload
%autoreload 2

# Get data using pyfixest
data = pf.get_data(model="Feols", N=10_000, seed=99292)
```


## Ordinary Least Squares (OLS)

### IID Inference

First, we estimate a model via `pyfixest. We compute "iid" standard errors.


```{python}
fit = pf.feols(fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov="iid")
```

We estimate the same model with weights:


```{python}
fit_weights = pf.feols(
    fml="Y ~ X1 + X2 | f1 + f2", data=data, weights="weights", vcov="iid"
)
```

Via `r-fixest` and `rpy2`, we get


```{python}
r_fit = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov="iid",
)

r_fit_weights = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    weights=ro.Formula("~weights"),
    vcov="iid",
)
```

    R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).

    R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).



Let's compare how close the covariance matrices are:


```{python}
fit_vcov = fit._vcov
r_vcov = stats.vcov(r_fit)
fit_vcov - r_vcov
```


And for WLS:


```{python}
fit_weights._vcov - stats.vcov(r_fit_weights)
```

We conclude by comparing all estimation results via the `tidy` methods:

```{python}
fit.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit)).T
```

```{python}
fit_weights.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit_weights)).T
```


### Heteroskedastic Errors

We repeat the same exercise with heteroskedastic (HC1) errors:


```{python}
fit = pf.feols(fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov="hetero")
fit_weights = pf.feols(
    fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov="hetero", weights="weights"
)
```


```{python}
r_fit = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov="hetero",
)

r_fit_weights = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    weights=ro.Formula("~weights"),
    vcov="hetero",
)
```

As before, we compare the variance covariance matrices:


```{python}
fit._vcov - stats.vcov(r_fit)
```

```{python}
fit_weights._vcov - stats.vcov(r_fit_weights)
```

We conclude by comparing all estimation results via the `tidy` methods:

```{python}
fit.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit)).T
```

```{python}
fit_weights.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit_weights)).T
```


### Cluster-Robust Errors

We conclude with cluster robust errors.


```{python}
fit = pf.feols(fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov={"CRV1": "f1"})
fit_weights = pf.feols(
    fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov={"CRV1": "f1"}, weights="weights"
)

r_fit = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov=ro.Formula("~f1"),
)
r_fit_weights = fixest.feols(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    weights=ro.Formula("~weights"),
    vcov=ro.Formula("~f1"),
)
```

```{python}
fit._vcov - stats.vcov(r_fit)
```

```{python}
fit_weights._vcov - stats.vcov(r_fit_weights)
```

We conclude by comparing all estimation results via the `tidy` methods:

```{python}
fit.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit)).T
```

```{python}
fit_weights.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(r_fit_weights)).T
```

## Poisson Regression


```{python}
data = pf.get_data(model="Fepois")
```


```{python}
fit_iid = pf.fepois(fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov="iid", iwls_tol=1e-10)
fit_hetero = pf.fepois(
    fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov="hetero", iwls_tol=1e-10
)
fit_crv = pf.fepois(
    fml="Y ~ X1 + X2 | f1 + f2", data=data, vcov={"CRV1": "f1"}, iwls_tol=1e-10
)

fit_r_iid = fixest.fepois(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov="iid",
)

fit_r_hetero = fixest.fepois(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov="hetero",
)

fit_r_crv = fixest.fepois(
    ro.Formula("Y ~ X1 + X2 | f1 + f2"),
    data=data,
    vcov=ro.Formula("~f1"),
)
```

```{python}
fit_iid._vcov - stats.vcov(fit_r_iid)
```

```{python}
fit_hetero._vcov - stats.vcov(fit_r_hetero)
```

```{python}
fit_crv._vcov - stats.vcov(fit_r_crv)
```

We conclude by comparing all estimation results via the `tidy` methods:


```{python}
fit_iid.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(fit_r_iid)).T
```

```{python}
fit_hetero.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(fit_r_hetero)).T
```

```{python}
fit_crv.tidy()
```

```{python}
pd.DataFrame(broom.tidy_fixest(fit_r_crv)).T
```

---

# contributing.html

## Overview

Thanks for showing interest in contributing to `pyfixest`! We appreciate all
contributions and constructive feedback, whether that be reporting bugs, requesting
new features, or suggesting improvements to documentation.

If you'd like to get involved, but are not yet sure how, please feel free to send us an [email](alexander-fischer1801@t-online.de). Some familiarity with
either Python or econometrics will help, but you really don't need to be a `numpy` core developer or have published in [Econometrica](https://onlinelibrary.wiley.com/journal/14680262) =) We'd be more than happy to invest time to help you get started!

::: {.callout-note}
## PyFixest Sprint in Heilbronn

We're hosting a [PyFixest Sprint](pyfixest-sprint.md) with [AppliedAI](https://www.appliedai.de/) in late February/early March 2026. If you're interested in contributing to PyFixest in person, [learn more and get in touch](pyfixest-sprint.md)!
:::

For a comprehensive overview of the codebase architecture and internals, check out the [DeepWiki](https://deepwiki.com/py-econometrics/pyfixest). While not perfect and correct in all regards, we think it is a pretty good starting point to learn about the codebase!

## Quick Start with GitHub Codespaces

If you are new to open source / Python development, the fastest way to start contributing might be with **GitHub Codespaces**.
You start by forking the repository. You can then launch a codespace by clicking

[![Open in Codespaces](https://img.shields.io/badge/Open%20in-Codespaces-blue?logo=github)](https://github.com/py-econometrics/pyfixest/codespaces)

This will launch a github codespace (there is a free tier for 60h a month, thank you github!).

If you run

```{.bash .code-copy}
# set up env and install dependencies
pixi install -e dev
# compile rust dependencies
pixi run -e dev maturin-develop
```
`pixi` will install the development environment and all dependencies.

Now, create a new Python script `debug.py` at the root of the repository and
paste the following:

```{python}
import pyfixest as pf

data = pf.get_data()
fit = pf.feols("Y ~ X1", data=data)
```

You can now run the script by typing

```{.bash .code-copy}
pixi r -e dev python3 debug.py
```

or run unit tests

```{.bash .code-copy}
pixi run tests-regular
```

After changing the docs, you can build them by running

```{.bash .code-copy}
# compile rust in docs environment
pixi run -e dev maturin-develop
# build, render, preview docs
docs-build
docs-render
docs-preview
```

Note that the entire process might take a few minutes.

## Reporting bugs

We use [GitHub issues](https://github.com/py-econometrics/pyfixest/issues) to track bugs. You can report a bug by opening a new issue or contribute to an existing issue if
related to the bug you are reporting.

Before creating a bug report, please check that your bug has not already been reported, and that your bug exists on the latest version of pyfixest. If you find a closed issue that seems to report the same bug you're experiencing, open a new issue and include a link to the original issue in your issue description.

Please include as many details as possible in your bug report. The information helps the maintainers resolve the issue faster.

## Suggesting enhancements

We use [GitHub issues](https://github.com/py-econometrics/pyfixest/issues?q=is%3Aissue+is%3Aopen+label%3Aenhancement) to track bugs and suggested enhancements. You can suggest an enhancement by opening a new feature request. Before creating an enhancement suggestion, please check that a similar issue does not already exist.

Please describe the behavior you want and why, and provide examples of how pyfixest would be used if your feature were added.

## Contributing to the codebase

### Setting up your local environment

Would you like to contribute to pyfixest, or run some of the unit tests locally? Awesome!
Here's how you can get started:

First, you'll want to fork the pyfixest GitHub repository. Then, clone your forked repo with git:

```{.bash .code-copy}
git clone https://github.com/<username>/pyfixest.git
cd pyfixest
```

To work on pyfixest, you'll need Python and R installed. If you're planning to work on the documentation, be sure to have Quarto installed as well.
Note: an R installation is only needed if you plan to run the unit tests locally.

For guidance on installing R and Python, check out the sections below.

### Package Management via `pixi`


`PyFixest` is using [pixi](https://pixi.sh/latest/).

To install `pixi`, just follow the [installation instructions](https://pixi.sh/latest/#installation) from the `pixi` documentation.

Once `pixi` is installed, you can initialize the project environment and install all dependencies with

```{.bash .code-copy}
cd path-to-pyfixest
pixi install
```

After installation, you can activate a custom `pixi` environment for `pyfixest` by typing:

```{.bash .code-copy}
pixi shell
```

You'll now be in the `pixi` environment and ready to go!

For most development tasks, it’s best to activate the development environment since it includes all the necessary dependencies for development.

```{.bash .code-copy}
pixi shell --environment dev    # open the dev environment
```

### `Pixi` tasks

To help with development, we've included several handy pixi tasks.

For example, we use `ruff` and `pre-commit` to ensure code consistency across the project.

To run (and install) all pre-commit hooks, you can run

```{.bash .code-copy}
pixi run -e lint pre-commit
```

We’ve included other tasks to help with testing. Almost all the necessary dependencies to run tests are included in the dev environment,
except for R packages unavailable through conda-forge.

In a first step, we need to compile the provided Rust code, which we can do by running

```{.bash .code-copy}
pixi run -e dev maturin-develop
```

Note that you do not require a global Rust installation - we install Rust via conda!

We can then run all dev tasks:

```{.bash .code-copy}
# attempt to install non-conda R dependencies
pixi run install-r-extended
# run all tests via pytest
pixi run -e dev tests
# run all tests excluding very computationally demanding tests or R-based tests
pixi run -e dev tests-regular
# run all -e dev tests that depend on the extra R dependencies
# rerun failed tests
pixi run -e dev tests-against-r-extended
pixi run -e dev tests-rerun
```

Building the documentation is also straightforward. We’ve got tasks to build, render, and preview the docs. As before,
we first need to compile the Rust extensions, this time in the docs env, by running

```{.bash .code-copy}
pixi run -e docs maturin-develop
```

Then we can build the documentation with quartodoc.

```{.bash .code-copy}
# Build documentation and website
pixi run -e docs docs-build
# render the docs
pixi run -e docs docs-render
# preview the docs
pixi run -e docs docs-preview
```

Keep in mind that you’ll need quarto installed to build the documentation locally.

## Installing Python, R and Quarto

#### Installing Python

The minimal Python version to develop `pyfixest` is `3.9`. You can installed it on Mac/Linux via [Hombrew](https://brew.sh/):

```{.bash .code-copy}
brew install python@3.11 # specify the version of python you prefer
```

On Windows via [Winget](https://winget.run/pkg/Python/Python.3.11):
```{.bash .code-copy}
winget install -e --id Python.Python.3.11
```

### Installing R

Note that R and R dependencies available through conda-forge are installed by pixi to the local project if you use the dev environment.
Some extra R dependencies may require additional development tools not included in the environment, for example:

Depending on your local set up, you may need to install additional libraries to compile those extra dependencies, like a version of `gcc` and `cmake`.

### Installing Quarto

Documentation for `pyfixest` is written, compiled, and published using Quarto.

To install Quarto, run:

On MacOS via [Homebrew](https://formulae.brew.sh/cask/quarto#default):

```{.bash .code-copy}
brew install --cask quarto
```

On Linux (Ubuntu using `gdebi`):

```{.bash .code-copy}
sudo curl -o quarto-linux-amd64.deb -L <https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb>
sudo gdebi quarto-linux-amd64.deb
```

On Windows:

```{.bash .code-copy}
scoop bucket add extras
scoop install extras/quarto
```

---

# difference-in-differences.html

`PyFixest` supports event study designs via the canonical two-way fixed effects design, the 2-Step imputation estimator, and local projections.

See also [NBER SI methods lectures on Linear Panel Event Studies](https://www.nber.org/conferences/si-2023-methods-lectures-linear-panel-event-studies).

## Setup

```{python}
from importlib import resources

import pandas as pd

import pyfixest as pf
from pyfixest.report.utils import rename_event_study_coefs
from pyfixest.utils.dgps import get_sharkfin

%load_ext watermark
%watermark --iversions
%load_ext autoreload
%autoreload 2
```



```{python}
# one-shot adoption data - parallel trends is true
df_one_cohort = get_sharkfin()
df_one_cohort.head()
```



```{python}
# multi-cohort adoption data
df_multi_cohort = pd.read_csv(
    resources.files("pyfixest.did.data").joinpath("df_het.csv")
)
df_multi_cohort.head()
```


## Examining Treatment Timing

Before any DiD estimation, we need to examine the treatment timing, since it is crucial to our choice of estimator.


```{python}
pf.panelview(
    df_one_cohort,
    unit="unit",
    time="year",
    treat="treat",
    collapse_to_cohort=True,
    sort_by_timing=True,
    ylab="Cohort",
    xlab="Year",
    title="Treatment Assignment Cohorts",
    figsize=(6, 5),
)
```



```{python}
pf.panelview(
    df_multi_cohort,
    unit="unit",
    time="year",
    treat="treat",
    collapse_to_cohort=True,
    sort_by_timing=True,
    ylab="Cohort",
    xlab="Year",
    title="Treatment Assignment Cohorts",
    figsize=(6, 5),
)
```


We immediately see that we have staggered adoption of treatment in the second case, which implies that a naive application of 2WFE might yield biased estimates under substantial effect heterogeneity.

We can also plot treatment assignment in a disaggregated fashion, which gives us a sense of cohort sizes.


```{python}
pf.panelview(
    df_multi_cohort,
    unit="unit",
    time="year",
    treat="treat",
    sort_by_timing=True,
    ylab="Unit",
    xlab="Year",
    title="Treatment Assignment (all units)",
    figsize=(6, 5),
)
```

## Inspecting the Outcome Variable

`pf.panelview()` further allows us to inspect the "outcome" variable over time:


```{python}
#| fig-width: 0.4
#| fig-height: 0.1

pf.panelview(
    df_multi_cohort,
    outcome="dep_var",
    unit="unit",
    time="year",
    treat="treat",
    collapse_to_cohort=True,
    title="Outcome Plot",
    legend=True,
    figsize=(7, 2.5),
)
```


We immediately see that the first cohort is switched into treatment in 2000, while the second cohort is switched into treatment by 2010.
Before each cohort is switched into treatment, the trends are parallel.

We can additionally inspect individual units by dropping the collapse_to_cohort argument. Because we have a large sample, we might want to inspect only a subset
of units.


```{python}
#| fig-width: 4
#| fig-height: 1

pf.panelview(
    df_multi_cohort,
    outcome="dep_var",
    unit="unit",
    time="year",
    treat="treat",
    subsamp=100,
    title = "Outcome Plot",
    legend=True,
    figsize=(7, 2.5),
)
```


## One-shot adoption: Static and Dynamic Specifications

After taking a first look at the data, let's turn to estimation. We return to the `df_one_cohort` data set (without staggered treatment rollout).


```{python}
fit_static_twfe = pf.feols(
    "Y ~ treat | unit + year",
    df_one_cohort,
    vcov={"CRV1": "unit"},
)
fit_static_twfe.summary()
```

Since this is a single-cohort dataset, this estimate is consistent for the ATT under parallel trends. We can estimate heterogeneous effects by time by interacting time with the treated group:


```{python}
fit_dynamic_twfe = pf.feols(
    "Y ~ i(year, ever_treated,  ref = 14) | unit + year",
    df_one_cohort,
    vcov={"CRV1": "unit"},
)
```


```{python}
fit_dynamic_twfe.iplot(
    coord_flip=False,
    title="Event Study",
    figsize=[1200, 400],
    yintercept=0,
    xintercept=13.5,
    labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),
)
```


Event study plots like this are very informative, as they allow us to visually inspect the parallel trends assumption and also the dynamic effects of the treatment.

Based on a cursory glance, one would conclude that parallel trends does not hold because one of the pre-treatment coefficient has a confidence interval that does not include zero. However, we know that parallel trends is true because the treatment is randomly assigned in the underlying DGP.

## Pointwise vs Simultaneous Inference in Event Studies

This is an example of a false positive in testing for pre-trends produced by _pointwise_ inference (where each element of the coefficient vector is tested separately).

As an alternative, we can use simultaneous confidence bands of the form $[a, b] = ([a_k, b_k])_{k=1}^K$ such that

$$
P(\beta \in [a, b]) = P(\beta_k \in [a_k, b_k] \forall k) \rightarrow 1 - \alpha
$$

These bands can be constructed by using a carefully chosen critical value $c$ that [accounts for the covariance between coefficients using the multiplier bootstrap](https://www.annualreviews.org/docserver/fulltext/statistics/10/1/annurev-statistics-040120-022239.pdf?expires=1724543273&id=id&accname=guest&checksum=0D11ADF816FFFA0AE21BD7EDC6DB1801#page=14). In pointwise inference, the critical value is $c = z_{1 - \alpha/2} = 1.96$ for $\alpha = 0.05$; the corresponding critical value for simultaneous inference is typically larger. These are also known as `sup-t` bands in the literature (see lec 3 of the NBER SI methods lectures linked above).

This is implemented in the `confint(joint=True)` method in the `feols` class. If we pass the `joint='both'` argument to `iplot`, we get the simultaneous confidence bands (for all event study coefficients) in addition to the pointwise confidence intervals. Note that simultaneous inference for all event study coefficients may be overly conservative, especially when the number of coefficients is large; one may instead choose to perform joint inference for [all pre-treatment coefficients and all post-treatment coefficients separately](https://gist.github.com/apoorvalal/8a7687d3620577fd5214f1d43fc740b3).


```{python}
fit_dynamic_twfe.iplot(
    coord_flip=False,
    title="Event Study",
    figsize=[1200, 400],
    yintercept=0,
    xintercept=13.5,
    joint="both",
    labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),
)
```


The joint confidence bands are wider than the pointwise confidence intervals, and they include zero for all pre-treatment coefficients. This is consistent with the parallel trends assumption.

## Event Study under Staggered Adoption via `feols()`, `event_study()`, `did2s()`,  `lpdid()`

We now return to the data set with staggered treatment rollout, `df_multi_cohort`.

### Two-Way Fixed Effects

As a baseline model, we can estimate a simple two-way fixed effects DiD regression via `feols()`:


```{python}
fit_twfe = pf.feols(
    "dep_var ~ i(rel_year, ref=-1.0) | state + year",
    df_multi_cohort,
    vcov={"CRV1": "state"},
)
```

You can also estimate a TWFE model via the `event_study()` function, which aims to provide a common interface to multiple
difference-in-differences implementations:

```{python}
fit_twfe_event = pf.event_study(
    data=df_multi_cohort,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    estimator="twfe",
)
```

### Fully-Interacted / Saturated Event Study (Sun-Abraham)

In a similar spirit, you can fit a fully-interacted difference-in-differences model by selecting the `estimator = "saturated"`:

```{python}
fit_saturated = pf.event_study(
    data=df_multi_cohort,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    estimator="saturated",
)

fit_saturated.iplot()
```

We can obtain treatment effects by period via the `aggregate()` method

```{python}
fit_saturated.aggregate(weighting = "shares")
```

and plot the effects

```{python}
fit_saturated.iplot_aggregate(weighting = "shares")
```

### When can we get away with using the two-way fixed effects regression?

We will motivate this section by lazily quoting the abstract of [Lal (2025)](https://arxiv.org/abs/2503.05125):

> The use of the two-way fixed effects regression in empirical social science was historically motivated by folk wisdom that it uncovers the Average Treatment effect on the Treated (ATT) as in the canonical two-period two-group case. This belief has come under scrutiny recently due to recent results in applied econometrics showing that it fails to uncover meaningful averages of heterogeneous treatment effects in the presence of effect heterogeneity over time and across adoption cohorts, and several heterogeneity-robust alternatives have been proposed. However, these estimators often have higher variance and are therefore under-powered for many applications, which poses a bias-variance tradeoff that is challenging for researchers to navigate. In this paper, we propose simple tests of linear restrictions that can be used to test for differences in dynamic treatment effects over cohorts, which allows us to test for when the two-way fixed effects regression is likely to yield biased estimates of the ATT.

You can employ the proposed test after running a saturated event study by calling the `test_treatment_heterogeneity()` method:

```{python}
fit_saturated.test_treatment_heterogeneity()
```

In this case, we might be willing to rely on the simple TWFE model to produce unbiased estimates. If we're not, two "new" difference-in-differences
estimators are implemented (beyond the already-presented saturated Sun-Abraham approach) that produce unbiased estimates under staggered assignment and heterogeneous treatment effects: Gardner's 2-Step Estimator and the Local Projections estimator from Dube et al.

### Gardner's 2-Step Estimator

To do the same via Gardners 2-stage estimator, we employ the the `pf.did2s()` function:

```{python}
fit_did2s = pf.did2s(
    df_multi_cohort,
    yname="dep_var",
    first_stage="~ 0 | unit + year",
    second_stage="~i(rel_year,ref=-1.0)",
    treatment="treat",
    cluster="state",
)
```

### Local Projections (Dube et al)

Last, we can estimate the ATT for each time period via local projections by using the `lpdid()` function:

```{python}
fit_lpdid = pf.lpdid(
    data=df_multi_cohort,
    yname="dep_var",
    gname="g",
    tname="year",
    idname="unit",
    vcov={"CRV1": "state"},
    pre_window=-20,
    post_window=20,
    att=False,
)
```

Let's look at some results:


```{python}
figsize = [1200, 400]
```


```{python}
fit_twfe.iplot(
    coord_flip=False,
    title="TWFE-Estimator",
    figsize=figsize,
    xintercept=18.5,
    yintercept=0,
    labels=rename_event_study_coefs(fit_twfe._coefnames),  # rename coefficients
).show()
```


```{python}
fit_lpdid.iplot(
    coord_flip=False,
    title="Local-Projections-Estimator",
    figsize=figsize,
    yintercept=0,
    xintercept=18.5,
).show()
```

What if we are not interested in the ATT per treatment period, but in a pooled effects?


```{python}
fit_twfe = pf.feols(
    "dep_var ~ i(treat) | unit + year",
    df_multi_cohort,
    vcov={"CRV1": "state"},
)

fit_did2s = pf.did2s(
    df_multi_cohort,
    yname="dep_var",
    first_stage="~ 0 | unit + year",
    second_stage="~i(treat)",
    treatment="treat",
    cluster="state",
)

fit_lpdid = pf.lpdid(
    data=df_multi_cohort,
    yname="dep_var",
    gname="g",
    tname="year",
    idname="unit",
    vcov={"CRV1": "state"},
    pre_window=-20,
    post_window=20,
    att=True,
)
pd.concat(
    [
        fit_twfe.tidy().assign(estimator="TWFE"),
        fit_did2s.tidy().assign(estimator="DID2s"),
        fit_lpdid.tidy().assign(estimator="LPDID").drop("N", axis=1),
    ],
    axis=0,
)
```

---

# index.html

<meta http-equiv="refresh" content="0; url=pyfixest.html" />
<script>
  window.location.replace("pyfixest.html");
</script>

If you are not redirected automatically, go to
[PyFixest Documentation](pyfixest.html).

---

# marginaleffects.html

We can compute marginal effects and linear and non-linear hypothesis tests via the excellent [marginaleffects](https://github.com/vincentarelbundock/pymarginaleffects) package.


```{python}
from marginaleffects import hypotheses

import pyfixest as pf

data = pf.get_data()
fit = pf.feols("Y ~ X1 + X2", data=data)

fit.tidy()
```


Suppose we were interested in testing the hypothesis that $X_{1} = X_{2}$. Given the relatively large differences in coefficients and
small standard errors, we will likely reject the null that the two parameters are equal.

We can run the formal test via the `hypotheses` function from the `marginaleffects` package.


```{python}
hypotheses(fit, "X1 - X2 = 0")
```


And indeed, we reject the null of equality of coefficients: we get a p-value of zero and a confidence interval that does not contain 0.

## Non-Linear Hypothesis Tests: Ratio Estimates

We can also test run-linear hypotheses, in which case `marginaleffects` will automatically compute correct standard errors
based on the estimated covariance matrix and the Delta method. This is for example useful for computing inferential
statistics for the "relative uplift" in an AB test.

For the moment, let's assume that $X1$ is a randomly assigned treatment variable. As before, $Y$ is our variable / KPI of interest.

Under randomization, the model intercept measures the "baseline", i.e. the population average of $Y$ in the absence of treatment. To compute a relative uplift, we might compute


```{python}
(fit.coef().xs("X1") / fit.coef().xs("Intercept") - 1) * 100
```


So we have a really big negative treatment effect of around minus 212%! To conduct correct inference on this
ratio statistic, we need to use the delta method.


### The Multivariate Delta Method

In a nutshell, the delta method provides a way to approximate the asympotic distribution of any non-linear transformation $g()$ or one or more random variables.

In the case of the ratio statistics, this non-linear transformation can be denoted as $g(\theta_{1}, \theta_{2}) = \theta_{1} / \theta_{2}$.

Here's the **Delta Method theorem**:

First, we define $\theta = (\theta_{1}, \theta_{2})'$ and $\mu = (\mu_{1}, \mu_{2})'$.

By the law of large numbers, we know that

$$
\sqrt{N} (\theta - \mu) \rightarrow_{d} N(0_{2}, \Sigma_{2,2}) \text{ if } N \rightarrow \infty.
$$

By the **Delta Method**, we can then approximate the limit distribution of $g(\theta)$ as


$$
\sqrt{N}  (g(\theta) - g(\mu)) \rightarrow_{d} N(0_{1}, g'(\theta) \times \Sigma \times g(\theta)) \text{ if } N \rightarrow \infty.
$$.

[Here's a long derivation of how to use the the delta method for inference of ratio statistics.](https://stats.stackexchange.com/questions/291594/estimation-of-population-ratio-using-delta-method). The key steps from the formula above is to derive the expression for the asymptotic variance $ g'(\theta) \times \Sigma \times g(\theta)$.

But hey - we're lucky, because marginaleffects will do all this work for us: we don't have to derive analytic gradients ourselves =)

### Using the Delta Method via `marginaleffects`:

We can employ the Delta Method via `marginaleffects` via the `hypotheses` function:


```{python}
hypotheses(fit, "(X1 / Intercept - 1) * 100 = 0")
```

As before, we get an estimate of around -212%. Additionally, we obtain a 95% CI via the Delta Method of [-228%, -195%].

Besides hypopotheses testing, you can do a range of other cool things with the `marginaleffects` package.
For example (and likely unsurprisingly), you can easily compute all sorts of marginal effects for your regression models.
For all the details, we highly recommend to take a look
at the [marginaleffects zoo book!](https://marginaleffects.com/index.html).

---

# mixtape.html

# The Mixtape with PyFixest

In this notebook, we translate some of the Python code in [Scott Cunningham's mixtape](https://mixtape.scunning.com/) to PyFixest.

```python
import numpy as np
import pandas as pd

import pyfixest as pf

%config InlineBackend.figure_format = "retina"
```

## Chapter 8: Panel Data

Instead of demeaning by hand and then fitting the model via statsmodels, we just let PyFixest do all the work for us. 

```python
# read the data from github & load into pandas
url = "https://raw.githubusercontent.com/scunning1975/mixtape/master/sasp_panel.dta"
sasp = pd.read_stata(url)
sasp.head()
```

```text
      id  session   age  age_cl  appearance_cl        bmi  schooling  asq_cl  \
0  243.0      2.0  27.0    30.0            5.0        NaN       11.0   900.0   
1  397.0      4.0  28.0    56.0            5.0  28.971931       16.0  3136.0   
2  598.0      4.0  50.0    52.0            6.0  21.453857       16.0  2704.0   
3   28.0      1.0  41.0    72.0            5.0  24.028320       12.0  5184.0   
4   28.0      4.0  41.0    46.0            8.0  24.028320       12.0  2116.0   

  provider_second  asian_cl  ...  hispanic  other  white     asq  cohab  \
0           1. No       0.0  ...       0.0    0.0    0.0   729.0    1.0   
1           1. No       0.0  ...       0.0    0.0    1.0   784.0    0.0   
2           1. No       0.0  ...       0.0    0.0    1.0  2500.0    0.0   
3           1. No       0.0  ...       0.0    0.0    1.0  1681.0    0.0   
4           1. No       0.0  ...       0.0    0.0    1.0  1681.0    0.0   

   married  divorced  separated  nevermarried  widowed  
0      0.0       0.0        0.0           0.0      0.0  
1      1.0       0.0        0.0           0.0      0.0  
2      0.0       1.0        0.0           0.0      0.0  
3      0.0       1.0        0.0           0.0      0.0  
4      0.0       1.0        0.0           0.0      0.0  

[5 rows x 31 columns]
```

```python
# some initial data cleaning
sasp = sasp.dropna()
# order by id and session
sasp.sort_values("id", inplace=True)

# create balanced panel
times = len(sasp.session.unique())
in_all_times = (
    sasp.groupby("id")["session"].apply(lambda x: len(x) == times).reset_index()
)
in_all_times.rename(columns={"session": "in_all_times"}, inplace=True)
balanced_sasp = pd.merge(in_all_times, sasp, how="left", on="id")
balanced_sasp = balanced_sasp[balanced_sasp.in_all_times]

provider_second = np.zeros(balanced_sasp.shape[0])
provider_second[balanced_sasp.provider_second == "2. Yes"] = 1
balanced_sasp.provider_second = provider_second
```

```python
# define formulas

covars = """
    age + asq + bmi + hispanic + black + other + asian + schooling + cohab +
            married + divorced + separated + age_cl + unsafe + llength + reg + asq_cl +
            appearance_cl + provider_second + asian_cl + black_cl + hispanic_cl +
           othrace_cl + hot + massage_cl
    """

# we fit on all covariates
fml_pooled = f"lnw ~ {covars}"
# we fit on all covariates and add one-hot encoded id fixed effects
fml_onehot = f"lnw ~  {covars} + C(id)"
# we fit on all covariates and swipe out the fixed effects (i.e. we apply the within transformation via pyfixest.feols)
fml_fe = f"lnw ~ {covars} | id"
```

```python
%%capture
fit_pooled = pf.feols(fml=fml_pooled, data=balanced_sasp, vcov={"CRV1": "id"})
fit_fe = pf.feols(fml=fml_fe, data=balanced_sasp, vcov={"CRV1": "id"})
```

```python
pf.etable(
    [fit_pooled, fit_fe],
    model_heads=["POLS", "FE"],
    keep=["unsafe", "llength", "reg"],
    labels={
        "unsafe": "Unprotected sex with client of any kind",
        "llength": "Ln(Length)",
        "reg": "Client was a Regular",
    },
    digits=6,
)
```

```text
GT(_tbl_data=  level_0                                  level_1  \
0    coef  Unprotected sex with client of any kind   
1    coef                               Ln(Length)   
2    coef                     Client was a Regular   
3      fe                                       id   
4   stats                             Observations   
5   stats                                S.E. type   
6   stats                            R<sup>2</sup>   
7   stats                     R<sup>2</sup> Within   

                              0                             1  
0      0.013407 <br> (0.042455)      0.051034 <br> (0.028283)  
1  -0.308251*** <br> (0.040905)  -0.434506*** <br> (0.024323)  
2     -0.047007 <br> (0.033282)    -0.037341* <br> (0.018761)  
3                             -                             x  
4                          1028                          1028  
5                        by: id                        by: id  
6                      0.302643                      0.832214  
7                             -                      0.515959  , _body=<great_tables._gt_data.Body object at 0x0000016CC0535D00>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x0000016CC0536600>, _spanners=Spanners([SpannerInfo(spanner_id='lnw', spanner_level=2, spanner_label='lnw', spanner_units=None, spanner_pattern=None, vars=['0', '1'], built=None), SpannerInfo(spanner_id='POLS', spanner_level=1, spanner_label='POLS', spanner_units=None, spanner_pattern=None, vars=['0'], built=None), SpannerInfo(spanner_id='FE', spanner_level=1, spanner_label='FE', spanner_units=None, spanner_pattern=None, vars=['1'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x0000016CC0534680>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo
...[truncated]...
```

Our point estimates match the Stata results that Scott reports in the mixtape exactly. The standard errors differ slightly due to differences in small sample adjustments in Stata and Pyfixest. See [here](https://pyfixest.org/ssc.html) for an overview of how pyfixest handles small sample adjustments (tldr - exactly like r-fixest). 

## Chapter 9: Difference-in-Differences

### Code Example 1

```python
abortion = pd.read_stata(
    "https://raw.githubusercontent.com/scunning1975/mixtape/master/abortion.dta"
)
abortion = abortion[~pd.isnull(abortion.lnr)]
abortion_bf15 = abortion[abortion.bf15 == 1]
# pf throws error when weights are 0
abortion_bf15 = abortion_bf15[abortion_bf15.totpop > 0]
abortion_bf15["year"] = abortion_bf15["year"].astype(int)
abortion_bf15.head()
```

```text
     fip   age  race  year  sex  totcase  totpop         rate      totrate  \
19   1.0  15.0   2.0  1985    2   5683.0  106187  6527.500000  5351.899902   
39   1.0  15.0   2.0  1986    2   5344.0  106831  6351.200195  5002.299805   
71   1.0  15.0   2.0  1987    2   4983.0  106496  5759.100098  4679.000000   
89   1.0  15.0   2.0  1988    2   5276.0  105238  6139.600098  5013.399902   
106  1.0  15.0   2.0  1989    2   5692.0  102956  5951.500000  5528.600098   

       id  ...  female       lnr    t  younger   fa   pi  wm15  wf15  bm15  \
19   14.0  ...     1.0  8.783779  1.0      1.0  1.0  0.0   0.0   0.0   0.0   
39   14.0  ...     1.0  8.756399  2.0      1.0  1.0  0.0   0.0   0.0   0.0   
71   14.0  ...     1.0  8.658537  3.0      1.0  1.0  1.0   0.0   0.0   0.0   
89   14.0  ...     1.0  8.722515  4.0      1.0  1.0  1.0   0.0   0.0   0.0   
106  14.0  ...     1.0  8.691399  5.0      1.0  1.0  1.0   0.0   0.0   0.0   

     bf15  
19    1.0  
39    1.0  
71    1.0  
89    1.0  
106   1.0  

[5 rows x 39 columns]
```

```python
# we use the i() operator pyfixest provides, as it allows us to easily set the
# reference year, and works smoothly with the iplot() method

fml = """lnr ~ i(year, repeal, ref = 1985) + C(repeal) + C(year) + C(fip)
        + acc + ir + pi + alcohol + crack + poverty + income + ur
"""

fit = pf.feols(fml=fml, data=abortion_bf15, weights="totpop", vcov={"CRV1": "fip"})

pf.iplot(
    fit,
    coord_flip=False,
    plot_backend="matplotlib",
    title="Event Study Estimate",
    cat_template="{value}",
)
```

```text
['C:\\Users\\alexa\\Documents\\pyfixest\\pyfixest\\estimation\\feols_.py:2759: UserWarning: \n', '            1 variables dropped due to multicollinearity.\n', "            The following variables are dropped: ['C(fip)[T.53.0]'].\n", '            \n', '  warnings.warn(\n']
```

```text
<Figure size 1000x600 with 1 Axes>
```

```python

```

```python

```

---

# multiple_testing.html

# Multiple Hypothesis Testing Corrections

When conducting online A/B tests or large-scale experiments, we often analyze multiple dependent variables simultaneously. Analyzing multiple KPIs introduces a significant statistical challenge: the multiple testing problem.

In classical hypothesis testing, the significance level $\alpha$ controls the probability of a false positive (Type I error), typically $\alpha=0.05$. While this ensures a 5% chance of a false positive for a single test, the likelihood of detecting at least one false positive grows rapidly when conducting multiple tests. For example, testing 20 KPIs independently at $\alpha = 0.05$ results in a 64% chance of at least one false positive—calculated as $1 - (1 - 0.05)^{20} \approx 0.64$. This issue, known as the multiple testing problem, can lead to false claims of significant effects when none exist.

To address this, the concept of controlling the **Familywise Error Rate (FWER)** has been widely adopted. FWER controls the probability of at least one Type I error across a family of hypotheses. Several correction methods exist to mitigate the multiple testing problem, including:

- **Bonferroni Correction**: A simple and conservative method that adjusts the significance level for each test by dividing $\alpha$ by the number of tests.
- **Romano-Wolf & Westfall-Young Stepwise Procedures**: Two more powerful methods that use resampling techniques to control the FWER.

This vignette demonstrates how these methods effectively control the FWER in a variety of scenarios. We will compare their performance and highlight the trade-offs between simplicity and statistical power. Specifically, we show that while Bonferroni provides strong error control, it is conservative in many practical applications. In contrast, Romano-Wolf and Westfall-Young methods are more powerful, offering greater sensitivity to detect true effects while maintaining robust control of the FWER.

## What is a Family-Wise Error Rate (FWER)? 

Suppose that we are running an experiment and want to test if our treatment impacts 20 different dependent variables (KPIs). For any given independent test, the chance of a false positive is given by the (significance) **level** of the individual test, which is most commonly set to $\alpha = 0.05$. Formally, we can define the false positive rate for a single hypothesis $H$ as:

$$
P(\text{reject } H \mid H \text{ is true}) = \alpha
$$

For a **family of tests** $S = {s \in \{1, \dots, P\} }$ hypotheses $\{H_{s}\}_{s \in S}$, we can analogously define the **family-wise error rate (FWER)** as:

$$
P(\text{reject at least one } H_{s} \text{ with } s \in I) = \alpha
$$

where $I$ is the set of **true hypotheses**. In other words, the FWER is the probability of making at least one false positive across all tests in the family.


## Setup

In a first step, we create a data set with multiple (potentially correlated) dependent variables that share a common set of covariates. 
All simulations in this notebook are greatly inspired by Clarke, Romano & Wolf (Stata Journal, 2020).

```python
%load_ext autoreload
%autoreload 2

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from great_tables import loc, style
from joblib import Parallel, delayed
from tqdm import tqdm

import pyfixest as pf
```

```python
N = 100
n_covariates = 5
n_depvars = 20
```

```python
def get_data(N, n_covariates, n_depvars, rho, seed):
    "Simulate data with true nulls."
    rng = np.random.default_rng(seed)
    Omega = np.eye(n_depvars)
    X = rng.standard_normal((N, n_covariates))
    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)
    beta = np.zeros((n_covariates, n_depvars))
    y = X @ beta + u_joint

    data = pd.DataFrame(X, columns=[f"X{i}" for i in range(n_covariates)])
    data = data.assign(**{f"y{i}": y[:, i] for i in range(n_depvars)})

    return data
```

```python
data = get_data(
    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345
)
data.head()
```

```text
         X0        X1        X2        X3        X4        y0        y1  \
0 -1.423825  1.263728 -0.870662 -0.259173 -0.075343  1.555991  0.761234   
1 -0.740885 -1.367793  0.648893  0.361058 -1.952863 -0.053753  0.043408   
2  2.347410  0.968497 -0.759387  0.902198 -0.466953 -0.933649  0.481619   
3 -0.060690  0.788844 -1.256668  0.575858  1.398979 -0.135890  0.329208   
4  1.322298 -0.299699  0.902919 -1.621583 -0.158189 -1.426639 -0.986879   

         y2        y3        y4  ...       y10       y11       y12       y13  \
0  1.888281 -0.614546  0.179550  ...  0.142083  1.689619 -0.425871  0.421775   
1  0.563138  1.088257 -0.219516  ... -0.882320  0.641363 -0.253652  0.888703   
2  0.793995  1.191231 -2.087805  ...  0.962981  3.198808  0.901849 -0.045545   
3  0.304811 -1.102051  0.800707  ...  2.080769  1.058883 -1.672165 -1.443892   
4 -0.140158  0.986182  1.498658  ...  0.109724 -0.463974  1.094400  1.542634   

        y14       y15       y16       y17       y18       y19  
0 -0.241513 -0.770971 -0.822431 -1.358922  0.817455  0.163253  
1  0.699996  0.984490  0.324864 -1.463119 -1.755173 -0.656597  
2 -0.408414 -0.127183  0.624903  1.565282 -0.480614  0.607006  
3 -0.250928 -1.014871 -2.315505  2.250437  1.324270  0.369040  
4 -0.336492 -1.932564  0.669541 -0.373858 -0.619859 -0.233117  

[5 rows x 25 columns]
```

Now that we have our data set at hand, we can fit 20 regression models - one for each dependent variable. To do so, we will use 
pyfixest's multiple estimation syntax.

```python
dependent_vars = " + ".join([f"y{i}" for i in range(20)])
independent_vars = " + ".join([f"X{i}" for i in range(5)])
fml = f"{dependent_vars} ~ {independent_vars}"
```

```python
fit = pf.feols(fml, data=data, vcov="hetero")
```

```python
(pf.etable(fit).tab_style(style=style.fill(color="yellow"), locations=loc.body(rows=1)))
```

```text
GT(_tbl_data=  level_0             level_1                    0                    1  \
0    coef                  X0  -0.069 <br> (0.104)  -0.019 <br> (0.111)   
1    coef                  X1   0.119 <br> (0.092)   0.182 <br> (0.118)   
2    coef                  X2   0.067 <br> (0.088)   0.088 <br> (0.094)   
3    coef                  X3   0.113 <br> (0.111)   0.100 <br> (0.119)   
4    coef                  X4  -0.027 <br> (0.105)   0.043 <br> (0.084)   
5    coef           Intercept  -0.136 <br> (0.105)  -0.091 <br> (0.090)   
6   stats        Observations                  100                  100   
7   stats           S.E. type               hetero               hetero   
8   stats       R<sup>2</sup>                0.026                0.050   
9   stats  Adj. R<sup>2</sup>               -0.025               -0.001   

                     2                     3                     4  \
0   0.090 <br> (0.093)    0.070 <br> (0.118)   -0.011 <br> (0.120)   
1   0.052 <br> (0.094)    0.096 <br> (0.119)   -0.036 <br> (0.109)   
2   0.082 <br> (0.071)    0.037 <br> (0.106)    0.042 <br> (0.103)   
3   0.012 <br> (0.082)   -0.006 <br> (0.101)   -0.072 <br> (0.111)   
4  -0.005 <br> (0.074)    0.090 <br> (0.114)  -0.198* <br> (0.094)   
5   0.041 <br> (0.086)  -0.246* <br> (0.106)    0.064 <br> (0.106)   
6                  100                   100                   100   
7               hetero                hetero                hetero   
8                0.026                 0.024                 0.047   
9               -0.025                -0.028                -0.004   

                     5                    6                    7  ...  \
0   0.061 <br> (0.102)   0.008 <br> (0.107)  -0.103 <br> (0.138)  ...   
1  -0.036 <br> (0.095)   0.024 <br> (0.100)   0.113 <br> (0.119)  ...   
2   0.123 <br> (0.110)   0.040 <br> (0.086)   0.099 <br> (0.105)  ...   
3  -0.127 <br> (0.112)   0.132 <br> (0.114)   0.206 <br> (0.147)  ...   
4   0.097 <br> (0.099)  -0.102 <br> (0.100)  -0.031 <br> (0.110)  ...   
5   0.174 <br> (0.112)  -0.053 <br> (0.101)  -0.016 <br> (0.120)  ...   
6                  100                  100                  100  ...   
7               hetero               hetero               hetero  ...   
8                0.035                0.024                0.046  ...   
9               -0.016               -0.028               -0.005  ...   

                     10                   11                   12  \
0    0.116 <br> (0.086)   0.070 <br> (0.139)  -0.169 <br> (0.099)   
1    0.085 <br> (0.087)  -0.148 <br> (0.102)   0.148 <br> (0.100)   
2  -0.171* <br> (0.075)  -0.198 <br> (0.109)   0.116 <br> (0.101)   
3   -0.067 <br> (0.085)   0.033 <br> (0.108)   0.023 <br> (0.109)   
4   0.220* <br> (0.093)   0.164 <br> (0.117)  -0.201 <br> (0.122)   
5    0.052 <br> (0.087)  -0.095 <br> (0.099)  -0.036 <br> (0.106)   
6                   100                  100                  100   
7                hetero               hetero               hetero   
8                 0.122                0.076                0.068   
9                 0.075                0.027                0.018   

                    13                    14                   15  \
0   0.152 <br> (0.112)    0.031 <br> (0.099)   0.147 <br> (0.106)   
1  -0.043 <br> (0.121)  -0.225* <br> (0.097)  -0.116 <br> (0.115)   
2   0.024 <br> (0.109)   -0.118 <br> (0.092)   0.013 <br> (0.126)   
3   0.021 <br> (0.100)    0.057 <br> (0.104)  -0.011 <br> (0.134)   
4  -0.025 <br> (0.120)   -0.069 <br> (0.095)  -0.146 <br> (0.100)   
5  -0.090 <br> (0.111)   -0.076 <br> (0.097)   0.150 <br> (0.114)   
6                  100                   100                  100   
7               hetero                hetero               hetero   
8                0.023                 0.080                0.049   
9               -0.029                 0.031               -0.002   

                    16                   17                   18  
...[truncated]...
```

We see that our estimation produces multiple false positives for the effect of X1 on multiple dependent variables. Recall that 
we had simulated X1 to have **no effect** on any of the dependent variables. Still, the estimation procedure produces a significant
effect for X1 on multiple dependent variables. 

`PyFixest` provides three functions to adjust inference for multiple testing: `pf.bonferroni()`, `pf.rwolf()`, and `pf.wyoung()`.
All three share a common API.

```python
pval_bonferroni = (
    pf.bonferroni(fit.to_list(), param="X1").xs("Bonferroni Pr(>|t|)").values
)

pval_rwolf = (
    pf.rwolf(fit.to_list(), param="X1", reps=1000, seed=22).xs("RW Pr(>|t|)").values
)

pval_wyoung = (
    pf.wyoung(fit.to_list(), param="X1", reps=1000, seed=22).xs("WY Pr(>|t|)").values
)
```

```python
(
    pf.etable(
        fit,
        custom_model_stats={
            "Bonferroni: pval(X1)": pval_bonferroni.round(4).tolist(),
            "RW: pval(X1)": pval_rwolf.round(4).tolist(),
            "WY: pval(X1)": pval_wyoung.round(4).tolist(),
        },
    ).tab_style(style=style.fill(color="yellow"), locations=loc.body(rows=[6, 7, 8]))
)
```

```text
GT(_tbl_data=   level_0               level_1                    0                    1  \
0     coef                    X0  -0.069 <br> (0.104)  -0.019 <br> (0.111)   
1     coef                    X1   0.119 <br> (0.092)   0.182 <br> (0.118)   
2     coef                    X2   0.067 <br> (0.088)   0.088 <br> (0.094)   
3     coef                    X3   0.113 <br> (0.111)   0.100 <br> (0.119)   
4     coef                    X4  -0.027 <br> (0.105)   0.043 <br> (0.084)   
5     coef             Intercept  -0.136 <br> (0.105)  -0.091 <br> (0.090)   
6    stats  Bonferroni: pval(X1)                  1.0                  1.0   
7    stats          RW: pval(X1)                0.968               0.9341   
8    stats          WY: pval(X1)                0.968                0.934   
9    stats          Observations                  100                  100   
10   stats             S.E. type               hetero               hetero   
11   stats         R<sup>2</sup>                0.026                0.050   
12   stats    Adj. R<sup>2</sup>               -0.025               -0.001   

                      2                     3                     4  \
0    0.090 <br> (0.093)    0.070 <br> (0.118)   -0.011 <br> (0.120)   
1    0.052 <br> (0.094)    0.096 <br> (0.119)   -0.036 <br> (0.109)   
2    0.082 <br> (0.071)    0.037 <br> (0.106)    0.042 <br> (0.103)   
3    0.012 <br> (0.082)   -0.006 <br> (0.101)   -0.072 <br> (0.111)   
4   -0.005 <br> (0.074)    0.090 <br> (0.114)  -0.198* <br> (0.094)   
5    0.041 <br> (0.086)  -0.246* <br> (0.106)    0.064 <br> (0.106)   
6                   1.0                   1.0                   1.0   
7                 0.995                 0.995                   1.0   
8                 0.995                 0.995                   1.0   
9                   100                   100                   100   
10               hetero                hetero                hetero   
11                0.026                 0.024                 0.047   
12               -0.025                -0.028                -0.004   

                      5                    6                    7  ...  \
0    0.061 <br> (0.102)   0.008 <br> (0.107)  -0.103 <br> (0.138)  ...   
1   -0.036 <br> (0.095)   0.024 <br> (0.100)   0.113 <br> (0.119)  ...   
2    0.123 <br> (0.110)   0.040 <br> (0.086)   0.099 <br> (0.105)  ...   
3   -0.127 <br> (0.112)   0.132 <br> (0.114)   0.206 <br> (0.147)  ...   
4    0.097 <br> (0.099)  -0.102 <br> (0.100)  -0.031 <br> (0.110)  ...   
5    0.174 <br> (0.112)  -0.053 <br> (0.101)  -0.016 <br> (0.120)  ...   
6                   1.0                  1.0                  1.0  ...   
7                   1.0                  1.0                0.995  ...   
8                   1.0                  1.0                0.995  ...   
9                   100                  100                  100  ...   
10               hetero               hetero               hetero  ...   
11                0.035                0.024                0.046  ...   
12               -0.016               -0.028               -0.005  ...   

                      10                   11                   12  \
0     0.116 <br> (0.086)   0.070 <br> (0.139)  -0.169 <br> (0.099)   
1     0.085 <br> (0.087)  -0.148 <br> (0.102)   0.148 <br> (0.100)   
2   -0.171* <br> (0.075)  -0.198 <br> (0.109)   0.116 <br> (0.101)   
3    -0.067 <br> (0.085)   0.033 <br> (0.108)   0.023 <br> (0.109)   
4    0.220* <br> (0.093)   0.164 <br> (0.117)  -0.201 <br> (0.122)   
5     0.052 <br> (0.087)  -0.095 <br> (0.099)  -0.036 <br> (0.106)   
6                    1.0                  1.0                  1.0   
7                  0.995               0.9491               0.9491   
8                  0.995                0.949                0.949   
9                    100                  100                  100   
10                hetero               hetero               hetero   
11                 0.1
...[truncated]...
```

We quickly see that the corrected p-values do not flag any false positives. 

## Controlling for the Familiy-Wise Error Rate (FWER)

We now show by means of simulation that the three methods control the family-wise error rate (FWER). To do so, we simulate 1000 
data sets imposing true nulls for the effect of X1 on all of 20 created dependent variables. 
For each simulation, we then count if the methods flag more than one false positive and report our results. 

```python
def compute_family_rejection(seed, rho):
    "Simulate data, estimate models, and compute family rejection rates."
    data = get_data(N, n_covariates, n_depvars, rho, seed=seed)
    fit = pf.feols(fml=fml, data=data, vcov="hetero")
    df = fit.tidy().reset_index().set_index("Coefficient").xs("X1")

    df["Pr(>|t|) reject"] = df["Pr(>|t|)"] < 0.05
    df["Bonferroni reject"] = (
        pf.bonferroni(fit, param="X1").xs("Bonferroni Pr(>|t|)").values < 0.05
    )
    df["rwolf reject"] = (
        pf.rwolf(fit, param="X1", reps=1000, seed=seed * 11).xs("RW Pr(>|t|)").values
        < 0.05
    )
    df["wyoung reject"] = (
        pf.wyoung(fit, param="X1", reps=1000, seed=seed * 11).xs("WY Pr(>|t|)").values
        < 0.05
    )

    # Compute family rejection means
    family_rejection = {
        "Pr(>|t|) reject family": df["Pr(>|t|) reject"].sum() > 0,
        "Bonferroni reject family": df["Bonferroni reject"].sum() > 0,
        "rwolf reject family": df["rwolf reject"].sum() > 0,
        "wyoung reject family": df["wyoung reject"].sum() > 0,
    }

    return pd.Series(family_rejection)
```

```python
def run_fwer_simulation(n_iter, rho):
    "Run simulation for family-wise error rate."
    results = Parallel(n_jobs=-1)(
        delayed(compute_family_rejection)(seed, rho=rho) for seed in tqdm(range(n_iter))
    )
    return pd.concat(results).reset_index().groupby("index").mean()
```

```python
run_fwer_simulation(n_iter=1000, rho=0.5)
```

```text
['  8%|▊         | 80/1000 [03:34<38:48,  2.53s/it]  ']
```

We see that all three correction methods get close to the desired 5% level. In contrast, the uncorrected method produces the expected much higher family-wise error rate. 

## Power 

Now that we've seen that all three methods effectively handle false positives, let's see how well they avoid **false negatives**. 
In other words, we will study how powerful all three methods are in detecting true effects. 

To do so, we slightly have to adjust the data generating process. Instead of simulating the impact of X1 on all dependent variables 
to be zero (a true null effect), we will now simulate the impact of X1 to be $0.5$ for all dependent variables. Hence 
we simulate **true positives** and count how often we correctly detect the true effect, or, equivalently stated, how often we correctly reject 
the null of no treatment effect. 

```python
def get_data_true_effect(N, n_covariates, n_depvars, rho=0.5, seed=12345, effect=0.1):
    "Generate data with true positives."
    rng = np.random.default_rng(seed)
    Omega = np.eye(n_depvars)
    Omega[Omega != 1] = rho
    X = rng.standard_normal((N, n_covariates))
    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)
    beta = np.zeros((n_covariates, n_depvars))
    beta[1, :] = effect
    y = X @ beta + u_joint

    data = pd.DataFrame(X, columns=[f"X{i}" for i in range(n_covariates)])
    data = data.assign(**{f"y{i}": y[:, i] for i in range(n_depvars)})

    return data
```

```python
data_true = get_data_true_effect(
    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345, effect=0.5
)
fit = pf.feols(fml, data=data_true)
```

```python
(
    pf.etable(fit).tab_style(
        style=style.fill(color="yellow"), locations=loc.body(rows=[1])
    )
)
```

```text
GT(_tbl_data=  level_0        level_1                      0                      1  \
0    coef             X0    -0.108 <br> (0.103)    -0.152 <br> (0.116)   
1    coef             X1  0.538*** <br> (0.099)  0.493*** <br> (0.112)   
2    coef             X2     0.081 <br> (0.101)     0.026 <br> (0.114)   
3    coef             X3    -0.068 <br> (0.104)    -0.110 <br> (0.117)   
4    coef             X4    -0.063 <br> (0.101)     0.013 <br> (0.114)   
5    coef      Intercept    -0.007 <br> (0.100)    -0.061 <br> (0.113)   
6   stats   Observations                    100                    100   
7   stats      S.E. type                    iid                    iid   
8   stats  R<sup>2</sup>                  0.259                  0.202   

                       2                      3                     4  \
0   -0.243* <br> (0.113)    -0.026 <br> (0.112)   -0.158 <br> (0.109)   
1  0.589*** <br> (0.109)  0.503*** <br> (0.108)  0.340** <br> (0.106)   
2     0.094 <br> (0.111)     0.094 <br> (0.110)    0.008 <br> (0.108)   
3    -0.033 <br> (0.114)    -0.098 <br> (0.113)   -0.127 <br> (0.110)   
4    -0.142 <br> (0.111)     0.002 <br> (0.110)    0.086 <br> (0.107)   
5    -0.010 <br> (0.110)     0.060 <br> (0.109)   -0.035 <br> (0.107)   
6                    100                    100                   100   
7                    iid                    iid                   iid   
8                  0.264                  0.214                 0.146   

                       5                      6                      7  ...  \
0    -0.132 <br> (0.108)    -0.155 <br> (0.104)    -0.180 <br> (0.114)  ...   
1  0.493*** <br> (0.104)  0.568*** <br> (0.100)  0.590*** <br> (0.110)  ...   
2    -0.036 <br> (0.106)    -0.054 <br> (0.102)    -0.014 <br> (0.112)  ...   
3    -0.086 <br> (0.109)    -0.056 <br> (0.104)    -0.094 <br> (0.115)  ...   
4    -0.092 <br> (0.106)    -0.059 <br> (0.102)     0.030 <br> (0.112)  ...   
5     0.003 <br> (0.105)    -0.013 <br> (0.101)    -0.014 <br> (0.111)  ...   
6                    100                    100                    100  ...   
7                    iid                    iid                    iid  ...   
8                  0.226                  0.288                  0.269  ...   

                      10                     11                     12  \
0    -0.066 <br> (0.109)    -0.005 <br> (0.115)   -0.214* <br> (0.101)   
1  0.375*** <br> (0.105)  0.411*** <br> (0.111)  0.452*** <br> (0.098)   
2     0.128 <br> (0.107)    -0.043 <br> (0.113)     0.039 <br> (0.099)   
3    -0.099 <br> (0.110)    -0.003 <br> (0.116)     0.032 <br> (0.102)   
4     0.068 <br> (0.107)    -0.134 <br> (0.113)    -0.012 <br> (0.099)   
5    -0.003 <br> (0.106)     0.071 <br> (0.112)     0.117 <br> (0.098)   
6                    100                    100                    100   
7                    iid                    iid                    iid   
8                  0.151                  0.149                  0.207   

                      13                     14                     15  \
0    -0.183 <br> (0.115)    -0.007 <br> (0.099)    -0.138 <br> (0.105)   
1  0.586*** <br> (0.111)  0.438*** <br> (0.096)  0.558*** <br> (0.101)   
2     0.053 <br> (0.113)    -0.048 <br> (0.097)     0.027 <br> (0.103)   
3    -0.157 <br> (0.116)    -0.085 <br> (0.100)    -0.017 <br> (0.106)   
4    -0.021 <br> (0.112)     0.046 <br> (0.097)     0.076 <br> (0.103)   
5     0.022 <br> (0.112)    -0.013 <br> (0.097)    -0.034 <br> (0.102)   
6                    100                    100                    100   
7                    iid                    iid                    iid   
8                  0.273                  0.218                  0.267   

                      16                     17                     18  \
0    -0.104 <br> (0.107)    -0.138 <br> (0.111)    -0.102 <br> (0.110)   
1  0.408*** <br> (0.103)  0.442*** <br> (0.107)  0.498*** <br> (0.106)   
2    -0.021 <br> (0.
...[truncated]...
```

We will now study power more systematically via a simulation. More concretely, we will compute how often 
we detect the **true effect** of **X1 on Y1, Y2, ..., etc** given a fixed sample size $N$ using "uncorrected" p-values, 
the Bonferroni, Romano-Wolf and Westfall-Young methods. 

```python
def compute_power(seed, rho, effect):
    "Simulate data, estimate models, and compute power."
    data = get_data_true_effect(
        N, n_covariates, n_depvars, rho, seed=seed, effect=effect
    )
    fit = pf.feols(
        fml=fml, data=data, vcov="hetero"
    )  # model '1' regresses on Y1 - we're only interested in the power of this specific test
    df = fit.tidy().reset_index().set_index("Coefficient").xs("X1")

    df["Pr(>|t|) detect"] = df["Pr(>|t|)"] < 0.05
    df["Bonferroni detect"] = (
        pf.bonferroni(fit, param="X1").xs("Bonferroni Pr(>|t|)").values < 0.05
    )
    df["rwolf detect"] = (
        pf.rwolf(fit, param="X1", reps=200, seed=seed * 11).xs("RW Pr(>|t|)").values
        < 0.05
    )
    df["wyoung detect"] = (
        pf.wyoung(fit, param="X1", reps=200, seed=seed * 11).xs("WY Pr(>|t|)").values
        < 0.05
    )

    # Compute family rejection means
    detect_effect = {
        "Pr(>|t|) detect effect": df["Pr(>|t|) detect"].mean(),
        "Bonferroni detect effect": df["Bonferroni detect"].mean(),
        "rwolf detect effect": df["rwolf detect"].mean(),
        "wyoung detect effect": df["wyoung detect"].mean(),
    }

    detect_effect_df = pd.DataFrame(detect_effect, index=[effect])
    return detect_effect_df
```

```python
def run_power_simulation(n_iter, rho, effect, nthreads=-1):
    "Run simulation for power."
    seeds = list(range(n_iter))
    results = Parallel(n_jobs=nthreads)(
        delayed(compute_power)(seed, rho=rho, effect=effect) for seed in tqdm(seeds)
    )

    return pd.concat(results).mean()
```

```python
run_power_simulation(n_iter=1000, rho=0.5, effect=0.4)
```

```text
['  0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\alexa\\Documents\\pyfixest\\.pixi\\envs\\dev\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n', '  warnings.warn(\n', '100%|██████████| 1000/1000 [12:00<00:00,  1.39it/s]\n']
```

```text
Pr(>|t|) detect effect      0.97045
Bonferroni detect effect    0.78485
rwolf detect effect         0.86985
wyoung detect effect        0.86985
dtype: float64
```

We see that the "unadjusted" method detects the "true effects" at the highest frequency with on average 97% correctly detected effects. Does this mean that we should use uncorrected tests then? Well, maybe, but these do not control the family-wise error rate. While we have a better chance to detect a true effect, we also have a higher risk of flagging false positives. 

Additionally, it looks as if the rwolf and wyoung methods detect the true positives at a slightly higher rate than the Bonferroni method. 

Do these findings generalize to other effect sizes? We can check this by simply imposing 
different effects on the data generating process and repeating the previous exercise 
multiple times. 

```python
def run_power_simulation_vary_effect():
    "Run power simulations with varying effect sizes."
    n_points = 10
    max_val = 0.7

    effects = (
        np.sign(np.linspace(-1, 1, n_points))
        * max_val
        * (np.linspace(-1, 1, n_points) ** 2)
    )

    res_list = []
    for effect_size in tqdm(effects):
        res = run_power_simulation(n_iter=1000, rho=0.5, effect=effect_size)
        res["effect"] = effect_size
        res_list.append(res)
    return pd.concat(res_list, axis=1).T.set_index("effect")


res = run_power_simulation_vary_effect()
```

```text
['100%|██████████| 1000/1000 [10:54<00:00,  1.53it/s]\n', '100%|██████████| 1000/1000 [10:17<00:00,  1.62it/s]\n', '100%|██████████| 1000/1000 [10:05<00:00,  1.65it/s]\n', '100%|██████████| 1000/1000 [10:09<00:00,  1.64it/s]\n', '100%|██████████| 1000/1000 [10:25<00:00,  1.60it/s]\n', '100%|██████████| 1000/1000 [09:54<00:00,  1.68it/s]\n', '100%|██████████| 1000/1000 [09:44<00:00,  1.71it/s]\n', '100%|██████████| 1000/1000 [10:53<00:00,  1.53it/s]\n', '100%|██████████| 1000/1000 [14:32<00:00,  1.15it/s]\n', '100%|██████████| 1000/1000 [14:38<00:00,  1.14it/s]\n', '100%|██████████| 10/10 [1:53:04<00:00, 678.48s/it]\n']
```

```python
column_to_label_dict = {
    "Pr(>|t|) detect effect": "Unadjusted",
    "Bonferroni detect effect": "Bonferroni",
    "rwolf detect effect": "RW",
    "wyoung detect effect": "WY",
}

plt.figure(figsize=(10, 6))
line_styles = ["-", "--", "-.", ":"]
for column, line_style in zip(res.columns, line_styles):
    plt.plot(
        res.index, res[column], linestyle=line_style, label=column_to_label_dict[column]
    )

plt.title("Power of Multiple Testing Correction Procedures")
plt.xlabel("Effect")
plt.ylabel("Proportion of correctly detected effects")
plt.legend()
plt.grid()
plt.show()
```

```text
<Figure size 1000x600 with 1 Axes>
```

We see that for any simulated effect size, the Romano-Wolf and Westfall-Young methods detect a higher share of true positives than the Bonferroni method: they have **higher power**.

## Literature 

- Clarke, Damian, Joseph P. Romano, and Michael Wolf. "The Romano–Wolf multiple-hypothesis correction in Stata." The Stata Journal 20.4 (2020): 812-843.
- Romano, Joseph P., and Michael Wolf. "Stepwise multiple testing as formalized data snooping." Econometrica 73.4 (2005): 1237-1282.
- Westfall, Peter H., and S. Stanley Young. Resampling-based multiple testing: Examples and methods for p-value adjustment. Vol. 279. John Wiley & Sons, 1993.

---

# panel_variance_reduction.html

## Panel Estimators for AB Tests with Repeated Observations

```python
import numpy as np
import pandas as pd
from IPython.display import display
from tqdm import tqdm

import pyfixest as pf
from pyfixest.utils.dgps import get_sharkfin
```

In this tutorial, we show how to use pyfixest to reduce the variance of your estimators in AB tests with repeated observations.

For example, we may be a streaming platform and we want to estimate the effect of a new feature on the amount of time users spend watching videos. To do so, 
we randomly assign the treatment to half of our users. For 15 days prior to the experiment, we track the desired outcome (minutes watched) for each user. If
users are not seen on the platform on a given day, the number of minutes watched is 0. Our experiment runs for 15 days. All in all, we have 30 days of data for each 
user. 

To get started, we simulate a panel data set of 100_000 users, with mentioned 30 days of data, with 15 days of pre and post data. 

```python
def get_data(num_units: int, sigma_unit: int) -> pd.DataFrame:
    """
    Random example data set.
    num_units: int
        The number of users
    sigma_unit: int
        The user-level idosyncratic error term.
    """
    data = get_sharkfin(
        num_units=100_000,
        num_periods=30,
        num_treated=500,
        treatment_start=15,
        seed=231,
        sigma_unit=18,
    )
    data = data.rename(columns={"Y": "minutes_watched", "unit": "user", "year": "day"})

    return data


data = get_data(num_units=100_000, sigma_unit=18)
data.head()
```

```text
   user  day  treat  minutes_watched  ever_treated
0     0    0      0         8.114488             0
1     0    1      0         7.633058             0
2     0    2      0         6.712332             0
3     0    3      0         6.230820             0
4     0    4      0         6.004489             0
```

We can inspect the data generating process via the `panelview()` function: 

```python
pf.panelview(
    data,
    unit="user",
    time="day",
    treat="treat",
    collapse_to_cohort=True,
    sort_by_timing=True,
    ylab="Cohort",
    xlab="Day",
    title="Treatment Assignment Cohorts",
    figsize=(6, 5),
)
```

```text
<Axes: title={'center': 'Treatment Assignment Cohorts'}, xlabel='Day', ylabel='Cohort'>
```

```text
<Figure size 600x500 with 1 Axes>
```

We see that half of our users are treated after half the time. 

In the next step, we will look at three different ways to compute the average treatment effect of our feature on the outcome: 

- First, we will compute a standard "difference in means" estimator and conduct a two-sampled t-test for inference. We do this both by hand and by means of linear regression. While standard, we will show that this estimator is relatively inefficient 
  as it throws aways valuable information from the pre-treatment periods.  
- In a second step, we improve on the difference in means estimator and include pre-treatment measures of the outcome variable to our regression model. In the tech blog world, this method 
  is often referred to as CUPED (which stands for "Controlled-experiment Using Pre-Experiment Data" as far as we know). We will demonstrate that CUPED uted leads to a significant reduction in the variance of our estimators. Pre-treatment averages are a "good control" variable because 
  a) under uncondional randomization, pre-treatment averages should be uncorrelated of the treatment assignment and b) pre-treatment averages should be highly predictive of the post-treatment outcome. The intuition here is simply that users will behave similarly "after" the experiment starts / when receiving the treatment than they did before. 
- In a third step, we show that instead of using pre-treatment averages as a control variable, we can simply fit a panel model and control for user and time fixed effect. 

All three estimators identify the same average treatment effect, but the CUPED and panel estimator are much more efficient: they produce lower variances. 

But first, let's discuss why we are interested in reducing the variance of our estimators. 

## Variance of Estimators, Statistical Power and Sample Size Requirements

In statistical experiments, we care about power to make sure that we detect a true effect. 

It depends on the **signal-to-noise ratio**:

$$
\text{Power} \;\sim\; f\!\left(\frac{|\tau|}{\text{SE}}\right)
$$

where

- $\tau$ = the true effect size  
- $\text{SE} = \frac{\sigma}{\sqrt{n}}$ = the standard error of the estimate  
- $\sigma$ = standard deviation of the outcome  
- $n$ = sample size per group (if balanced)  

So, anything that **increases the effect size**, **increases the sample size**, or **reduces outcome variance** will improve power. That's where our interest in variance reduction stems from. 


## Simple Difference in Means Estimator

The simplest way to analyse an AB test / estimate an average treatment effect is the difference in means estimator:  
$$
    \tau = \frac{1}{n_1} \sum_{i=1}^{n_1} Y_{i,1} - \frac{1}{n_0} \sum_{i=1}^{n_0}  Y_{i,0}
$$

We can compute it in a few lines of `pandas` by implementing three steps: 
- First, we throw away all pre-experimental data. 
- Then, we sum the post-treatment minutes watched into a single data point per user. 
- Finally, we compute the difference of means of total minutes watched between the treated and control group.
Done! 

```python
def _difference_in_means_pd(data):
    # standard analyses: throw away pre-experimental data
    data2 = data.copy()
    data_post = data2[data2.day >= 15]
    # collapse post-treatment minutes watched into a single data point per user
    data_post_agg = (
        data_post.groupby(["user", "treat"])
        .agg({"minutes_watched": "mean"})
        .reset_index()
        .rename(columns={"minutes_watched": "total_minutes_watched"})
    )
    # compute difference of means estimator
    return data_post_agg.groupby("treat").mean().diff()


_difference_in_means_pd(data)
```

```text
              user  total_minutes_watched
treat                                    
0              NaN                    NaN
1     -1387.630151               0.761514
```

Because linear regression is just a fency way to compute and compare differences, we could also have estimated this via `pf.feols()`: 

```python
def _difference_in_means(data):
    data2 = data.copy()

    data_post = data2[data2.day >= 15]
    fit = pf.feols("minutes_watched ~ treat", data=data_post)

    return fit


fit_difference_in_means = _difference_in_means(data)
fit_difference_in_means.summary()
```

```text
['###\n', '\n', 'Estimation:  OLS\n', 'Dep. var.: minutes_watched, Fixed effects: 0\n', 'Inference:  iid\n', 'Observations:  1500000\n', '\n', '| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |\n', '|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n', '| Intercept     |      0.357 |        0.015 |    24.163 |      0.000 |  0.328 |   0.386 |\n', '| treat         |      0.762 |        0.209 |     3.641 |      0.000 |  0.352 |   1.171 |\n', '---\n', 'RMSE: 18.07 R2: 0.0 \n']
```

Note that no aggregation step is needed and that we get identical point estimates. As a bonus, we get standard errors and confidence intervals in the process. 

## Cuped: Using Pre-Experimental Measures of the Outcomes Variable as Controls

Sometimes, throwing away data can be a winning strategy ("garbage in garbage out"), but not in this example: we have high-quality pre-experimental measures of the behavior 
of our users at hand, and we should use it in our favor! 

More specifically, instead of throwing away all of the pre-experimental measures, we could instead have used them as a control! 

This should help reduce the variance of our estimators because pre-experimental behavior is likely to be highly predicitive of what users do after the launch of the experiment. Consequently, including these baselines in our regression models should help us "explain residual errors" and thereby reduce the variance of our 
estimators. 

If this works, it's great news, as it allows us to run the same experiment on a smaller number of users and still achieve the same level of power!

```python
def _regression_cuped(data):
    data = data.copy()
    data["pre_experiment"] = data["day"] < 15

    # aggregate pre-post averages by user
    agg = data.groupby(["user", "pre_experiment"], as_index=False).agg(
        minutes_watched=("minutes_watched", "mean")
    )

    wide = (
        agg.pivot(index="user", columns="pre_experiment", values="minutes_watched")
        .rename(columns={True: "minutes_pre", False: "minutes_post"})
        .reset_index()
    )

    wide = wide.merge(
        data[["user", "ever_treated"]].drop_duplicates(), on="user", how="left"
    ).rename(columns={"ever_treated": "treat"})

    # center the pre metric
    mu_pre = wide["minutes_pre"].mean()
    wide["minutes_pre_c"] = wide["minutes_pre"] - mu_pre

    fit_cuped = pf.feols(
        "minutes_post ~ treat + minutes_pre_c", data=wide, vcov="hetero"
    )

    return fit_cuped, wide


fit_cuped, data_cuped = _regression_cuped(data)
```

Per user, we now log the total minutes watched before and after the treatment and then fit a regression model of the following form: ^^

$$
    \text{total minutes watched after treatment} = \alpha + \beta \text{treat} + \gamma (\text{total minutes watched before treatment} - \text{avg total minutes watched before treatment}) + \epsilon
$$

```python
data_cuped.head()
```

```text
   user  minutes_post  minutes_pre  treat  minutes_pre_c
0     0      7.747841     7.767178      0       7.894779
1     1     25.595928    24.674535      0      24.802136
2     2    -32.224392   -32.130334      0     -32.002733
3     3    -12.269762   -13.142435      0     -13.014835
4     4     -1.448348    -1.392043      0      -1.264442
```

We can compare with the previous results: 

```python
pf.etable([fit_difference_in_means, fit_cuped])
```

```text
GT(_tbl_data=  level_0             level_1                      0                      1
0    coef               treat  0.762*** <br> (0.209)  0.192*** <br> (0.029)
1    coef       minutes_pre_c                         0.999*** <br> (0.000)
2    coef           Intercept  0.357*** <br> (0.015)  0.360*** <br> (0.002)
3   stats        Observations                1500000                 100000
4   stats           S.E. type                    iid                 hetero
5   stats       R<sup>2</sup>                  0.000                  0.999
6   stats  Adj. R<sup>2</sup>                  0.000                  0.999, _body=<great_tables._gt_data.Body object at 0x0000028E524DB3E0>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x0000028E6AB85610>, _spanners=Spanners([SpannerInfo(spanner_id='minutes_watched', spanner_level=1, spanner_label='minutes_watched', spanner_units=None, spanner_pattern=None, vars=['0'], built=None), SpannerInfo(spanner_id='minutes_post', spanner_level=1, spanner_label='minutes_post', spanner_units=None, spanner_pattern=None, vars=['1'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x0000028E6AB84680>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo(scss=True, category='table', type='value', value='solid'), table_border_top_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_top_color=OptionsInfo(scss=True, category='table', type='value', value='#A8A8A8'), table_border_right_style=OptionsInfo(scss=True, category='table', type='value', value='none'), table_border_right_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_right_color=OptionsInfo(scss=True, category='table', type='value', value='#D3D3D3'), table_border_bottom_incl
...[truncated]...
```

Point estimates for the difference-in-means estimator is 0.762, while it is 0.192 for the Cuped estimator. But most importantly, the standard errors are much smaller for the CUPED estimator (0.03 vs 0.209). Because we have fitted a regression model with covariates, we have used heteroskedasticity-robust standard errors, which should be more conservative than the iid errors used in the difference-in-means regression.

## Panel Estimator

Instead of collapsing all pre-and post information into a single average (and thereby losing information), we can as well be lazy and just use a panel estimator.

Via `pyfixest`, that's one line of code: 

```python
fit_panel = pf.feols(
    "minutes_watched ~ treat | user + day",
    data=data,
    vcov="hetero",
    demeaner_backend="rust",
)
```

We can compare all results: 

```python
pf.etable([fit_difference_in_means, fit_cuped, fit_panel])
```

```text
GT(_tbl_data=  level_0               level_1                      0                      1  \
0    coef                 treat  0.762*** <br> (0.209)  0.192*** <br> (0.029)   
1    coef         minutes_pre_c                         0.999*** <br> (0.000)   
2    coef             Intercept  0.357*** <br> (0.015)  0.360*** <br> (0.002)   
3      fe                   day                      -                      -   
4      fe                  user                      -                      -   
5   stats          Observations                1500000                 100000   
6   stats             S.E. type                    iid                 hetero   
7   stats         R<sup>2</sup>                  0.000                  0.999   
8   stats  R<sup>2</sup> Within                      -                      -   

                       2  
0  0.191*** <br> (0.012)  
1                         
2                         
3                      x  
4                      x  
5                3000000  
6                 hetero  
7                  0.999  
8                  0.000  , _body=<great_tables._gt_data.Body object at 0x0000028E6AAC2690>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None), ColInfo(var='2', type=<ColInfoTypeEnum.default: 1>, column_label='(3)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x0000028E6AAC2F30>, _spanners=Spanners([SpannerInfo(spanner_id='minutes_watched', spanner_level=1, spanner_label='minutes_watched', spanner_units=None, spanner_pattern=None, vars=['0', '2'], built=None), SpannerInfo(spanner_id='minutes_post', spanner_level=1, spanner_label='minutes_post', spanner_units=None, spanner_pattern=None, vars=['1'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x0000028E6AAC2BD0>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=T
...[truncated]...
```

The panel estimator almost exactly matches CUPED, and we do even better in terms of variance. 

However, because we are working with panel data, the error terms are likely correlated over time within each user. If we ignore this dependence, our standard errors may be underestimated, which in turn can lead to over-rejecting the null hypothesis of no effect. A common way to address this issue is to compute cluster-robust standard errors at the user level, which account for arbitrary heteroskedasticity and autocorrelation within users.

```python
fit_panel_crv = pf.feols(
    "minutes_watched ~ treat | user + day",
    data=data,
    vcov={"CRV1": "user"},
    demeaner_backend="rust",
)
```

Comparing all results, the panel estimator still does quite well in terms of the size of the estimated standard errors. 

```python
pf.etable(
    [fit_difference_in_means, fit_cuped, fit_panel, fit_panel_crv],
    digits=4,
)
```

```text
GT(_tbl_data=  level_0               level_1                        0  \
0    coef                 treat  0.7615*** <br> (0.2092)   
1    coef         minutes_pre_c                            
2    coef             Intercept  0.3574*** <br> (0.0148)   
3      fe                   day                        -   
4      fe                  user                        -   
5   stats          Observations                  1500000   
6   stats             S.E. type                      iid   
7   stats         R<sup>2</sup>                   0.0000   
8   stats  R<sup>2</sup> Within                        -   

                         1                        2                        3  
0  0.1918*** <br> (0.0288)  0.1913*** <br> (0.0118)  0.1913*** <br> (0.0287)  
1  0.9992*** <br> (0.0001)                                                    
2  0.3602*** <br> (0.0021)                                                    
3                        -                        x                        x  
4                        -                        x                        x  
5                   100000                  3000000                  3000000  
6                   hetero                   hetero                 by: user  
7                   0.9987                   0.9985                   0.9985  
8                        -                   0.0001                   0.0001  , _body=<great_tables._gt_data.Body object at 0x0000028E52473D10>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None), ColInfo(var='2', type=<ColInfoTypeEnum.default: 1>, column_label='(3)', column_align='center', column_width=None), ColInfo(var='3', type=<ColInfoTypeEnum.default: 1>, column_label='(4)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x0000028E6A8F0440>, _spanners=Spanners([SpannerInfo(spanner_id='minutes_watched', spanner_level=1, spanner_label='minutes_watched', spanner_units=None, spanner_pattern=None, vars=['0', '2', '3'], built=None), SpannerInfo(spanner_id='minutes_post', spanner_level=1, spanner_label='minutes_post', spanner_units=None, spanner_pattern=None, vars=['1'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x0000028E6A8F1D00>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=Options
...[truncated]...
```

The panel errors produces point estimates and standard errors for the treatment variable that are very close to CUPED. 

The sceptical reader might now object that this is all far from overwhelming evidence. After all , we fit each model only once! So it might well be 
that we were just lucky and our results are driven by sampling error. In the next section, we will argue that this is not the case by means of a monte carlo simulation. 

### Digression: Heterogeneous Effects over Time via Panel Estimators

One example of the panel estimator is that it allows us to track treatment effects over time. This allows us to track novelty effects where in the beginning, 
users interact a lot with a new feature, i.e. they spend a lot of time playing a gamee, but over time lose interest. 

```python
fit_panel_time = pf.feols(
    "minutes_watched ~ i(day, ever_treated, ref = 14) | user + day",
    vcov={"CRV1": "user"},
    data=data,
    demeaner_backend="rust",
)
fit_panel_time.iplot(
    coord_flip=False,
    plot_backend="matplotlib",
    cat_template="{value}",
    title="Event Study Plot",
)
```

```text
<Figure size 1000x600 with 1 Axes>
```

Initially, we observe a large treatment effect that reverts to a null effect on day 23: a clear novelty effect pattern. Clearly, we should not accept this 
experiment as it only adds technical complexity, but has no impact on user behavior beyond the initial effect. When ignoring time heterogeneity (as with the difference in means, cuped, and ATE panel estimators above), we completely miss that the effect fades out. With a dynamic panel estimator, we get it almost for free. 

For more examples, see the paper by [Lal et al](https://arxiv.org/abs/2410.09952), which also shows how to estimate very large panel models in [SQL/duckdb](https://github.com/py-econometrics/duckreg) (yep, that's not a joke!). 

## Monte Carlo Simulation

To rule out that the examples above were purely do to look, we simply repeat them a couple of times using a monte carlo simulation. 

```python
def _variance_monte_carlo(data):
    fit_dim = _difference_in_means(data)
    fit_cuped, _ = _regression_cuped(data)
    fit_panel = pf.feols(
        "minutes_watched ~ treat | user + day",
        data=data,
        vcov={"CRV1": "user"},
        demeaner_backend="rust",
    )

    dim_se = fit_dim.tidy().xs("treat")["Std. Error"]
    cuped_se = fit_cuped.tidy().xs("treat")["Std. Error"]
    panel_se = fit_panel.tidy().xs("treat")["Std. Error"]

    return dim_se, cuped_se, panel_se


def _run_simulation(N, sigma_unit, n_sim=100):
    dim_se_arr = np.zeros(n_sim)
    cuped_se_arr = np.zeros(n_sim)
    panel_se_arr = np.zeros(n_sim)

    for i in tqdm(range(n_sim)):
        data = get_sharkfin(
            num_units=N,
            num_periods=30,
            num_treated=int(N / 2),
            treatment_start=15,
            seed=i,
            sigma_unit=sigma_unit,
        )
        data.rename(
            columns={"Y": "minutes_watched", "unit": "user", "year": "day"},
            inplace=True,
        )

        dim_se_arr[i], cuped_se_arr[i], panel_se_arr[i] = _variance_monte_carlo(data)

    return pd.Series(
        {
            "Difference-in-Means": np.mean(dim_se_arr),
            "Cuped": np.mean(cuped_se_arr),
            "Panel": np.mean(panel_se_arr),
        }
    )
```

```python
# note that in a proper scientific simulation, we might want to set the
# number of iterations to be much higher than 10
se_sim_18 = _run_simulation(N=100_000, sigma_unit=18, n_sim=20)
display(se_sim_18)
```

```text
['100%|██████████| 20/20 [01:30<00:00,  4.53s/it]\n']
```

```text
Difference-in-Means    0.029457
Cuped                  0.004184
Panel                  0.004185
dtype: float64
```

So we manage to reduce our standard errors by around 6x! This is pretty fantastic news, as it implies that we can run our experiment on a much smaller sample, but still achieve the same level of power! This is what we want to verify in the last step. 

## Power Simulation 

In all simulations, we conduct a two-sided t-test with size $\alpha = 0.01$. 

```python
def _power(nobs, n_sim):
    res_df = pd.DataFrame()

    for N in nobs:
        dim_reject_null_arr = np.zeros(n_sim)
        cuped_reject_null_arr = np.zeros(n_sim)
        panel_reject_null_arr = np.zeros(n_sim)

        for i in tqdm(range(n_sim)):
            data = get_sharkfin(
                num_units=N,
                num_periods=30,
                num_treated=int(N / 2),
                treatment_start=15,
                seed=i,
                sigma_unit=18,
            )
            data.rename(
                columns={"Y": "minutes_watched", "unit": "user", "year": "day"},
                inplace=True,
            )

            fit_dim = _difference_in_means(data)
            fit_cuped, _ = _regression_cuped(data)
            fit_panel = pf.feols(
                "minutes_watched ~ treat | user + day", data=data, vcov={"CRV1": "user"}
            )

            dim_reject_null_arr[i] = fit_dim.pvalue().xs("treat") < 0.01
            cuped_reject_null_arr[i] = fit_cuped.pvalue().xs("treat") < 0.01
            panel_reject_null_arr[i] = fit_panel.pvalue().xs("treat") < 0.01

        dim_reject_null_mean = np.mean(dim_reject_null_arr)
        cuped_reject_null_mean = np.mean(cuped_reject_null_arr)
        panel_reject_null_mean = np.mean(panel_reject_null_arr)

        res = pd.DataFrame(
            {
                "N": [N],
                "Difference-in-Means": [dim_reject_null_mean],
                "Cuped": [cuped_reject_null_mean],
                "Panel": [panel_reject_null_mean],
            }
        )

        res_df = pd.concat([res_df, res], axis=0, ignore_index=True)

    return res_df
```

```python
# TODO: set n_sim higher
power_df = _power(nobs=[100, 500, 1000, 5_000, 10_000, 100_000], n_sim=10)
```

```text
['100%|██████████| 10/10 [00:30<00:00,  3.01s/it]\n', '100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\n', '100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n', '100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n', '100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n', '100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n']
```

```python
display(power_df)
```

```text
        N  Difference-in-Means  Cuped  Panel
0     100                  0.5    0.1    0.1
1     500                  0.4    0.9    0.9
2    1000                  0.7    1.0    1.0
3    5000                  0.5    1.0    1.0
4   10000                  0.6    1.0    1.0
5  100000                  0.9    1.0    1.0
```

CUPED and the panel estimator achieve 90% power with 500 observations, while the difference in means-estimator requires a sample that is orders of magnitude larger. 
To conclude, we should mention that these effects stem from a stylized exmaple data set - in practice, the gains from CUPED / panel methods for variance reduction are much lower - 
companies with access to high quality panel data report reductions of 10% to 50% (see e.g. this paper by [Microsoft Bing](https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf)).

## When does CUPED not work? 

CUPED only works when we explain a lot of "residual" variation, which in our example data set is controlled by the `sigma_unit` parameter. Let's see what happens if we drastically reduce it - we now change the parameter from `18` to `4`.

We repeat the standard error monte carlo simulation, but set `sigma_unit = 4`.

```python
se_sim_1 = _run_simulation(N=100_000, sigma_unit=4, n_sim=10)
```

```text
['100%|██████████| 10/10 [00:48<00:00,  4.89s/it]\n']
```

```python
display(se_sim_1)
```

```text
Difference-in-Means    0.006852
Cuped                  0.004174
Panel                  0.004186
dtype: float64
```

All of a sudden, the difference-in-means estimator has lower variance!

```python
display(se_sim_18)
```

```text
Difference-in-Means    0.029457
Cuped                  0.004184
Panel                  0.004185
dtype: float64
```

The reason we do not achieve variance reduction this time is that the addition of pre-treatment covariates / unit fixed effects simply explains less unobserved variance than before. 

## Other useful links

- [Matteo Courthoud's great blog post on CUPED](https://matteocourthoud.github.io/post/cuped/), goes through some theory, compares CUPED with other estimators, runs simulations studies, and summarizes the literature. 
- [Lal et al on panel experiments](https://arxiv.org/abs/2410.09952) explains why using panel estimators might be a good idea when analysing AB tests, and shows how it can be done efficiently in SQL.

---

# pyfixest-gpu-cupy.html

# PyFixest on the GPU via CuPy

Besides JAX, it is possible to run PyFixest on the GPU via CuPy (linux and windows). Instead of applying the alternating projections algorithm to demean fixed effects, CuPy works with sparse matrices and the sparse LSMR solver (as is e.g. [available in scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html)).

This strategy is amenable for GPU acceleration, and for problems where the standard demeaner struggles to converge, this strategy can lead to significant speedups if paired with a GPU.

![Complex Fixed Effects Structure: Benchmark](https://raw.githubusercontent.com/py-econometrics/pyfixest/master/benchmarks/complex_benchmarks.png)

Note that for smaller and more well-behaved problems, running the alternating projections algorithm on the CPU via `numba` or `rust` usually seems to work better: 

![Simply Fixed Effects Structure: Benchmark](https://raw.githubusercontent.com/py-econometrics/pyfixest/master/benchmarks/simple_benchmarks_cupy.png)

**Benchmark Hardware Specifications:**
- **CPU**: x86_64, 8 physical cores @ 3.2 GHz, 44 GB RAM
- **GPU**: NVIDIA RTX A6000, 48 GB memory, Compute Capability 8.6

## Installation

To run pyfixest via cup on the GPU, you need to install the required dependency: 

```bash
# For CUDA 11.x, 12.x, 13.x
pip install cupy-cuda11x
pip install cupy-cuda12x
pip install cupy-cuda13x
```

---

# pyfixest-sprint.html

# PyFixest Sprint in Heilbronn


We're organizing a PyFixest development sprint in partnership with the [appliedAI Institute](https://appliedai-institute.de/) at their Heilbronn office.

**Dates:** March 4th–6th 2026.

**Interested in joining?** Reach out to [Alex](mailto:alexander-fischer1801@t-online.de) with a brief note about your background and motivation. We have some funding available to support student participation.

### What we're working on

Our main goals for the sprint:

- **Rust backend:** Finalize the port from Numba to Rust and deprecate the Numba dependency, with continued optimization of our core demeaning algorithm. We still have to port logic for HAC standard errors and randomisation inference.
- **GPU acceleration:** Continue building out JAX, CuPy, and potentially PyTorch backends (for Mac users), potentially re-implementing the [LSMR algorithm](https://web.stanford.edu/group/SOL/software/lsmr/LSMR-SISC-2011.pdf) by hand & run experiments on pre-conditioning
- **Internal refactor:** Introduce a cleaner class hierarchy with a proper base estimation class. Currently, all estimation classes inherit from `Feols`.
- **NumPy-style API:** Rewrite estimation classes (Feols, Fepois, etc.) to follow sklearn-style conventions. Users should be able to fit a regression model by passing `data`, `X` or `y` to `Feols`. If data is passed, a `Feols.from_formula` method creates the design matrix. Core functional estimation APIs (`feols()`, `fepois`, etc) remain as they are.
- **Clean Handling of Regression Weights**: The logic for weighted least squares (WLS) is currently a bit hard to follow, as dependent variable and design matrix are pre-multiplied with `np.sqrt(weights)`. By reading the code, it is not always immediately clear if `self._X` is already weighted, or not yet. This of course can lead to confusion and bugs, which we should strive to avoid =)


We're also hoping to make progress on:

- **Standalone demeaning package:** Spin out the demeaning algorithms into a lightweight, cross-language package
- **Varying slopes support:** Add varying slopes to the demeaning algorithm and extend the formula API
- **Narwhals integration:** Better support for running analyses with either pandas or polars
- **maketables cleanup:** Refactor the codebase and open PRs to third-party packages (doubleML, CausalPy, econML, etc.)
- **Documentation:** Add how-to guides and add a *statistical documentation* in which we explan the math behind all estimators
- **Instrumental Variables**: We want to support IV models with more than one endogenous variable, and implement a range of diagnostics.
- **moderndid contributions:** Port our DiD estimators (Gardner, local projections, Sun & Abraham) to the [moderndid](https://github.com/jordandeklerk/moderndid) package
- **Tests**: The `pyfixest` test suite has grown quite a lot over time, and resembles more of
a djungle than an english garden. It would be great to clean things up little bit. In particular, it would be lovely to move away from `rpy2`/ use modern features of it.

### What would be helpful

If you're excited about econometrics tooling and want to contribute, we'd love to have you. You don't need to be an expert in Rust or GPU programming. If you've been looking for a way to get started with open source, we'd be happy to help you make your first contributions.

---

# pyfixest.html

![](figures/pyfixest-logo.png)

# PyFixest: Fast High-Dimensional Fixed Effects Regression in Python

[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/license/mit)
![Python Versions](https://img.shields.io/badge/Python-3.9–3.14-blue)
[![PyPI -Version](https://img.shields.io/pypi/v/pyfixest.svg)](https://pypi.org/project/pyfixest/)
[![Project Chat][chat-badge]][chat-url]
[![image](https://codecov.io/gh/py-econometrics/pyfixest/branch/master/graph/badge.svg)](https://codecov.io/gh/py-econometrics/pyfixest)
[![Known Bugs](https://img.shields.io/github/issues/py-econometrics/pyfixest/bug?color=red&label=Bugs)](https://github.com/py-econometrics/pyfixest/issues?q=is%3Aissue+is%3Aopen+label%3Abug)
[![File an Issue](https://img.shields.io/github/issues/py-econometrics/pyfixest)](https://github.com/py-econometrics/pyfixest/issues)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Pixi Badge][pixi-badge]][pixi-url]
[![Donate | GiveDirectly](https://img.shields.io/static/v1?label=GiveDirectly&message=Donate&color=blue&style=flat-square)](https://github.com/py-econometrics/pyfixest?tab=readme-ov-file#support-pyfixest)
[![PyPI](https://img.shields.io/pypi/v/pyfixest)](https://pypi.org/project/pyfixest)
[![Citation](https://img.shields.io/badge/Cite%20as-PyFixest-blue)](https://github.com/py-econometrics/pyfixest?tab=readme-ov-file#how-to-cite)
[![Documentation](https://img.shields.io/badge/Cite%20as-PyFixest-green)](https://pyfixest.org/pyfixest.html)
[![Function Reference](https://img.shields.io/badge/Cite%20as-PyFixest-yellow)](https://pyfixest.org/reference/)
[![DeepWiki](https://img.shields.io/badge/DeepWiki-pyfixest-blue)](https://deepwiki.com/py-econometrics/pyfixest)


[pixi-badge]:https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/prefix-dev/pixi/main/assets/badge/v0.json&style=flat-square
[pixi-url]: https://pixi.sh

[chat-badge]: https://img.shields.io/discord/1259933360726216754.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2&style=flat-square
[chat-url]: https://discord.gg/gBAydeDMVK

`PyFixest` is a Python package for fast high-dimensional fixed effects regression.

The package aims to mimic the syntax and functionality of [Laurent Bergé's](https://sites.google.com/site/laurentrberge/) formidable [fixest](https://github.com/lrberge/fixest) package as closely as Python allows: if you know `fixest` well, the goal is that you won't have to read the docs to get started! In particular, this means that all of `fixest's` defaults are mirrored by `PyFixest`.

For a quick introduction, you can take a look at the [quickstart](https://pyfixest.org/quickstart.html) or the regression chapter of [Coding for Economists](https://aeturrell.github.io/coding-for-economists/econmt-regression.html#imports). You can find documentation of all user facing functions in the [Function Reference](https://pyfixest.org/reference/) section of the [documentation](https://pyfixest.org/pyfixest.html).

For questions on `PyFixest`, head on over to our [github discussions](https://github.com/py-econometrics/pyfixest/discussions), or (more informally) join our [Discord server](https://discord.gg/gBAydeDMVK).

## Support PyFixest

If you enjoy using `PyFixest`, please consider donating to [GiveDirectly](https://donate.givedirectly.org/dedicate) and dedicating your donation to `pyfixest.dev@gmail.com`.
You can also leave a message through the donation form - your support and encouragement mean a lot to the developers!

## Features

-   **OLS**, **WLS** and **IV** Regression with Fixed-Effects Demeaning via [Frisch-Waugh-Lovell](https://bookdown.org/ts_robinson1994/10EconometricTheorems/frisch.html)
-   **Poisson Regression** following the [pplmhdfe algorithm](https://journals.sagepub.com/doi/full/10.1177/1536867X20909691)
-   Probit, Logit and Gaussian Family **GLMs** (currently without fixed effects demeaning, this is WIP)
-   **Quantile Regression** using an Interior Point Solver
-   Multiple Estimation Syntax
-   Several **Robust**, **Cluster Robust** and **HAC Variance-Covariance** Estimators
-   **Wild Cluster Bootstrap** Inference (via
    [wildboottest](https://github.com/py-econometrics/wildboottest))
-   **Difference-in-Differences** Estimators:
    -   The canonical Two-Way Fixed Effects Estimator
    -   [Gardner's two-stage
        ("`Did2s`")](https://jrgcmu.github.io/2sdd_current.pdf)
        estimator
    -   Basic Versions of the Local Projections estimator following
        [Dube et al (2023)](https://www.nber.org/papers/w31184)
    - The fully saturated Event-Study estimator following [Sun & Abraham (2021)](https://www.sciencedirect.com/science/article/abs/pii/S030440762030378X)
- **Multiple Hypothesis Corrections** following the Procedure by [Romano and Wolf](https://journals.sagepub.com/doi/pdf/10.1177/1536867X20976314) and **Simultaneous Confidence Intervals** using a **Multiplier Bootstrap**
- The **Causal Cluster Variance Estimator (CCV)** following [Abadie et al.](https://economics.mit.edu/sites/default/files/2022-09/When%20Should%20You%20Adjust%20Standard%20Errors%20for%20Clustering.pdf)
- Regression **Decomposition** following [Gelbach (2016)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)
- **Publication-ready tables** with [Great Tables](https://posit-dev.github.io/great-tables/articles/intro.html) or LaTex booktabs

## Installation

You can install the release version from `PyPi` by running

``` py
pip install -U pyfixest
```

or the development version from github by running

``` py
pip install git+https://github.com/py-econometrics/pyfixest.git
```


### GPU Acceleration (Optional)

PyFixest supports GPU-accelerated fixed effects demeaning via CuPy. To enable GPU acceleration, install CuPy matching your CUDA version:

```bash
# For CUDA 11.x, 12.x, 13.x
pip install cupy-cuda11x
pip install cupy-cuda12x
pip install cupy-cuda13x
```

Once installed, you can use GPU-accelerated demeaning by setting the `demean_backend` parameter:

```python
# Use GPU with float32 and float64 precision
pf.feols("Y ~ X1 | f1 + f2", data=data, demean_backend="cupy32")
pf.feols("Y ~ X1 | f1 + f2", data=data, demean_backend="cupy64")
```

## Benchmarks

All benchmarks follow the [fixest
benchmarks](https://github.com/lrberge/fixest/tree/master/_BENCHMARK).
All non-pyfixest timings are taken from the `fixest` benchmarks.

![](figures/benchmarks_ols.svg)
![](figures/benchmarks_poisson.svg)
![](figures/quantreg_benchmarks.png)

## Quickstart


```python
import pyfixest as pf

data = pf.get_data()
pf.feols("Y ~ X1 | f1 + f2", data=data).summary()
```

    ###

    Estimation:  OLS
    Dep. var.: Y, Fixed effects: f1+f2
    Inference:  CRV1
    Observations:  997

    | Coefficient   |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |
    |:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|
    | X1            |     -0.919 |        0.065 |   -14.057 |      0.000 | -1.053 |  -0.786 |
    ---
    RMSE: 1.441   R2: 0.609   R2 Within: 0.2


### Multiple Estimation

You can estimate multiple models at once by using [multiple estimation
syntax](https://aeturrell.github.io/coding-for-economists/econmt-regression.html#multiple-regression-models):



```python
# OLS Estimation: estimate multiple models at once
fit = pf.feols("Y + Y2 ~X1 | csw0(f1, f2)", data = data, vcov = {'CRV1':'group_id'})
# Print the results
fit.etable()
```

                               est1               est2               est3               est4               est5               est6
    ------------  -----------------  -----------------  -----------------  -----------------  -----------------  -----------------
    depvar                        Y                 Y2                  Y                 Y2                  Y                 Y2
    ------------------------------------------------------------------------------------------------------------------------------
    Intercept      0.919*** (0.121)   1.064*** (0.232)
    X1            -1.000*** (0.117)  -1.322*** (0.211)  -0.949*** (0.087)  -1.266*** (0.212)  -0.919*** (0.069)  -1.228*** (0.194)
    ------------------------------------------------------------------------------------------------------------------------------
    f2                            -                  -                  -                  -                  x                  x
    f1                            -                  -                  x                  x                  x                  x
    ------------------------------------------------------------------------------------------------------------------------------
    R2                        0.123              0.037              0.437              0.115              0.609              0.168
    S.E. type          by: group_id       by: group_id       by: group_id       by: group_id       by: group_id       by: group_id
    Observations                998                999                997                998                997                998
    ------------------------------------------------------------------------------------------------------------------------------
    Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001
    Format of coefficient cell:
    Coefficient (Std. Error)




### Adjust Standard Errors "on-the-fly"

Standard Errors can be adjusted after estimation, "on-the-fly":


```python
fit1 = fit.fetch_model(0)
fit1.vcov("hetero").summary()
```

    Model:  Y~X1
    ###

    Estimation:  OLS
    Dep. var.: Y
    Inference:  hetero
    Observations:  998

    | Coefficient   |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |
    |:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|
    | Intercept     |      0.919 |        0.112 |     8.223 |      0.000 |  0.699 |   1.138 |
    | X1            |     -1.000 |        0.082 |   -12.134 |      0.000 | -1.162 |  -0.838 |
    ---
    RMSE: 2.158   R2: 0.123


### Poisson Regression via `fepois()`

You can estimate Poisson Regressions via the `fepois()` function:


```python
poisson_data = pf.get_data(model = "Fepois")
pf.fepois("Y ~ X1 + X2 | f1 + f2", data = poisson_data).summary()
```

    ###

    Estimation:  Poisson
    Dep. var.: Y, Fixed effects: f1+f2
    Inference:  CRV1
    Observations:  997

    | Coefficient   |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |
    |:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|
    | X1            |     -0.007 |        0.035 |    -0.190 |      0.850 | -0.075 |   0.062 |
    | X2            |     -0.015 |        0.010 |    -1.449 |      0.147 | -0.035 |   0.005 |
    ---
    Deviance: 1068.169


### IV Estimation via three-part formulas

Last, `PyFixest` also supports IV estimation via three part formula
syntax:


```python
fit_iv = pf.feols("Y ~ 1 | f1 | X1 ~ Z1", data = data)
fit_iv.summary()
```

    ###

    Estimation:  IV
    Dep. var.: Y, Fixed effects: f1
    Inference:  CRV1
    Observations:  997

    | Coefficient   |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |
    |:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|
    | X1            |     -1.025 |        0.115 |    -8.930 |      0.000 | -1.259 |  -0.790 |
    ---

## Quantile Regression via `pf.quantreg`

```python
fit_qr = pf.quantreg("Y ~ X1 + X2", data = data, quantile = 0.5)
```


## Call for Contributions

Thanks for showing interest in contributing to `pyfixest`! We appreciate all
contributions and constructive feedback, whether that be reporting bugs, requesting
new features, or suggesting improvements to documentation.

**Upcoming:** We're hosting a [PyFixest Sprint in Heilbronn](pyfixest-sprint.md) with [AppliedAI](https://www.appliedai.de/) in late February/early March 2026. Interested in joining? [Learn more and get in touch](pyfixest-sprint.md).

If you'd like to get involved, but are not yet sure how, please feel free to send us an [email](alexander-fischer1801@t-online.de). Some familiarity with
either Python or econometrics will help, but you really don't need to be a `numpy` core developer or have published in [Econometrica](https://onlinelibrary.wiley.com/journal/14680262) =) We'd be more than happy to invest time to help you get started!

## Contributors ✨

Thanks goes to these wonderful people:

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/styfenschaer"><img src="https://avatars.githubusercontent.com/u/79762922?v=4?s=100" width="100px;" alt="styfenschaer"/><br /><sub><b>styfenschaer</b></sub></a><br /><a href="https://github.com/py-econometrics/pyfixest/commits?author=styfenschaer" title="Code">💻</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://www.nkeleher.com/"><img src="https://avatars.githubusercontent.com/u/5607589?v=4?s=100" width="100px;" alt="Niall Keleher"/><br /><sub><b>Niall Keleher</b></sub></a><br /><a href="#infra-NKeleher" title="Infrastructure (Hosting, Build-Tools, etc)">🚇</a> <a href="https://github.com/py-econometrics/pyfixest/commits?author=NKeleher" title="Code">💻</a></td>
      <td align="center" valign="top" width="14.28%"><a href="http://wenzhi-ding.com"><img src="https://avatars.githubusercontent.com/u/30380959?v=4?s=100" width="100px;" alt="Wenzhi Ding"/><br /><sub><b>Wenzhi Ding</b></sub></a><br /><a href="https://github.com/py-econometrics/pyfixest/commits?author=Wenzhi-Ding" title="Code">💻</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://apoorvalal.github.io/"><img src="https://avatars.githubusercontent.com/u/12086926?v=4?s=100" width="100px;" alt="Apoorva Lal"/><br /><sub><b>Apoorva Lal</b></sub></a><br /><a href="https://github.com/py-econometrics/pyfixest/commits?author=apoorvalal" title="Code">💻</a> <a href="https://github.com/py-econometrics/pyfixest/issues?q=author%3Aapoorvalal" title="Bug reports">🐛</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://juanitorduz.github.io"><img src="https://avatars.githubusercontent.com/u/22996444?v=4?s=100" width="100px;" alt="Juan Orduz"/><br /><sub><b>Juan Orduz</b></sub></a><br /><a href="#infra-juanitorduz" title="Infrastructure (Hosting, Build-Tools, etc)">🚇</a> <a href="https://github.com/py-econometrics/pyfixest/commits?author=juanitorduz" title="Code">💻</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://pyfixest.org/blog/"><img src="https://avatars.githubusercontent.com/u/19531450?v=4?s=100" width="100px;" alt="Alexander Fischer"/><br /><sub><b>Alexander Fischer</b></sub></a><br /><a href="https://github.com/py-econometrics/pyfixest/commits?author=py-econometrics" title="Code">💻</a> <a href="#infra-py-econometrics" title="Infrastructure (Hosting, Build-Tools, etc)">🚇</a></td>
      <td align="center" valign="top" width="14.28%"><a href="http://www.aeturrell.com"><img src="https://avatars.githubusercontent.com/u/11294320?v=4?s=100" width="100px;" alt="aeturrell"/><br /><sub><b>aeturrell</b></sub></a><br /><a href="#tutorial-aeturrell" title="Tutorials">✅</a> <a href="https://github.com/py-econometrics/pyfixest/commits?author=aeturrell" title="Documentation">📖</a> <a href="#promotion-aeturrell" title="Promotion">📣</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

## Acknowledgements

First and foremost, we want to acknowledge [Laurent Bergé's](https://sites.google.com/site/laurentrberge/) formidable [fixest](https://github.com/lrberge/fixest), which [is so good we decided to stick to its API and conventions](https://youtu.be/kSQxGGA7Rr4?si=8-wTbzLPnIZQ7lYI&t=576) as closely as Python allows.

For a full list of software packages and papers that have influenced PyFixest, please take a look at the [Acknowledgements page](https://pyfixest.org/acknowledgements.html).

We also want to thank all institutions that have funded or supported work on PyFixest!

<img src="../figures/aai-institute-logo.svg" width="185">


## How to Cite

If you want to cite PyFixest, you can use the following BibTeX entry:

```bibtex
@software{pyfixest,
  author  = {{The PyFixest Authors}},
  title   = {{pyfixest: Fast high-dimensional fixed effect estimation in Python}},
  year    = {2025},
  url     = {https://github.com/py-econometrics/pyfixest}
}
```

---

# pyfixest_gpu.html

## `PyFixest` on professional-tier GPUs 

`PyFixest` allows to run the fixed effects demeaning on the GPU via the `demeaner_backend` argument. 
To do so, you will have to install `jax` and `jaxblib`, for example by typing `pip install pyfixest[jax]`.

We test two back-ends for the iterative alternating-projections component of the fixed-effects regression on an Nvidia A100 GPU with 40 GB VRAM (a GPU that one typically wouldn't have installed to play graphics-intensive videogames on consumer hardware). `numba` benchmarks are run on a 12-core xeon CPU. 

The JAX backend exhibits major performance improvements **on the GPU** over numba in large problems. 

![](figures/gpu_benchmarks.png)


On the **CPU** instead, we find that `numba` outperforms the JAX backend. You can find details in the [benchmark section](https://github.com/py-econometrics/pyfixest/tree/master/benchmarks) of the github repo.

---

# quantile-regression.html

PyFixest now experimentally supports quantile regression!

```{python}
%load_ext autoreload

import pyfixest as pf
data = pf.get_data()
```

## Basic Example

Just as in `statsmodels`, the function that runs a quantile regression is `quantreg()`.

Below, we loop over 10 different quantiles.

```{python}
%%capture
fits = pf.quantreg(
  fml = "Y~X1 + X2",
  data = data,
  quantile=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
)
```

We can inspect the quantile regression results using the dedicated `qplot()` function.

```{python}
pf.qplot(fits, nrow = 2)
```

We observe some heterogeneity in the intercept, but all other variants are homogeneous across users.

## Solvers

By default, `pf.quantreg` uses an interior-point solver as in [Koenker and Ng (2004)](http://www.econ.uiuc.edu/~roger/research/sparse/fn3.pdf) (`methd = "fn"`). This is different to e.g. `statsmodels`, which implements an iterated weighted least squares solver.

For big data sets with many observations, it is often sensible to use an interior-point solver with pre-processing (as in [Portnoy and Koenker (1997)](https://experts.illinois.edu/en/publications/the-gaussian-hare-and-the-laplacian-tortoise-computability-of-squ), see [Chernozhukov et al (2019)](https://arxiv.org/abs/1909.05782) for details), which can speed up the estimation time significantly. Because the pre-processing step requires taking a random sample, the method assumes that observations are independent. Additionally, for the purpose of reproducibility, it is advisable to set a seed.

You can access the "preprocessing frisch-newton" algorithm by setting the `method` argument to `"pfn"`:

```{python}
%%capture
fit_fn = pf.quantreg(
  fml = "Y ~ X1",
  method = "fn",     # standard frisch newton interior point solver
  data = data,
)
fit_pfn = pf.quantreg(
  fml = "Y ~ X1",
  method = "pfn",   # standard frisch newton interior point solver with pre-processing
  seed = 92,         # set a seed for reproducibility
  data = data,
)

pf.etable([fit_fn, fit_pfn])
```

## Quantile Regression Process

Instead of running multiple independent quantile regression via a for-loop, the literature on quantile regression has developed multiple algorithms to speed up the "quantile regression process". Two such algorithms are described in detail in Chernozhukov, Fernandez-Val and Melly (2019) and are implemented in PyFixest. They can be accessed via the `multi_method` argument, and both can significantly speed up estimation time of the full quantile regression process.

```{python}
fml = "Y~X1"
method = "pfn"
seed = 929
quantiles = [0.1, 0.5, 0.9]

fit_multi1 = pf.quantreg(
  fml = fml,
  data = data,
  method = method,
  multi_method = "cfm1",  # this is algorithm 2 in CFM, the 1rst algorithm for the full qr process
  seed = seed,
  quantile = quantiles,
)

fit_multi2 = pf.quantreg(
  fml = fml,
  data = data,
  method = method,
  multi_method = "cfm2",  # this is algorithm 3 in CFM, the 2nd algorithm for the full qr process
  seed = seed,
  quantile = quantiles
)

pf.etable(fit_multi1.to_list() +  fit_multi2.to_list())
```

Note that the first method `cfm1` is exactly identical to running separate regressions per quantile, while the second method `cfm2` is only **asymptotically** identical.

You can combine different estimation `method`'s with different `multi_methods`:

```{python}
fit_multi2a = pf.quantreg(
  fml = "Y~X1",
  data = data,
  method = "fn",
  multi_method = "cfm1",
  seed = 233,
  quantile = [0.25, 0.75]
)

fit_multi2b = pf.quantreg(
  fml = "Y~X1",
  data = data,
  method = "pfn",
  multi_method = "cfm1",
  seed = 233,
  quantile = [0.25, 0.75]
)

pf.etable(fit_multi2a.to_list() +  fit_multi2b.to_list())

```

## Inference

By default, the `"iid", "hetero"` and cluster robust variance estimators implement (sandwich) estimators as in [Powell (1991)](https://econpapers.repec.org/paper/attwimass/8818.htm), using a uniform kernel to estimate the "sparsity".

The cluster robust estimator follows Parente & Santos Silva. See this [slide set](https://www.stata.com/meeting/uk15/abstracts/materials/uk15_santossilva.pdf) or the [Journal of Econometrics paper](https://repository.essex.ac.uk/8976/1/dp728.pdf) for details.

Additionally, `pf.quantreg` supports the `"nid"` ("non-iid") estimator from [Hendricks and Koenker (1991)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475175), which uses a linear approximation of the conditional quantile function.

```{python}
fit_nid = pf.quantreg("Y ~ X1 + X2 + f1", data = data, quantile = 0.5, vcov = "nid")
fit_crv = pf.quantreg("Y ~ X1 + X2 + f1", data = data, quantile = 0.5, vcov = {"CRV1": "f1"})
```

## Performance

Here we benchmark the performance of the solvers accessible via the `method` and `multi_method` arguments.

### Different Solvers

Tba.

### Quantile Regression Process

We fit a quantile regression process on $q = 0.1, 0.2, ..., 0.9$ quantiles and vary sample size and number of covariates. We test pyfixest's implementation of the quantile regression process against a "naive" for loop implementation via `statsmodels`. We can see that both `multi_method = "cmf1"` and `multi_method = "cmf2"` outperform
the for-loop strategy for large problems. Note that the plot is in log-scale!

![](figures/quantreg_benchmarks.png)

# Literature

- Victor Chernozhukov, Iván Fernández-Val, Blaise Melly (2019): Fast Algorithms for the Quantile Regression Process - [link](https://arxiv.org/abs/1909.05782)
- Hendricks & Koenker (1991): Hierarchical spline models for conditional quantiles and the demand for electricity - [link](https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475175)
- Koenker and Ng (2004): A Frisch-Newton Algorithm for Sparse Quantile Regression - [link](http://www.econ.uiuc.edu/~roger/research/sparse/fn3.pdf)
- Parente & Santos Silva (2015): Quantile Regression with Clustered Data - [link](https://econpapers.repec.org/article/bpjjecome/v_3a5_3ay_3a2016_3ai_3a1_3ap_3a1-15_3an_3a5.htm)
- Portnoy & Koenker (1997): The gaussian hare and the laplacian tortoise: Computability of squared-error versus absolute-error estimators - [link](https://experts.illinois.edu/en/publications/the-gaussian-hare-and-the-laplacian-tortoise-computability-of-squ)
- Powell (1991): Estimation of monotonic regression models under quantile restrictions - [link](https://econpapers.repec.org/paper/attwimass/8818.htm)

---

# quickstart.html

# OLS with Fixed Effects

## What is a fixed effect model?

A fixed effect model is a statistical model that includes fixed effects, which are parameters that are estimated to be constant across different groups.

**Example [Panel Data]:** In the context of panel data, fixed effects are parameters that are constant across different individuals or time. The typical model example is given by the following equation:

$$
Y_{it} = \beta X_{it} + \alpha_i + \psi_t + \varepsilon_{it}
$$

where $Y_{it}$ is the dependent variable for individual $i$ at time $t$, $X_{it}$ is the independent variable, $\beta$ is the coefficient of the independent variable, $\alpha_i$ is the individual fixed effect, $\psi_t$ is the time fixed effect, and $\varepsilon_{it}$ is the error term. The individual fixed effect $\alpha_i$ is a parameter that is constant across time for each individual, while the time fixed effect $\psi_t$ is a parameter that is constant across individuals for each time period.

Note however that, despite the fact that fixed effects are commonly used in panel setting, one does not need a panel data set to work with fixed effects. For example, cluster randomized trials with cluster fixed effects, or wage regressions with worker and firm fixed effects.

In this "quick start" guide, we will show you how to estimate a fixed effect model using the `PyFixest` package. We do not go into the details of the theory behind fixed effect models, but we focus on how to estimate them using `PyFixest`.

## Read Sample Data

In a first step, we load the module and some synthetic example data:

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
try:
    from lets_plot import LetsPlot
    _HAS_LETS_PLOT = True
except ImportError:
    _HAS_LETS_PLOT = False

from marginaleffects import slopes, avg_slopes

import pyfixest as pf

if _HAS_LETS_PLOT:
    LetsPlot.setup_html()

plt.style.use("seaborn-v0_8")

%load_ext watermark
%config InlineBackend.figure_format = "retina"
%watermark --iversions

data = pf.get_data()

data.head()
```

```{python}
data.info()
```

We see that some of our columns have missing data.

## OLS Estimation

We are interested in the relation between the dependent variable `Y` and the independent variables `X1` using a fixed effect model for `group_id`. Let's see how the data looks like:

```{python}
ax = data.plot(kind="scatter", x="X1", y="Y", c="group_id", colormap="viridis")
```

We can estimate a fixed effects regression via the `feols()` function. `feols()` has three arguments: a two-sided model formula, the data, and optionally, the type of inference.

```{python}
fit = pf.feols(fml="Y ~ X1 | group_id", data=data, vcov="HC1")
type(fit)
```


The first part of the formula contains the dependent variable and "regular" covariates, while the second part contains fixed effects.

`feols()` returns an instance of the `Fixest` class.

## Inspecting Model Results

To inspect the results, we can use a summary function or method:

```{python}
fit.summary()
```

Or display a formatted regression table:

```{python}
pf.etable(fit)
```

Alternatively, the `.summarize` module contains a `summary` function, which can be applied on instances of regression model objects or lists of regression model objects. For details on how to customize `etable()`, please take a look at the [dedicated vignette](https://pyfixest.org/table-layout.html).

```{python}
pf.summary(fit)
```

You can access individual elements of the summary via dedicated methods: `.tidy()` returns a "tidy" `pd.DataFrame`,
`.coef()` returns estimated parameters, and `se()` estimated standard errors. Other methods include `pvalue()`, `confint()`
and `tstat()`.

```{python}
fit.tidy()
```

```{python}
fit.coef()
```

```{python}
fit.se()
```

```{python}
fit.tstat()
```

```{python}
fit.confint()
```

Last, model results can be visualized via dedicated methods for plotting:

```{python}
fit.coefplot()
# or pf.coefplot([fit])
```

## How to interpret the results?

Let's have a quick d-tour on the intuition behind fixed effects models using the example above. To do so, let us begin by comparing it with a simple OLS model.

```{python}
fit_simple = pf.feols("Y ~ X1", data=data, vcov="HC1")

fit_simple.summary()
```

We can compare both models side by side in a regression table:

```{python}
pf.etable([fit, fit_simple])
```

We see that the `X1` coefficient is `-1.019`, which is less than the value from the OLS model in column (2). Where is the difference coming from?
Well, in the fixed effect model we are interested in controlling for the feature `group_id`. One possibility to do this is by adding a simple dummy variable for each level of `group_id`.

```{python}
fit_dummy = pf.feols("Y ~ X1 + C(group_id) ", data=data, vcov="HC1")

fit_dummy.summary()
```

This is does not scale well! Imagine you have 1000 different levels of `group_id`. You would need to add 1000 dummy variables to your model. This is where fixed effect models come in handy. They allow you to control for these fixed effects without adding all these dummy variables. The way to do it is by a *demeaning procedure*. The idea is to subtract the average value of each level of `group_id` from the respective observations. This way, we control for the fixed effects without adding all these dummy variables. Let's try to do this manually:

```{python}
def _demean_column(df: pd.DataFrame, column: str, by: str) -> pd.Series:
    return df[column] - df.groupby(by)[column].transform("mean")


fit_demeaned = pf.feols(
    fml="Y_demeaned ~ X1_demeaned",
    data=data.assign(
        Y_demeaned=lambda df: _demean_column(df, "Y", "group_id"),
        X1_demeaned=lambda df: _demean_column(df, "X1", "group_id"),
    ),
    vcov="HC1",
)

fit_demeaned.summary()
```

We get the same results as the fixed effect model `Y1 ~ X | group_id` above. The `PyFixest` package uses a more efficient algorithm to estimate the fixed effect model, but the intuition is the same.

## Updating Regression Coefficients

You can update the coefficients of a model object via the `update()` method, which may be useful in an online learning setting where data arrives sequentially.

To see this in action, let us first fit a model on a subset of the data:

```{python}
data_subsample = data.sample(frac=0.5)
m = pf.feols("Y ~ X1 + X2", data=data_subsample)
# current coefficient vector
m._beta_hat
```

Then sample 5 new observations and update the model with the new data. The update rule is

$$
\hat{\beta}_{n+1} = \hat{\beta}_n + (X_{n+1}' X_{n+1})^{-1} x_{n+1} + (y_{n+1} - x_{n+1} \hat{\beta}_n)
$$

for a new observation $(x_{n+1}, y_{n+1})$.

```{python}
new_points_id = np.random.choice(list(set(data.index) - set(data_subsample.index)), 5)
X_new, y_new = (
    np.c_[np.ones(len(new_points_id)), data.loc[new_points_id][["X1", "X2"]].values],
    data.loc[new_points_id]["Y"].values,
)
m.update(X_new, y_new)
```

We verify that we get the same results if we had estimated the model on the appended data.

```{python}
pf.feols(
    "Y ~ X1 + X2", data=data.loc[data_subsample.index.append(pd.Index(new_points_id))]
).coef().values
```

# Standard Errors and Inference

Supported covariance types are "iid", "HC1-3", CRV1 and CRV3 (up to two-way clustering).

**Why do we have so many different types of standard errors?**

The standard errors of the coefficients are crucial for inference. They tell us how certain we can be about the estimated coefficients. In the presence of heteroskedasticity (a situation which typically arises with cross-sectional data), the standard OLS standard errors are biased. The `pyfixest` package provides several types of standard errors that are robust to heteroskedasticity.

- `iid`: assumes that the error variance is spherical, i.e. errors are homoskedastic and not correlated (independent and identically distributed errors have a spherical error variance).
- `HC1-3`: heteroskedasticity-robust standard errors according to White (1980) and MacKinnon and White (1985). See [Econometric Computing with HC and HAC
Covariance Matrix Estimators](https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf) from the [`sandwich`](https://cran.r-project.org/web/packages/sandwich/) package for more details.
- `CRV1` and `CRV3`: cluster robust standard errors according to Cameron, Gelbach, and Miller (2011). See [A Practitioner's Guide to Cluster-Robust Inference](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf).  For   `CRV1` and `CRV3` one should pass a dictionaty of the form `{"CRV1": "clustervar"}`.

Inference can be adjusted "on-the-fly" via the `.vcov()` method:

```{python}
fit.vcov({"CRV1": "group_id + f2"}).summary()

fit.vcov({"CRV3": "group_id"}).summary()
```

The estimated covariance matrix is available as an attribute of the `Feols` object called `._vcov`.

## Inference via the Wild Bootstrap

It is also possible to run a wild (cluster) bootstrap after estimation (via the [wildboottest module](https://github.com/py-econometrics/wildboottest), see [MacKinnon, J. G., Nielsen, M. Ø., & Webb, M. D. (2023). Fast and reliable jackknife and bootstrap methods for cluster-robust inference. Journal of Applied Econometrics, 38(5), 671–694.](http://qed.econ.queensu.ca/pub/faculty/mackinnon/working-papers/qed_wp_1485.pdf)):

```{python}
fit2 = pf.feols(fml="Y ~ X1", data=data, vcov={"CRV1": "group_id"})
fit2.wildboottest(param="X1", reps=999)
```

## The Causal Cluster Variance Estimator

Additionally, `PyFixest` supports the causal cluster variance estimator following [Abadie et al. (2023)](https://academic.oup.com/qje/article/138/1/1/6750017). Let's look into it with another data set:

```{python}
df = pd.read_stata("http://www.damianclarke.net/stata/census2000_5pc.dta")

df.head()
```


```{python}
axes = df.plot.hist(column=["ln_earnings"], by=["college"])
```

Now we can estimate the model `ln_earnings ~ college` where we cluster the standard errors at the state level:

```{python}
fit3 = pf.feols("ln_earnings ~ college", vcov={"CRV1": "state"}, data=df)
fit3.ccv(treatment="college", pk=0.05, n_splits=2, seed=929)
```

## Randomization Inference

You can also conduct inference via randomization inference [(see Heß, Stata Journal 2017)](https://hesss.org/ritest.pdf).
`PyFixest` supports random and cluster random sampling.

```{python}
fit2.ritest(resampvar="X1=0", reps=1000, cluster="group_id")
```

## Multiple Testing Corrections: Bonferroni and Romano-Wolf

To correct for multiple testing, p-values can be adjusted via either the [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction), the method by Romano and Wolf (2005), see for example [The Romano-Wolf Multiple Hypothesis
Correction in Stata](https://docs.iza.org/dp12845.pdf), and the method by Westfall & Young (see [here](https://www.jstor.org/stable/2532216)).

```{python}
pf.bonferroni([fit, fit2], param="X1").round(3)
```

```{python}
pf.rwolf([fit, fit2], param="X1", reps=9999, seed=1234).round(3)
```

```{python}
pf.wyoung([fit, fit2], param="X1", reps=9999, seed=1234).round(3)
```

## Joint Confidence Intervals

Simultaneous confidence bands for a vector of parameters can be computed via the `joint_confint()` method. See [Simultaneous confidence bands: Theory, implementation, and an application to SVARs](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2656) for background.

```{python}
fit_ci = pf.feols("Y ~ X1+ C(f1)", data=data)
fit_ci.confint(joint=True).head()
```

# Panel Data Example: Causal Inference for the Brave and True

In this example we replicate the results of the great (freely available reference!) [Causal Inference for the Brave and True - Chapter 14](https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html). Please refer to the original text for a detailed explanation of the data.

```{python}
data_path = "https://raw.githubusercontent.com/bashtage/linearmodels/main/linearmodels/datasets/wage_panel/wage_panel.csv.bz2"
data_df = pd.read_csv(data_path)

data_df.head()
```

The objective is to estimate the effect of the variable `married` on the variable `lwage` using a fixed effect model on the entity variable `nr` and the time variable `year`.

```{python}
panel_fit = pf.feols(
    fml="lwage ~ expersq + union + married + hours | nr + year",
    data=data_df,
    vcov={"CRV1": "nr + year"},
)

pf.etable(panel_fit)
```

We obtain the same results as in the book!

# Instrumental Variables (IV) Estimation

It is also possible to estimate [instrumental variable models](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) with *one* endogenous variable and (potentially multiple) instruments.

In general, the syntax for IV is `depvar ~ exog.vars | fixef effects | endog.vars ~ instruments`.

```{python}
iv_fit = pf.feols(fml="Y2 ~ 1 | f1 + f2 | X1 ~ Z1 + Z2", data=data)
iv_fit.summary()
```

If the model does not contain any fixed effects, just drop the second part of the formula above:

```{python}
pf.feols(fml="Y ~ 1 | X1 ~ Z1 + Z2", data=data).summary()
```

You can access the first stage regression object via the `._model_1st_stage` attribute:

```{python}
pf.etable([iv_fit._model_1st_stage, iv_fit])
```

You can access the F-Statistic of the first stage via the `_f_stat_1st_stage` attribute:

```{python}
iv_fit._f_stat_1st_stage
```

Via the `IV_Diag` method, you can compute additional IV Diagnostics, as the **effective F-statistic** following Olea & Pflueger (2013):

```{python}
iv_fit.IV_Diag()
iv_fit._eff_F
```

IV estimation with multiple endogenous variables and multiple estimation syntax is currently not supported.

# Poisson Regression

It is possible to estimate Poisson Regressions (for example, to model count data). We can showcase this feature with another synthetic data set.


```{python}
pois_data = pf.get_data(model="Fepois")

ax = pois_data.plot(
    kind="scatter",
    x="X1",
    y="Y",
    c="group_id",
    colormap="viridis",
    s="f2",
)
```

```{python}
pois_fit = pf.fepois(fml="Y ~ X1 | group_id", data=pois_data, vcov={"CRV1": "group_id"})
pois_fit.summary()
```

# Quantile Regression

You can fit a quantile regression via the `quantreg` function:

```{python}
fit_qr = pf.quantreg("Y ~ X1 + X2", data = data, quantile = [0.1, 0.5, 0.9])
pf.qplot(fit_qr)
```

For details, take a look at the dedicated [quantreg vignette](https://pyfixest.org/quantile-regression.html).

# Tests of Multiple Hypothesis / Wald Tests

You can test multiple hypotheses simultaneously via the `wald_test` method.

```{python}
fit = pf.feols("Y ~ X1 + X2 | f1", data=data)
```

For example, to test the joint null hypothesis of $X_{1} = 0$ and $X_{2} = 0$ vs the alternative that $X_{1} \neq 0$ or $X_{2} \neq 0$, we would run

```{python}
fit.wald_test(R=np.eye(2))
```

Alternatively, suppose we wanted to test a more complicated joint null hypothesis:  $X_{1} + 2X_{2} = 2.0$ and $X_{2} = 1.0$. To do so, we would define $R$ and $q$ as

```{python}
R1 = np.array([[1, 2], [0, 1]])
q1 = np.array([2.0, 1.0])
fit.wald_test(R=R1, q=q1)
```

# Generalized Linear Models (GLMs) with Fixed Effects

`PyFixest` supports GLMs (logit, probit, gaussian) with high-dimensional fixed effects via `pf.feglm()`. Fixed effects are specified after the `|` symbol, just like in `feols()`.

```{python}
data_glm = pf.get_data()
data_glm["Y"] = np.where(data_glm["Y"] > 0, 1, 0)

# Logit with fixed effects
fit_fe = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="logit")
fit_fe.summary()
```

Fixed effects estimation via demeaning produces identical point estimates as one-hot encoding the fixed effects via `C()`:

```{python}
# Compare FE demeaning vs one-hot encoding
fit_onehot = pf.feglm("Y ~ X1 + X2 + C(f1)", data=data_glm, family="logit")

# Coefficients are identical
print("FE demeaning:", fit_fe.coef().values)
print("One-hot:     ", fit_onehot.coef()[["X1", "X2"]].values)
```

Note that standard errors differ between the two approaches due to different degrees of freedom adjustments.

You can compare different GLM families using `etable()`:

```{python}
fit_gaussian = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="gaussian")
fit_logit = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="logit")
fit_probit = pf.feglm("Y ~ X1 + X2 | f1", data=data_glm, family="probit")

pf.etable([fit_gaussian, fit_logit, fit_probit])
```

You can make predictions on the `response` and `link` scale via the `predict()` method:

```{python}
fit_logit.predict(type="response")[0:5]
fit_logit.predict(type="link")[0:5]
```

You can compute the **average marginal effect** via the [marginaleffects package](https://github.com/vincentarelbundock/pymarginaleffects):

```{python}
avg_slopes(fit_logit, variables="X1")
```

Please take a look at the [marginaleffects book](https://marginaleffects.com/) to learn about other transformations that the `marginaleffects` package supports.

# Multiple Estimation

`PyFixest` supports a range of multiple estimation functionality: `sw`, `sw0`, `csw`, `csw0`, and multiple dependent variables. The meaning of these options is explained in the [Multiple Estimations](https://lrberge.github.io/fixest/articles/multiple_estimations.html) vignette of the `fixest` package:

> - `sw`: this function is replaced sequentially by each of its arguments. For example, `y ~ x1 + sw(x2, x3)` leads to two estimations: `y ~ x1 + x2` and `y ~ x1 + x3`.
> - `sw0`: identical to sw but first adds the empty element. E.g. `y ~ x1 + sw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x2` and `y ~ x1 + x3`.
> - `csw`: it stands for cumulative stepwise. It adds to the formula each of its arguments sequentially. E.g. `y ~ x1 + csw(x2, x3)` will become `y ~ x1 + x2` and `y ~ x1 + x2 + x3`.
> - `csw0`: identical to csw but first adds the empty element. E.g. `y ~ x1 + csw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x`2 and `y ~ x1 + x2 + x3`.

Additionally, we support `split` and `fsplit` function arguments.
> - `split` allows to split a sample by a given variable. If specified, `pf.feols()` and `pf.fepois()` will loop through all resulting sample splits.
> - `fsplit` works just as `split`, but fits the model on the full sample as well.

If multiple regression syntax is used,
`feols()` and `fepois` returns an instance of a `FixestMulti` object, which essentially consists of a dicionary of `Fepois` or [Feols](/reference/estimation.models.feols_.Feols.qmd) instances.

```{python}
multi_fit = pf.feols(fml="Y ~ X1 | csw0(f1, f2)", data=data, vcov="HC1")
multi_fit
```

```{python}
multi_fit.etable()
```

You can access an individual model by its name - i.e. a formula - via the `all_fitted_models` attribute.

```{python}
multi_fit.all_fitted_models["Y ~ X1"].tidy()
```

or equivalently via the `fetch_model` method:

```{python}
multi_fit.fetch_model(0).tidy()
```

Here, `0` simply fetches the first model stored in the `all_fitted_models` dictionary, `1` the second etc.

Objects of type `Fixest` come with a range of additional methods: `tidy()`, `coef()`, `vcov()` etc, which
essentially loop over the equivalent methods of all fitted models. E.g. `Fixest.vcov()` updates inference for all
models stored in `Fixest`.


```{python}
multi_fit.vcov("iid").summary()
```

You can summarize multiple models at once via `etable()`. `etable()` has many options to customize the output to obtain publication-ready tables.

```{python}
pf.etable(
    [fit, fit2],
    labels={"Y": "Wage", "X1": "Age", "X2": "Years of Schooling"},
    felabels={"f1": "Industry Fixed Effects"},
    caption="Regression Results",
)
```

You can also visualize multiple estimation results via `iplot()` and `coefplot()`:

```{python}
multi_fit.coefplot().show()
```

# Difference-in-Differences / Event Study Designs

`PyFixest` supports eventy study designs via two-way fixed effects, Gardner's 2-stage estimator, and the linear projections estimator.

```{python}
url = "https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv"
df_het = pd.read_csv(url)

df_het.head()
```


```{python}
fit_did2s = pf.did2s(
    df_het,
    yname="dep_var",
    first_stage="~ 0 | state + year",
    second_stage="~i(rel_year,ref= -1.0)",
    treatment="treat",
    cluster="state",
)


fit_twfe = pf.feols(
    "dep_var ~ i(rel_year,ref = -1.0) | state + year",
    df_het,
    vcov={"CRV1": "state"},
)

from pyfixest.report.utils import rename_categoricals
pf.iplot(
    [fit_did2s, fit_twfe], coord_flip=False, figsize=(900, 400), title="TWFE vs DID2S", rotate_xticks=90,
    labels= rename_categoricals(fit_did2s._coefnames, template="{value_int}")
)
```


The `event_study()` function provides a common API for several event study estimators.

```{python}
fit_twfe = pf.event_study(
    data=df_het,
    yname="dep_var",
    idname="state",
    tname="year",
    gname="g",
    estimator="twfe",
)

fit_did2s = pf.event_study(
    data=df_het,
    yname="dep_var",
    idname="state",
    tname="year",
    gname="g",
    estimator="did2s",
)

pf.etable([fit_twfe, fit_did2s])
```

For more details see the vignette on [Difference-in-Differences Estimation](https://pyfixest.org/difference-in-differences.html).

---

# reference/did.estimation.did2s.html

# did.estimation.did2s { #pyfixest.did.estimation.did2s }

```python
did.estimation.did2s(
    data,
    yname,
    first_stage,
    second_stage,
    treatment,
    cluster,
    weights=None,
)
```

Estimate a Difference-in-Differences model using Gardner's two-step DID2S estimator.

## Parameters {.doc-section .doc-section-parameters}

| Name         | Type         | Description                                          | Default    |
|--------------|--------------|------------------------------------------------------|------------|
| data         | pd.DataFrame | The DataFrame containing all variables.              | _required_ |
| yname        | str          | The name of the dependent variable.                  | _required_ |
| first_stage  | str          | The formula for the first stage, starting with '~'.  | _required_ |
| second_stage | str          | The formula for the second stage, starting with '~'. | _required_ |
| treatment    | str          | The name of the treatment variable.                  | _required_ |
| cluster      | str          | The name of the cluster variable.                    | _required_ |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                            |
|--------|--------|----------------------------------------------------------------------------------------|
|        | object | A fitted model object of class [Feols](/reference/estimation.models.feols_.Feols.qmd). |

## Examples {.doc-section .doc-section-examples}

```{python}
import pandas as pd
import numpy as np
import pyfixest as pf

url = "https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv"
df_het = pd.read_csv(url)
df_het.head()
```

In a first step, we estimate a classical event study model:

```{python}
# estimate the model
fit = pf.did2s(
    df_het,
    yname="dep_var",
    first_stage="~ 0 | unit + year",
    second_stage="~i(rel_year, ref=-1.0)",
    treatment="treat",
    cluster="state",
)

fit.tidy().head()
```

We can also inspect the model visually:

```{python}
fit.iplot(figsize= [1200, 400], coord_flip=False).show()
```

To estimate a pooled effect, we need to slightly update the second stage formula:

```{python}
fit = pf.did2s(
    df_het,
    yname="dep_var",
    first_stage="~ 0 | unit + year",
    second_stage="~i(treat)",
    treatment="treat",
    cluster="state"
)
fit.tidy().head()
```

---

# reference/did.estimation.event_study.html

# did.estimation.event_study { #pyfixest.did.estimation.event_study }

```python
did.estimation.event_study(
    data,
    yname,
    idname,
    tname,
    gname,
    xfml=None,
    cluster=None,
    estimator='twfe',
    att=True,
)
```

Estimate Event Study Model.

This function allows for the estimation of treatment effects using different
estimators. Currently, it supports "twfe" for the two-way fixed effects
estimator and "did2s" for Gardner's two-step DID2S estimator. Other estimators
are in development.

## Parameters {.doc-section .doc-section-parameters}

| Name      | Type            | Description                                                                                                                                                          | Default    |
|-----------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| data      | DataFrame       | The DataFrame containing all variables.                                                                                                                              | _required_ |
| yname     | str             | The name of the dependent variable.                                                                                                                                  | _required_ |
| idname    | str             | The name of the id variable.                                                                                                                                         | _required_ |
| tname     | str             | Variable name for calendar period.                                                                                                                                   | _required_ |
| gname     | str             | Unit-specific time of initial treatment.                                                                                                                             | _required_ |
| cluster   | Optional\[str\] | The name of the cluster variable. If None, defaults to idname.                                                                                                       | `None`     |
| xfml      | str             | The formula for the covariates.                                                                                                                                      | `None`     |
| estimator | str             | The estimator to use. Options are "did2s", "twfe", and "saturated".                                                                                                  | `'twfe'`   |
| att       | bool            | If True, estimates the average treatment effect on the treated (ATT). If False, estimates the canonical event study design with all leads and lags. Default is True. | `True`     |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                            |
|--------|--------|----------------------------------------------------------------------------------------|
|        | object | A fitted model object of class [Feols](/reference/estimation.models.feols_.Feols.qmd). |

## Examples {.doc-section .doc-section-examples}

```{python}
import pandas as pd
import pyfixest as pf

url = "https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv"
df_het = pd.read_csv(url)

fit_twfe = pf.event_study(
    df_het,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    estimator="twfe",
    att=True,
)

fit_twfe.tidy()

# run saturated event study
fit_twfe_saturated = pf.event_study(
    df_het,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    estimator="saturated",
)

fit_twfe_saturated.aggregate()
fit_twfe_saturated.iplot_aggregate()
```

---

# reference/did.estimation.lpdid.html

# did.estimation.lpdid { #pyfixest.did.estimation.lpdid }

```python
did.estimation.lpdid(
    data,
    yname,
    idname,
    tname,
    gname,
    vcov=None,
    pre_window=None,
    post_window=None,
    never_treated=0,
    att=True,
    xfml=None,
)
```

Local projections approach to estimation.

Estimate a Difference-in-Differences / Event Study Model via the Local
Projections Approach.

## Parameters {.doc-section .doc-section-parameters}

| Name          | Type        | Description                                                                                                                            | Default    |
|---------------|-------------|----------------------------------------------------------------------------------------------------------------------------------------|------------|
| data          | DataFrame   | The DataFrame containing all variables.                                                                                                | _required_ |
| yname         | str         | The name of the dependent variable.                                                                                                    | _required_ |
| idname        | str         | The name of the id variable.                                                                                                           | _required_ |
| tname         | str         | Variable name for calendar period.                                                                                                     | _required_ |
| gname         | str         | Unit-specific time of initial treatment.                                                                                               | _required_ |
| vcov          | (str, dict) | The type of inference to employ. Defaults to {"CRV1": idname}. Options include "iid", "hetero", or a dictionary like {"CRV1": idname}. | `None`     |
| pre_window    | int         | The number of periods before the treatment to include in the estimation. Default is the minimum relative year in the data.             | `None`     |
| post_window   | int         | The number of periods after the treatment to include in the estimation. Default is the maximum relative year in the data.              | `None`     |
| never_treated | int         | Value in gname indicating units never treated. Default is 0.                                                                           | `0`        |
| att           | bool        | If True, estimates the pooled average treatment effect on the treated (ATT). Default is False.                                         | `True`     |
| xfml          | str         | Formula for the covariates. Not yet supported.                                                                                         | `None`     |

## Returns {.doc-section .doc-section-returns}

| Name   | Type      | Description                                  |
|--------|-----------|----------------------------------------------|
|        | DataFrame | A DataFrame with the estimated coefficients. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pandas as pd
import pyfixest as pf

url = "https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv"
df_het = pd.read_csv(url)

fit = pf.lpdid(
    df_het,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    vcov={"CRV1": "state"},
    pre_window=-20,
    post_window=20,
    att=False
)

fit.tidy().head()
fit.iplot(figsize= [1200, 400], coord_flip=False).show()
```

To get the ATT, set `att=True`:

```{python}
fit = pf.lpdid(
    df_het,
    yname="dep_var",
    idname="unit",
    tname="year",
    gname="g",
    vcov={"CRV1": "state"},
    pre_window=-20,
    post_window=20,
    att=True
)
fit.tidy()
```

---

# reference/did.visualize.panelview.html

# did.visualize.panelview { #pyfixest.did.visualize.panelview }

```python
did.visualize.panelview(
    data,
    unit,
    time,
    treat,
    outcome=None,
    collapse_to_cohort=False,
    subsamp=None,
    units_to_plot=None,
    sort_by_timing=False,
    xlab=None,
    ylab=None,
    figsize=(11, 3),
    noticks=False,
    title=None,
    legend=False,
    ax=None,
    xlim=None,
    ylim=None,
)
```

Generate a panel view of the treatment variable over time for each unit.

## Parameters {.doc-section .doc-section-parameters}

| Name               | Type                   | Description                                                                                   | Default    |
|--------------------|------------------------|-----------------------------------------------------------------------------------------------|------------|
| data               | pandas.DataFrame       | The input dataframe containing the data.                                                      | _required_ |
| unit               | str                    | The column name representing the unit identifier.                                             | _required_ |
| time               | str                    | The column name representing the time identifier.                                             | _required_ |
| treat              | str                    | The column name representing the treatment variable.                                          | _required_ |
| outcome            | str                    | The column name representing the outcome variable. If not None, an outcome plot is generated. | `None`     |
| collapse_to_cohort | bool                   | Whether to collapse units into treatment cohorts.                                             | `False`    |
| subsamp            | int                    | The number of samples to draw from data set for display (default is None).                    | `None`     |
| sort_by_timing     | bool                   | Whether to sort the treatment cohorts by the number of treated periods.                       | `False`    |
| xlab               | str                    | The label for the x-axis. Default is None, in which case default labels are used.             | `None`     |
| ylab               | str                    | The label for the y-axis. Default is None, in which case default labels are used.             | `None`     |
| figsize            | tuple                  | The figure size for the outcome plot. Default is (11, 3).                                     | `(11, 3)`  |
| noticks            | bool                   | Whether to display ticks on the plot. Default is False.                                       | `False`    |
| title              | str                    | The title for the plot. Default is None, in which case no title is displayed.                 | `None`     |
| legend             | bool                   | Whether to display a legend. Default is False (since binary treatments are self-explanatory). | `False`    |
| ax                 | matplotlib.pyplot.Axes | The axes on which to draw the plot. Default is None, in which case a new figure is created.   | `None`     |
| xlim               | tuple                  | The limits for the x-axis of the plot. Default is None.                                       | `None`     |
| ylim               | tuple                  | The limits for the y-axis of the plot. Default is None.                                       | `None`     |
| units_to_plot      | list                   | A list of unit to include in the plot. If None, all units in the dataset are plotted.         | `None`     |

## Returns {.doc-section .doc-section-returns}

| Name   | Type                   | Description   |
|--------|------------------------|---------------|
| ax     | matplotlib.pyplot.Axes |               |

## Examples {.doc-section .doc-section-examples}

```{python}
import pandas as pd
import numpy as np
import pyfixest as pf

url = "https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv"
df_het = pd.read_csv(url)

# Inspect treatment assignment
pf.panelview(
    data = df_het,
    unit = "unit",
    time = "year",
    treat = "treat",
    subsamp = 50,
    title = "Treatment Assignment"
)

# Outcome plot
pf.panelview(
    data = df_het,
    unit = "unit",
    time = "year",
    outcome = "dep_var",
    treat = "treat",
    subsamp = 50,
    title = "Outcome Plot"
)
```

---

# reference/estimation.FixestMulti_.FixestMulti.html

# estimation.FixestMulti_.FixestMulti { #pyfixest.estimation.FixestMulti_.FixestMulti }

```python
estimation.FixestMulti_.FixestMulti(
    data,
    copy_data,
    store_data,
    lean,
    fixef_tol,
    fixef_maxiter,
    weights_type,
    use_compression,
    reps,
    seed,
    split,
    fsplit,
    separation_check=None,
    context=0,
    quantreg_method='fn',
    quantreg_multi_method='cfm1',
)
```

A class to estimate multiple regression models with fixed effects.

## Methods

| Name | Description |
| --- | --- |
| [coef](#pyfixest.estimation.FixestMulti_.FixestMulti.coef) | Obtain the coefficients of the fitted models. |
| [confint](#pyfixest.estimation.FixestMulti_.FixestMulti.confint) | Obtain confidence intervals for the fitted models. |
| [fetch_model](#pyfixest.estimation.FixestMulti_.FixestMulti.fetch_model) | Fetch a model of class Feols from the Fixest class. |
| [pvalue](#pyfixest.estimation.FixestMulti_.FixestMulti.pvalue) | Obtain the p-values of the fitted models. |
| [se](#pyfixest.estimation.FixestMulti_.FixestMulti.se) | Obtain the standard errors of the fitted models. |
| [tidy](#pyfixest.estimation.FixestMulti_.FixestMulti.tidy) | Return the results of an estimation using `feols()` as a tidy Pandas DataFrame. |
| [to_list](#pyfixest.estimation.FixestMulti_.FixestMulti.to_list) | Return a list of all fitted models. |
| [tstat](#pyfixest.estimation.FixestMulti_.FixestMulti.tstat) | Obtain the t-statistics of the fitted models. |
| [vcov](#pyfixest.estimation.FixestMulti_.FixestMulti.vcov) | Update regression inference "on the fly". |
| [wildboottest](#pyfixest.estimation.FixestMulti_.FixestMulti.wildboottest) | Run a wild cluster bootstrap for all regressions in the Fixest object. |

### coef { #pyfixest.estimation.FixestMulti_.FixestMulti.coef }

```python
estimation.FixestMulti_.FixestMulti.coef()
```

Obtain the coefficients of the fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type          | Description                                                                                                            |
|--------|---------------|------------------------------------------------------------------------------------------------------------------------|
|        | pandas.Series | A pd.Series with coefficient names and Estimates. The key indicates which models the estimated statistic derives from. |

### confint { #pyfixest.estimation.FixestMulti_.FixestMulti.confint }

```python
estimation.FixestMulti_.FixestMulti.confint()
```

Obtain confidence intervals for the fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type          | Description                                                                                                                       |
|--------|---------------|-----------------------------------------------------------------------------------------------------------------------------------|
|        | pandas.Series | A pd.Series with coefficient names and confidence intervals. The key indicates which models the estimated statistic derives from. |

### fetch_model { #pyfixest.estimation.FixestMulti_.FixestMulti.fetch_model }

```python
estimation.FixestMulti_.FixestMulti.fetch_model(i, print_fml=True)
```

Fetch a model of class Feols from the Fixest class.

#### Parameters {.doc-section .doc-section-parameters}

| Name      | Type       | Description                                                 | Default    |
|-----------|------------|-------------------------------------------------------------|------------|
| i         | int or str | The index of the model to fetch.                            | _required_ |
| print_fml | bool       | Whether to print the formula of the model. Default is True. | `True`     |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type            | Description   |
|--------|-----------------|---------------|
|        | A Feols object. |               |

### pvalue { #pyfixest.estimation.FixestMulti_.FixestMulti.pvalue }

```python
estimation.FixestMulti_.FixestMulti.pvalue()
```

Obtain the p-values of the fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type          | Description                                                                                                           |
|--------|---------------|-----------------------------------------------------------------------------------------------------------------------|
|        | pandas.Series | A pd.Series with coefficient names and p-values. The key indicates which models the estimated statistic derives from. |

### se { #pyfixest.estimation.FixestMulti_.FixestMulti.se }

```python
estimation.FixestMulti_.FixestMulti.se()
```

Obtain the standard errors of the fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type          | Description                                                                                                                           |
|--------|---------------|---------------------------------------------------------------------------------------------------------------------------------------|
|        | pandas.Series | A pd.Series with coefficient names and standard error estimates. The key indicates which models the estimated statistic derives from. |

### tidy { #pyfixest.estimation.FixestMulti_.FixestMulti.tidy }

```python
estimation.FixestMulti_.FixestMulti.tidy()
```

Return the results of an estimation using `feols()` as a tidy Pandas DataFrame.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                    | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|--------|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | pandas.DataFrame or str | A tidy DataFrame with the following columns: - fml: the formula used to generate the results - Coefficient: the names of the coefficients - Estimate: the estimated coefficients - Std. Error: the standard errors of the estimated coefficients - t value: the t-values of the estimated coefficients - Pr(>\|t\|): the p-values of the estimated coefficients - 2.5%: the lower bound of the 95% confidence interval - 97.5%: the upper bound of the 95% confidence interval If `type` is set to "markdown", the resulting DataFrame will be returned as a markdown-formatted string with three decimal places. |

### to_list { #pyfixest.estimation.FixestMulti_.FixestMulti.to_list }

```python
estimation.FixestMulti_.FixestMulti.to_list()
```

Return a list of all fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                  | Description   |
|--------|-------------------------------------------------------|---------------|
|        | A list of all fitted models of types Feols or Fepois. |               |

### tstat { #pyfixest.estimation.FixestMulti_.FixestMulti.tstat }

```python
estimation.FixestMulti_.FixestMulti.tstat()
```

Obtain the t-statistics of the fitted models.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                           | Description                                                          |
|--------|----------------------------------------------------------------|----------------------------------------------------------------------|
|        | A pd.Series with coefficient names and estimated t-statistics. | The key indicates which models the estimated statistic derives from. |

### vcov { #pyfixest.estimation.FixestMulti_.FixestMulti.vcov }

```python
estimation.FixestMulti_.FixestMulti.vcov(vcov, vcov_kwargs=None)
```

Update regression inference "on the fly".

By calling vcov() on a "Fixest" object, all inference procedures applied
to the "Fixest" object are replaced with the variance-covariance matrix
specified via the method.

#### Parameters {.doc-section .doc-section-parameters}

| Name        | Type                            | Description                                                                                                                                                                                                                                                                                            | Default    |
|-------------|---------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| vcov        | Union\[str, dict\[str, str\]\]) | A string or dictionary specifying the type of variance-covariance matrix to use for inference. - If a string, can be one of "iid", "hetero", "HC1", "HC2", "HC3". - If a dictionary, it should have the format {"CRV1": "clustervar"} for CRV1 inference or {"CRV3": "clustervar"} for CRV3 inference. | _required_ |
| vcov_kwargs | Optional\[dict\[str, any\]\]    | Additional keyword arguments for the variance-covariance matrix.                                                                                                                                                                                                                                       | `None`     |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                        | Description   |
|--------|-------------------------------------------------------------|---------------|
|        | An instance of the \"Fixest\" class with updated inference. |               |

### wildboottest { #pyfixest.estimation.FixestMulti_.FixestMulti.wildboottest }

```python
estimation.FixestMulti_.FixestMulti.wildboottest(
    reps,
    cluster=None,
    param=None,
    weights_type='rademacher',
    impose_null=True,
    bootstrap_type='11',
    seed=None,
    k_adj=True,
    G_adj=True,
)
```

Run a wild cluster bootstrap for all regressions in the Fixest object.

#### Parameters {.doc-section .doc-section-parameters}

| Name           | Type               | Description                                                                                                                                                                                                                | Default        |
|----------------|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|
| B              | int                | The number of bootstrap iterations to run.                                                                                                                                                                                 | _required_     |
| param          | Union\[str, None\] | A string of length one, containing the test parameter of interest. Default is None.                                                                                                                                        | `None`         |
| cluster        | Union\[str, None\] | The name of the cluster variable. Default is None. If None, uses the `self._clustervar` attribute as the cluster variable. If the `self._clustervar` attribute is None, a heteroskedasticity-robust wild bootstrap is run. | `None`         |
| weights_type   | str                | The type of bootstrap weights. Either 'rademacher', 'mammen', 'webb', or 'normal'. Default is 'rademacher'.                                                                                                                | `'rademacher'` |
| impose_null    | bool               | Should the null hypothesis be imposed on the bootstrap dgp, or not? Default is True.                                                                                                                                       | `True`         |
| bootstrap_type | str                | A string of length one. Allows choosing the bootstrap type to be run. Either '11', '31', '13', or '33'. Default is '11'.                                                                                                   | `'11'`         |
| seed           | Union\[str, None\] | Option to provide a random seed. Default is None.                                                                                                                                                                          | `None`         |
| k_adj          | bool               | Whether to adjust the original coefficients with the bootstrap distribution. Default is True.                                                                                                                              | `True`         |
| G_adj          | bool               | Whether to adjust standard errors for clustering in the bootstrap. Default is True.                                                                                                                                        | `True`         |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type             | Description                                                                                                                     |
|--------|------------------|---------------------------------------------------------------------------------------------------------------------------------|
|        | pandas.DataFrame | A pd.DataFrame with bootstrapped t-statistic and p-value. The index indicates which model the estimated statistic derives from. |

---

# reference/estimation.api.feglm.feglm.html

# estimation.api.feglm.feglm { #pyfixest.estimation.api.feglm.feglm }

```python
estimation.api.feglm.feglm(
    fml,
    data,
    family,
    vcov=None,
    vcov_kwargs=None,
    ssc=None,
    fixef_rm='singleton',
    fixef_tol=1e-06,
    fixef_maxiter=100000,
    iwls_tol=1e-08,
    iwls_maxiter=25,
    collin_tol=1e-09,
    separation_check=None,
    solver='scipy.linalg.solve',
    demeaner_backend='numba',
    drop_intercept=False,
    copy_data=True,
    store_data=True,
    lean=False,
    context=None,
    split=None,
    fsplit=None,
    accelerate=True,
)
```

Estimate GLM regression models with fixed effects.

Supported families: [logit](/reference/estimation.models.felogit_.Felogit.qmd),
[probit](/reference/estimation.models.feprobit_.Feprobit.qmd),
[gaussian](/reference/estimation.models.fegaussian_.Fegaussian.qmd).

## References {.doc-section .doc-section-references}

- Bergé, L. (2018). Efficient estimation of maximum likelihood models with
  multiple fixed-effects: the R package FENmlm.
  [CREA Discussion Paper](https://ideas.repec.org/p/luc/wpaper/18-13.html).
- Correia, S., Guimaraes, P., & Zylkin, T. (2019). ppmlhdfe: Fast Poisson
  Estimation with High-Dimensional Fixed Effects.
  [The Stata Journal](https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909691).
- Stammann, A. (2018). Fast and Feasible Estimation of Generalized Linear
  Models with High-Dimensional k-way Fixed Effects.
  [arXiv:1707.01815](https://arxiv.org/pdf/1707.01815).

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Default                |
|------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
| fml              | str                                        | A two-sided formula string using fixest formula syntax. Syntax: "Y ~ X1 + X2 \| FE1 + FE2". "\|" separates left-hand side and fixed effects. Special syntax includes: - Stepwise regressions (sw, sw0) - Cumulative stepwise regression (csw, csw0) - Multiple dependent variables (Y1 + Y2 ~ X) - Interaction of variables (i(X1,X2)) - Interacted fixed effects (fe1^fe2) Compatible with formula parsing via the formulaic module.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | _required_             |
| data             | DataFrameType                              | A pandas or polars dataframe containing the variables in the formula.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | _required_             |
| family           | str                                        | The family of the GLM model. Options include "gaussian", "logit" and "probit".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | _required_             |
| vcov             | Union\[VcovTypeOptions, dict\[str, str\]\] | Type of variance-covariance matrix for inference. Options include "iid",   "hetero", "HC1", "HC2", "HC3", "NW" for Newey-West HAC standard errors, "DK" for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the `vcov_kwargs` argument. For time-series HAC, you need to pass the 'time_id' column. For panel-HAC, you need to add pass both 'time_id' and 'panel_id'. See `vcov_kwargs` for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `None`                 |
| vcov_kwargs      | Optional\[dict\[str, any\]\]               | Additional keyword arguments to pass to the vcov function. These keywoards include "lag" for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. "time_id" for the time ID used for NW and DK standard errors, and "panel_id" for the panel  identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time  periods is always treated as 1. More flexible time-step selection is work in progress.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`                 |
| ssc              | str                                        | A ssc object specifying the small sample correction for inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `None`                 |
| fixef_rm         | FixedRmOptions                             | Specifies whether to drop singleton fixed effects. Can be equal to "singleton" (default), or "none". "singletons" will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `'singleton'`          |
| fixef_tol        | float                                      | Tolerance for the fixed effects demeaning algorithm. Defaults to 1e-06. Currently does not do anything, as fixed effects are not supported for GLMs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `1e-06`                |
| fixef_maxiter    | int                                        | Maximum iterations for the demeaning algorithm. Currently does not do anything, as fixed effects are not supported for GLMs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `100000`               |
| iwls_tol         | Optional\[float\]                          | Tolerance for IWLS convergence, by default 1e-08.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `1e-08`                |
| iwls_maxiter     | Optional\[float\]                          | Maximum number of iterations for IWLS convergence, by default 25.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `25`                   |
| collin_tol       | float                                      | Tolerance for collinearity check, by default 1e-10.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `1e-09`                |
| separation_check | Optional\[list\[str\]\]                    | Methods to identify and drop separated observations. Either "fe" or "ir". Executes "fe" by default (when None).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `None`                 |
| solver           | SolverOptions, optional.                   | The solver to use for the regression. Can be "np.linalg.lstsq", "np.linalg.solve", "scipy.linalg.solve", "scipy.sparse.linalg.lsqr" and "jax". Defaults to "scipy.linalg.solve".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `'scipy.linalg.solve'` |
| demeaner_backend | DemeanerBackendOptions                     | The backend to use for demeaning. Options include: - "numba" (default): CPU-based demeaning using Numba JIT via the Alternating Projections Algorithm. - "rust": CPU-based demeaning implemented in Rust via the Alternating Projections Algorithm. - "jax": CPU or GPU-accelerated using JAX (requires jax/jaxlib) via the Alternating Projections Algorithm. - "cupy" or "cupy64": GPU-accelerated using CuPy with float64 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU if no GPU available) - "cupy32": GPU-accelerated using CuPy with float32 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU and float64 if no GPU available) - "scipy": Direct application of the Frisch-Waugh-Lovell Theorem on sparse matrice.   Forces to use a scipy-sparse backend even when cupy is installed and GPU is available. Defaults to "numba". | `'numba'`              |
| drop_intercept   | bool                                       | Whether to drop the intercept from the model, by default False.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `False`                |
| copy_data        | bool                                       | Whether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of `fepois`. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you'll find a column with interacted fixed effects in the data set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `True`                 |
| store_data       | bool                                       | Whether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the `data` attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. `predict()` or `vcov()`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `True`                 |
| lean             | bool                                       | False by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won't be possible afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `False`                |
| context          | int or Mapping\[str, Any\]                 | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| split            | Optional\[str\]                            | A character string, i.e. 'split = var'. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| fsplit           | Optional\[str\]                            | This argument is the same as split but also includes the full sample as the first estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`                 |
| accelerate       | bool                                       | Whether to use acceleration tricks developed in the ppmlhdfe paper (warm start and adaptive fixed effects tolerance) for models with fixed effects. Produces numerically identical results faster, so we recommend to always set it to True.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `True`                 |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|--------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | object | An instance of the [Feglm](/reference/estimation.models.feglm_.Feglm.qmd) class (or one of its subclasses: [Felogit](/reference/estimation.models.felogit_.Felogit.qmd), [Feprobit](/reference/estimation.models.feprobit_.Feprobit.qmd), [Fegaussian](/reference/estimation.models.fegaussian_.Fegaussian.qmd)) or an instance of class [FixestMulti](/reference/estimation.FixestMulti_.FixestMulti.qmd) for multiple models specified via `fml`. |

## Examples {.doc-section .doc-section-examples}

The following example regresses `Y` on `X1` and `X2` with fixed effects for
`f1` and `f2`: fixed effects are specified after the `|` symbol.

```{python}
import pyfixest as pf
import numpy as np

data = pf.get_data()
data["Y"] = np.where(data["Y"] > 0, 1, 0)
data["f1"] = np.where(data["f1"] > data["f1"].median(), "group1", "group2")

fit_probit = pf.feglm("Y ~ X1*f1", data, family = "probit")
fit_logit = pf.feglm("Y ~ X1*f1", data, family = "logit")
fit_gaussian = pf.feglm("Y ~ X1*f1", data, family = "gaussian")

pf.etable([fit_probit, fit_logit, fit_gaussian])
```

`PyFixest` integrates with the [marginaleffects](https://marginaleffects.com/bonus/python.html) package. For example, to compute average marginal effects
for the probit model above, you can use the following code:

```{python}
# we load polars as marginaleffects outputs pl.DataFrame's
import polars as pl
from marginaleffects import avg_slopes
results = [avg_slopes(model, variables  = "X1") for model in [fit_probit, fit_logit, fit_gaussian]]
pl.concat([r.to_polars() for r in results])
```

We can also compute marginal effects by group (group average marginal effects):

```{python}
avg_slopes(fit_probit, variables  = "X1", by = "f1")
```

We find homogeneous effects by "f1" in the probit model.

For more examples of other function arguments, please take a look at the documentation of the [feols()](https://pyfixest.org/reference/estimation.api.feols.html#pyfixest.estimation.api.feols)
function.

---

# reference/estimation.api.feols.feols.html

# estimation.api.feols.feols { #pyfixest.estimation.api.feols.feols }

```python
estimation.api.feols.feols(
    fml,
    data,
    vcov=None,
    vcov_kwargs=None,
    weights=None,
    ssc=None,
    fixef_rm='singleton',
    fixef_tol=1e-06,
    fixef_maxiter=10000,
    collin_tol=1e-09,
    drop_intercept=False,
    copy_data=True,
    store_data=True,
    lean=False,
    weights_type='aweights',
    solver='scipy.linalg.solve',
    demeaner_backend='numba',
    use_compression=False,
    reps=100,
    context=None,
    seed=None,
    split=None,
    fsplit=None,
)
```

Estimate a linear regression models with fixed effects using fixest formula syntax.

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Default                |
|------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
| fml              | str                                        | A three-sided formula string using fixest formula syntax. Syntax: "Y ~ X1 + X2 \| FE1 + FE2 \| X1 ~ Z1". "\|" separates dependent variable, fixed effects, and instruments. Special syntax includes stepwise regressions, cumulative stepwise regression, multiple dependent variables, interaction of variables (i(X1,X2)), and interacted fixed effects (fe1^fe2).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | _required_             |
| data             | DataFrameType                              | A pandas or polars dataframe containing the variables in the formula.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | _required_             |
| vcov             | Union\[VcovTypeOptions, dict\[str, str\]\] | Type of variance-covariance matrix for inference. Options include "iid",   "hetero", "HC1", "HC2", "HC3", "NW" for Newey-West HAC standard errors, "DK" for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the `vcov_kwargs` argument. For time-series HAC, you need to pass the 'time_id' column. For panel-HAC, you need to add pass both 'time_id' and 'panel_id'. See `vcov_kwargs` for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `None`                 |
| vcov_kwargs      | Optional\[dict\[str, any\]\]               | Additional keyword arguments to pass to the vcov function. These keywoards include "lag" for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. "time_id" for the time ID used for NW and DK standard errors, and "panel_id" for the panel  identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time  periods is always treated as 1. More flexible time-step selection is work in progress.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`                 |
| weights          | Union\[None, str\], optional.              | Default is None. Weights for WLS estimation. If None, all observations are weighted equally. If a string, the name of the column in `data` that contains the weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `None`                 |
| ssc              | str                                        | A ssc object specifying the small sample correction for inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `None`                 |
| fixef_rm         | FixedRmOptions                             | Specifies whether to drop singleton fixed effects. Can be equal to "singleton" (default), or "none". "singletons" will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `'singleton'`          |
| collin_tol       | float                                      | Tolerance for collinearity check, by default 1e-10.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `1e-09`                |
| fixef_tol        |                                            | Tolerance for the fixed effects demeaning algorithm. Defaults to 1e-06.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `1e-06`                |
| fixef_maxiter    | int                                        | Maximum number of iterations for the demeaning algorithm. Defaults to 100,000.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `10000`                |
| drop_intercept   | bool                                       | Whether to drop the intercept from the model, by default False.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `False`                |
| copy_data        | bool                                       | Whether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of `fepois`. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you'll find a column with interacted fixed effects in the data set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `True`                 |
| store_data       | bool                                       | Whether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the `data` attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. `predict()` or `vcov()`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `True`                 |
| lean             | bool                                       | False by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won't be possible afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `False`                |
| weights_type     | WeightsTypeOptions                         | Options include `aweights` or `fweights`. `aweights` implement analytic or precision weights, while `fweights` implement frequency weights. For details see this blog post: https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `'aweights'`           |
| solver           | SolverOptions, optional.                   | The solver to use for the regression. Can be "np.linalg.lstsq", "np.linalg.solve", "scipy.linalg.solve", "scipy.sparse.linalg.lsqr" and "jax". Defaults to "scipy.linalg.solve".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `'scipy.linalg.solve'` |
| demeaner_backend | DemeanerBackendOptions                     | The backend to use for demeaning. Options include: - "numba" (default): CPU-based demeaning using Numba JIT via the Alternating Projections Algorithm. - "rust": CPU-based demeaning implemented in Rust via the Alternating Projections Algorithm. - "jax": CPU or GPU-accelerated using JAX (requires jax/jaxlib) via the Alternating Projections Algorithm. - "cupy" or "cupy64": GPU-accelerated using CuPy with float64 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU if no GPU available) - "cupy32": GPU-accelerated using CuPy with float32 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU and float64 if no GPU available) - "scipy": Direct application of the Frisch-Waugh-Lovell Theorem on sparse matrice.   Forces to use a scipy-sparse backend even when cupy is installed and GPU is available. Defaults to "numba". | `'numba'`              |
| use_compression  | bool                                       | Whether to use sufficient statistics to losslessly fit the regression model on compressed data. False by default. If True, the model is estimated on compressed data, which can lead to a significant speed-up for large data sets. See the paper by Wong et al (2021) for more details https://arxiv.org/abs/2102.11297. Note that if `use_compression = True`, inference is lossless. If standard errors are clustered, a wild cluster bootstrap is employed. Parameters for the wild bootstrap can be specified via the `reps` and `seed` arguments. Additionally, note that for one-way fixed effects, the estimation method uses a Mundlak transform to "control" for the fixed effects. For two-way fixed effects, a two-way Mundlak transform is employed. For two-way fixed effects, the Mundlak transform is only identical to a two-way fixed effects model if the data set is a panel. We do not provide any checks for the panel status of the data set.                                               | `False`                |
| reps             | int                                        | Number of bootstrap repetitions. Only relevant for boostrap inference applied to compute cluster robust errors when `use_compression = True`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `100`                  |
| context          | int or Mapping\[str, Any\]                 | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| seed             | Optional\[int\]                            | Seed for the random number generator. Only relevant for boostrap inference applied to compute cluster robust errors when `use_compression = True`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `None`                 |
| split            | Optional\[str\]                            | A character string, i.e. 'split = var'. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| fsplit           | Optional\[str\]                            | This argument is the same as split but also includes the full sample as the first estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`                 |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                                                                                                                         |
|--------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | object | An instance of the [Feols](/reference/estimation.models.feols_.Feols.qmd) class or [FixestMulti](/reference/estimation.FixestMulti_.FixestMulti.qmd) class for multiple models specified via `fml`. |

## Examples {.doc-section .doc-section-examples}

As in `fixest`, the [feols()](/reference/estimation.api.feols.feols.qmd) function can be used to
estimate a simple linear regression model with fixed effects.
The following example regresses `Y` on `X1` and `X2` with fixed effects for
`f1` and `f2`: fixed effects are specified after the `|` symbol.

```{python}
import pyfixest as pf
import pandas as pd
import numpy as np

data = pf.get_data()

fit = pf.feols("Y ~ X1 + X2 | f1 + f2", data)
fit.summary()
```

Calling `feols()` returns an instance of the [Feols](/reference/estimation.models.feols_.Feols.qmd)
class. The `summary()` method can be used to print the results.

An alternative way to retrieve model results is via the `tidy()` method, which
returns a pandas dataframe with the estimated coefficients, standard errors,
t-statistics, and p-values.

```{python}
fit.tidy()
```

You can also access all elements in the tidy data frame by dedicated methods,
e.g. `fit.coef()` for the coefficients, `fit.se()` for the standard errors,
`fit.tstat()` for the t-statistics, and `fit.pval()` for the p-values, and
`fit.confint()` for the confidence intervals.

The employed type of inference can be specified via the `vcov` argument. For compatibility
with `fixest`, if vcov is not provided, `PyFixest` always employs "iid" inference by default
starting with pyfixest 0.31.0. Prior to pyfixest 0.31.0, if vcov was not provided, `PyFixest`
would cluster by the first fixed effect if no vcov was provided.

```{python}
fit1 = pf.feols("Y ~ X1 + X2 | f1 + f2", data, vcov="iid")
fit2 = pf.feols("Y ~ X1 + X2 | f1 + f2", data, vcov="hetero")
fit3 = pf.feols("Y ~ X1 + X2 | f1 + f2", data, vcov={"CRV1": "f1"})
```

Supported inference types are "iid", "hetero", "HC1", "HC2", "HC3", and
"CRV1"/"CRV3". Clustered standard errors are specified via a dictionary,
e.g. `{"CRV1": "f1"}` for CRV1 inference with clustering by `f1` or
`{"CRV3": "f1"}` for CRV3 inference with clustering by `f1`. For two-way
clustering, you can provide a formula string, e.g. `{"CRV1": "f1 + f2"}` for
CRV1 inference with clustering by `f1`.

```{python}
fit4 = pf.feols("Y ~ X1 + X2 | f1 + f2", data, vcov={"CRV1": "f1 + f2"})
```

Inference can be adjusted post estimation via the `vcov` method:

```{python}
fit.summary()
fit.vcov("iid").summary()
```

The `ssc` argument specifies the small sample correction for inference. In
general, `feols()` uses all of `fixest::feols()` defaults, but sets the
`fixef.K` argument to `"none"` whereas the `fixest::feols()` default is `"nested"`.
See here for more details: [link to github](https://github.com/py-econometrics/pyfixest/issues/260).

`feols()` supports a range of multiple estimation syntax, i.e. you can estimate
multiple models in one call. The following example estimates two models, one with
fixed effects for `f1` and one with fixed effects for `f2` using the `sw()` syntax.

```{python}
fit = pf.feols("Y ~ X1 + X2 | sw(f1, f2)", data)
type(fit)
```

The returned object is an instance of the `FixestMulti` class. You can access
the results of the first model via `fit.fetch_model(0)` and the results of
the second model via `fit.fetch_model(1)`. You can compare the model results
via the `etable()` function:

```{python}
pf.etable(fit)
```

Other supported multiple estimation syntax include `sw0()`, `csw()` and `csw0()`.
While `sw()` adds variables in a "stepwise" fashion, `csw()` does so cumulatively.

```{python}
fit = pf.feols("Y ~ X1 + X2 | csw(f1, f2)", data)
pf.etable(fit)
```

The `sw0()` and `csw0()` syntax are similar to `sw()` and `csw()`, but start
with a model that excludes the variables specified in `sw()` and `csw()`:

```{python}
fit = pf.feols("Y ~ X1 + X2 | sw0(f1, f2)", data)
pf.etable(fit)
```

The `feols()` function also supports multiple dependent variables. The following
example estimates two models, one with `Y1` as the dependent variable and one
with `Y2` as the dependent variable.

```{python}
fit = pf.feols("Y + Y2 ~ X1 | f1 + f2", data)
pf.etable(fit)
```

It is possible to combine different multiple estimation operators:

```{python}
fit = pf.feols("Y + Y2 ~ X1 | sw(f1, f2)", data)
pf.etable(fit)
```

In general, using muliple estimation syntax can improve the estimation time
as covariates that are demeaned in one model and are used in another model do
not need to be demeaned again: `feols()` implements a caching mechanism that
stores the demeaned covariates.

Additionally, you can fit models on different samples via the split and fsplit
arguments. The split argument splits the sample according to the variable
specified in the argument, while the fsplit argument also includes the full
sample in the estimation.

```{python}
fit = pf.feols("Y ~ X1 + X2 | f1 + f2", data, split = "f1")
pf.etable(fit)
```

Besides OLS, `feols()` also supports IV estimation via three part formulas:

```{python}
fit = pf.feols("Y ~  X2 | f1 + f2 | X1 ~ Z1", data)
fit.tidy()
```
Here, `X1` is the endogenous variable and `Z1` is the instrument. `f1` and `f2`
are the fixed effects, as before. To estimate IV models without fixed effects,
simply omit the fixed effects part of the formula:

```{python}
fit = pf.feols("Y ~  X2 | X1 ~ Z1", data)
fit.tidy()
```

Last, `feols()` supports interaction of variables via the `i()` syntax.
Documentation on this is tba.

You can pass custom transforms via the `context` argument. If you set `context = 0`, all
functions from the level of the call to `feols()` will be available:

```{python}
def _lspline(series: pd.Series, knots: list[float]) -> np.array:
    'Generate a linear spline design matrix for the input series based on knots.'
    vector = series.values
    columns = []

    for i, knot in enumerate(knots):
        column = np.minimum(vector, knot if i == 0 else knot - knots[i - 1])
        columns.append(column)
        vector = vector - column

    # Add the remainder as the last column
    columns.append(vector)

    # Combine columns into a design matrix
    return np.column_stack(columns)

spline_split = _lspline(data["X2"], [0, 1])
data["X2_0"] = spline_split[:, 0]
data["0_X2_1"] = spline_split[:, 1]
data["1_X2"] = spline_split[:, 2]

explicit_fit = pf.feols("Y ~ X2_0 + 0_X2_1 + 1_X2 | f1 + f2", data=data)
# set context = 0 to make _lspline available for feols' internal call to Formulaic.model_matrix
context_captured_fit = pf.feols("Y ~ _lspline(X2,[0,1]) | f1 + f2", data=data, context = 0)
# or provide it as a dict / mapping
context_captured_fit_map = pf.feols("Y ~ _lspline(X2,[0,1]) | f1 + f2", data=data, context = {"_lspline":_lspline})

pf.etable([explicit_fit, context_captured_fit, context_captured_fit_map])
```

After fitting a model via `feols()`, you can use the `predict()` method to
get the predicted values:

```{python}
fit = pf.feols("Y ~ X1 + X2 | f1 + f2", data)
fit.predict()[0:5]
```

The `predict()` method also supports a `newdata` argument to predict on new data,
which returns a numpy array of the predicted values:

```{python}
fit = pf.feols("Y ~ X1 + X2 | f1 + f2", data)
fit.predict(newdata=data)[0:5]
```

Last, you can plot the results of a model via the `coefplot()` method:

```{python}
fit = pf.feols("Y ~ X1 + X2 | f1 + f2", data)
fit.coefplot()
```

We can conduct a regression decomposition via the `decompose()` method, which implements
a regression decomposition following the method developed in Gelbach (2016):

```{python}
import re
import pyfixest as pf
from pyfixest.utils.dgps import gelbach_data

data_gelbach = gelbach_data(nobs = 1000)
fit = pf.feols("y ~ x1 + x21 + x22 + x23", data=data_gelbach)

# simple decomposition
res = fit.decompose(param = "x1")
res.etable()

# group covariates via "combine_covariates" argument
res = fit.decompose(param = "x1", combine_covariates={"g1": ["x21", "x22"], "g2": ["x23"]})
res.etable()

# group covariates via regex
res = fit.decompose(param="x1", combine_covariates={"g1": re.compile("x2[1-2]"), "g2": re.compile("x23")})
```

Objects of type `Feols` support a range of other methods to conduct inference.
For example, you can run a wild (cluster) bootstrap via the `wildboottest()` method:

```{python}
fit = pf.feols("Y ~ X1 + X2", data)
fit.wildboottest(param = "X1", reps=1000)
```
would run a wild bootstrap test for the coefficient of `X1` with 1000
bootstrap repetitions.

For a wild cluster bootstrap, you can specify the cluster variable
  via the `cluster` argument:

```{python}
fit.wildboottest(param = "X1", reps=1000, cluster="group_id")
```

The `ritest()` method can be used to conduct randomization inference:

```{python}
fit.ritest(resampvar = "X1", reps=1000)
```

Last, you can compute the cluster causal variance estimator by Athey et
al by using the `ccv()` method:

```{python}
import numpy as np
rng = np.random.default_rng(1234)
data["D"] = rng.choice([0, 1], size = data.shape[0])
fit_D = pf.feols("Y ~ D", data = data)
fit_D.ccv(treatment = "D", cluster = "group_id")
```

---

# reference/estimation.api.fepois.fepois.html

# estimation.api.fepois.fepois { #pyfixest.estimation.api.fepois.fepois }

```python
estimation.api.fepois.fepois(
    fml,
    data,
    vcov=None,
    vcov_kwargs=None,
    weights=None,
    weights_type='aweights',
    ssc=None,
    fixef_rm='singleton',
    fixef_tol=1e-06,
    fixef_maxiter=10000,
    iwls_tol=1e-08,
    iwls_maxiter=25,
    collin_tol=1e-09,
    separation_check=None,
    solver='scipy.linalg.solve',
    demeaner_backend='numba',
    drop_intercept=False,
    copy_data=True,
    store_data=True,
    lean=False,
    context=None,
    split=None,
    fsplit=None,
)
```

Estimate Poisson regression model with fixed effects using the `ppmlhdfe` algorithm.

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Default                |
|------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
| fml              | str                                        | A two-sided formula string using fixest formula syntax. Syntax: "Y ~ X1 + X2 \| FE1 + FE2". "\|" separates left-hand side and fixed effects. Special syntax includes: - Stepwise regressions (sw, sw0) - Cumulative stepwise regression (csw, csw0) - Multiple dependent variables (Y1 + Y2 ~ X) - Interaction of variables (i(X1,X2)) - Interacted fixed effects (fe1^fe2) Compatible with formula parsing via the formulaic module.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | _required_             |
| data             | DataFrameType                              | A pandas or polars dataframe containing the variables in the formula.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | _required_             |
| vcov             | Union\[VcovTypeOptions, dict\[str, str\]\] | Type of variance-covariance matrix for inference. Options include "iid",   "hetero", "HC1", "HC2", "HC3", "NW" for Newey-West HAC standard errors, "DK" for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the `vcov_kwargs` argument. For time-series HAC, you need to pass the 'time_id' column. For panel-HAC, you need to add pass both 'time_id' and 'panel_id'. See `vcov_kwargs` for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `None`                 |
| vcov_kwargs      | Optional\[dict\[str, any\]\]               | Additional keyword arguments to pass to the vcov function. These keywoards include "lag" for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. "time_id" for the time ID used for NW and DK standard errors, and "panel_id" for the panel  identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time  periods is always treated as 1. More flexible time-step selection is work in progress.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`                 |
| weights          | Union\[None, str\], optional.              | Default is None. Weights for weighted Poisson regression. If None, all observations are weighted equally. If a string, the name of the column in `data` that contains the weights.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `None`                 |
| weights_type     | WeightsTypeOptions                         | Options include `aweights` or `fweights`. `aweights` implement analytic or precision weights, while `fweights` implement frequency weights. Frequency weights are useful for compressed count data where identical observations are aggregated. For details see this blog post: https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `'aweights'`           |
| ssc              | str                                        | A ssc object specifying the small sample correction for inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `None`                 |
| fixef_rm         | FixedRmOptions                             | Specifies whether to drop singleton fixed effects. Can be equal to "singletons" (default) or "none". "singletons" will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `'singleton'`          |
| fixef_tol        | float                                      | Tolerance for the fixed effects demeaning algorithm. Defaults to 1e-06.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `1e-06`                |
| fixef_maxiter    | int                                        | Maximum number of iterations for the demeaning algorithm. Defaults to 100,000.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `10000`                |
| iwls_tol         | Optional\[float\]                          | Tolerance for IWLS convergence, by default 1e-08.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `1e-08`                |
| iwls_maxiter     | Optional\[float\]                          | Maximum number of iterations for IWLS convergence, by default 25.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `25`                   |
| collin_tol       | float                                      | Tolerance for collinearity check, by default 1e-10.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `1e-09`                |
| separation_check | Optional\[list\[str\]\]                    | Methods to identify and drop separated observations. Either "fe" or "ir". Executes "fe" by default (when None).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `None`                 |
| solver           | SolverOptions, optional.                   | The solver to use for the regression. Can be "np.linalg.lstsq", "np.linalg.solve", "scipy.linalg.solve", "scipy.sparse.linalg.lsqr" and "jax". Defaults to "scipy.linalg.solve".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `'scipy.linalg.solve'` |
| demeaner_backend | DemeanerBackendOptions                     | The backend to use for demeaning. Options include: - "numba" (default): CPU-based demeaning using Numba JIT via the Alternating Projections Algorithm. - "rust": CPU-based demeaning implemented in Rust via the Alternating Projections Algorithm. - "jax": CPU or GPU-accelerated using JAX (requires jax/jaxlib) via the Alternating Projections Algorithm. - "cupy" or "cupy64": GPU-accelerated using CuPy with float64 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU if no GPU available) - "cupy32": GPU-accelerated using CuPy with float32 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse   matrices (requires cupy & GPU, defaults to scipy/CPU and float64 if no GPU available) - "scipy": Direct application of the Frisch-Waugh-Lovell Theorem on sparse matrice.   Forces to use a scipy-sparse backend even when cupy is installed and GPU is available. Defaults to "numba". | `'numba'`              |
| drop_intercept   | bool                                       | Whether to drop the intercept from the model, by default False.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `False`                |
| copy_data        | bool                                       | Whether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of `fepois`. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you'll find a column with interacted fixed effects in the data set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `True`                 |
| store_data       | bool                                       | Whether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the `data` attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. `predict()` or `vcov()`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `True`                 |
| lean             | bool                                       | False by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won't be possible afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `False`                |
| context          | int or Mapping\[str, Any\]                 | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| split            | Optional\[str\]                            | A character string, i.e. 'split = var'. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`                 |
| fsplit           | Optional\[str\]                            | This argument is the same as split but also includes the full sample as the first estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`                 |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                                                                                                                                           |
|--------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | object | An instance of the [Fepois](/reference/estimation.models.fepois_.Fepois.qmd) class or an instance of class [FixestMulti](/reference/estimation.FixestMulti_.FixestMulti.qmd) for multiple models specified via `fml`. |

## Examples {.doc-section .doc-section-examples}

The `fepois()` function can be used to estimate a simple Poisson regression
model with fixed effects.
The following example regresses `Y` on `X1` and `X2` with fixed effects for
`f1` and `f2`: fixed effects are specified after the `|` symbol.

```{python}
import pyfixest as pf

data = pf.get_data(model = "Fepois")
fit = pf.fepois("Y ~ X1 + X2 | f1 + f2", data)
fit.summary()
```

For more examples on the use of other function arguments, please take a look at the documentation of the [feols()](https://pyfixest.org/reference/estimation.api.feols.html#pyfixest.estimation.api.feols) function.

---

# reference/estimation.api.quantreg.quantreg.html

# estimation.api.quantreg.quantreg { #pyfixest.estimation.api.quantreg.quantreg }

```python
estimation.api.quantreg.quantreg(
    fml,
    data,
    vcov='nid',
    quantile=0.5,
    method='fn',
    multi_method='cfm1',
    tol=1e-06,
    maxiter=None,
    ssc=None,
    collin_tol=1e-09,
    separation_check=None,
    drop_intercept=False,
    copy_data=True,
    store_data=True,
    lean=False,
    context=None,
    split=None,
    fsplit=None,
    seed=None,
)
```

Fit a quantile regression model using the interior point algorithm from Portnoy and Koenker (1997).
Note that the interior point algorithm assumes independent observations.

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Default    |
|------------------|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| fml              | str                                        | A two-sided formula string using fixest formula syntax. In contrast to `feols()` and `feglm()`, no fixed effects formula syntax is supported.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | _required_ |
| data             | DataFrameType                              | A pandas or polars dataframe containing the variables in the formula.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | _required_ |
| quantile         | float                                      | The quantile to estimate. Must be between 0 and 1.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `0.5`      |
| method           | QuantregMethodOptions                      | The method to use for the quantile regression. Currently, only "fn" is supported. In the future, will be either "fn" or "pfn". "fn" implements the Frisch-Newton interior point algorithm described in Portnoy and Koenker (1997). The "pfn" method implements a variant of the algorithm proposed by Portnoy and Koenker (1997) including preprocessing steps, which a) can speed up the algorithm if N is very large but b) assumes independent observations. For details, you can either take a look at the Portnoy and Koenker paper, or "Fast Algorithms for the Quantile Regression Process" by Chernozhukov, Fernández-Val, and Melly (2019). | `'fn'`     |
| multi_method     | QuantregMultiOptions                       | Controls the algorithm for running the quantile regression process. Only relevant if more than one quantile regression is fit in one `quantreg` call. Options are 'cmf1', which is the default and implements algorithm 2 from Chernozhukov et al, 'cmf2', which implements their algorithm 3, and 'none', which just loops over separate model calls.                                                                                                                                                                                                                                                                                               | `'cfm1'`   |
| tol              | float                                      | The tolerance for the algorithm. Defaults to 1e-06. As in R's quantreg package, the algorithm stops when the relative change in the duality gap is less than tol.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `1e-06`    |
| maxiter          | int                                        | The maximum number of iterations. If None, maxiter = the number of observations in the model (as in R's quantreg package via nit(3) = n).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `None`     |
| vcov             | Union\[VcovTypeOptions, dict\[str, str\]\] | Type of variance-covariance matrix for inference. Currently supported are "iid", "nid", and cluster robust errors, "iid" by default. All of "iid", "hetero"and "cluster" robust error are based on a kernel-based estimator as in Powell (1991). The "nid" method implements the robust sandwich estimator proposed in Hendricks and Koenker (1993). Any of "HC1 / HC2 / HC3 also works and is equivalent to "hetero". Cluster robust inference following Parente and Santos Silva (2016) can be specified via a dictionary with the keys "type" and "cluster". Only one-way clustering is supported.                                                | `'nid'`    |
| ssc              | dict\[str, Union\[str, bool\]\]            | A dictionary specifying the small sample correction for inference. If None, uses default settings from `ssc_func()`. Note that by default, R's quantreg and Stata's qreg2 do not use small sample corrections. To match their behavior, set `ssc = pf.ssc(adj = False, cluster_adj = False)`.                                                                                                                                                                                                                                                                                                                                                        | `None`     |
| collin_tol       | float                                      | Tolerance for collinearity check, by default 1e-10.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `1e-09`    |
| separation_check | list\[str\]                                | Methods to identify and drop separated observations. Not used in quantile regression.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `None`     |
| drop_intercept   | bool                                       | Whether to drop the intercept from the model, by default False.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `False`    |
| copy_data        | bool                                       | Whether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of `quantreg`.                                                                                                                                                                                                                                                                                                                                                                                                                                              | `True`     |
| store_data       | bool                                       | Whether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the `data` attribute of the model object.                                                                                                                                                                                                                                                                                                                                                                  | `True`     |
| lean             | bool                                       | False by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won't be possible afterwards.                                                                                                                                                                                                                                                                                                               | `False`    |
| context          | int or Mapping\[str, Any\]                 | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.                                                                                                                                                                                                                                                                                                                                                                                 | `None`     |
| split            | str                                        | A character string, i.e. 'split = var'. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.                                                                                                                                                                                                                                                                                                                                                                                 | `None`     |
| fsplit           | str                                        | This argument is the same as split but also includes the full sample as the first estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `None`     |
| seed             | Optional\[int\]                            | A random seed for reproducibility. If None, no seed is set. Only relevant for the "pfn" method. The "fn" method is deterministic and does not require a seed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `None`     |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                                                                                                                                    |
|--------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | object | An instance of the [Quantreg](/reference/estimation.quantreg.quantreg_.Quantreg.qmd) class or [FixestMulti](/reference/estimation.FixestMulti_.FixestMulti.qmd) class for multiple models specified via `fml`. |

## Examples {.doc-section .doc-section-examples}

The following example regresses `Y` on `X1` and `X2` at the median (0.5 quantile):

```{python}
import pyfixest as pf
import pandas as pd
import numpy as np

data = pf.get_data()

fit = pf.quantreg("Y ~ X1 + X2", data, quantile=0.5)
fit.summary()
```

For details around inference, estimation techniques, (fast) fitting and visualizing the full quantile regression
process, please take a look at the dedicated [vignette](https://pyfixest.org/quantile-regression.html).

---

# reference/estimation.deprecated.model_matrix_fixest_.model_matrix_fixest.html

# estimation.deprecated.model_matrix_fixest_.model_matrix_fixest { #pyfixest.estimation.deprecated.model_matrix_fixest_.model_matrix_fixest }

```python
estimation.deprecated.model_matrix_fixest_.model_matrix_fixest(
    FixestFormula,
    data,
    drop_singletons=False,
    weights=None,
    drop_intercept=False,
    context=0,
)
```

Create model matrices for fixed effects estimation.

This function processes the data and then calls
`formulaic.Formula.get_model_matrix()` to create the model matrices.

## Parameters {.doc-section .doc-section-parameters}

| Name            | Type                                                     | Description                                                                                                                                                                                                                                                          | Default    |
|-----------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| FixestFormula   | A pyfixest.estimation.FormulaParser.FixestFormula object | that contains information on the model formula, the formula of the first and second stage, dependent variable, covariates, fixed effects, endogenous variables (if any), and instruments (if any).                                                                   | _required_ |
| data            | pd.DataFrame                                             | The input DataFrame containing the data.                                                                                                                                                                                                                             | _required_ |
| drop_singletons | bool                                                     | Whether to drop singleton fixed effects. Default is False.                                                                                                                                                                                                           | `False`    |
| weights         | str or None                                              | A string specifying the name of the weights column in `data`. Default is None.                                                                                                                                                                                       | `None`     |
| data            | pd.DataFrame                                             | The input DataFrame containing the data.                                                                                                                                                                                                                             | _required_ |
| drop_intercept  | bool                                                     | Whether to drop the intercept from the model matrix. Default is False. If True, the intercept is dropped ex post from the model matrix created by formulaic.                                                                                                         | `False`    |
| context         | int or Mapping\[str, Any\]                               | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment. | `0`        |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|--------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | dict   | A dictionary with the following keys and value types: - 'Y' : pd.DataFrame     The dependent variable. - 'X' : pd.DataFrame     The Design Matrix. - 'fe' : Optional[pd.DataFrame]     The model's fixed effects. None if not applicable. - 'endogvar' : Optional[pd.DataFrame]     The model's endogenous variable(s), None if not applicable. - 'Z' : np.ndarray     The model's set of instruments (exogenous covariates plus instruments).     None if not applicable. - 'weights_df' : Optional[pd.DataFrame]     DataFrame containing weights, None if weights are not used. - 'na_index' : np.ndarray     Array indicating rows droppled beause of NA values or singleton     fixed effects. - 'na_index_str' : str     String representation of 'na_index'. - '_icovars' : Optional[list[str]]     List of variables interacted with i() syntax, None if not applicable. - 'X_is_empty' : bool     Flag indicating whether X is empty. - 'model_spec' : formulaic ModelSpec     The model specification used to create the model matrices. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf
from pyfixest.estimation.deprecated.model_matrix_fixest_ import model_matrix_fixest

data = pf.get_data()
fit = pf.feols("Y ~ X1 + f1 + f2", data=data)
FixestFormula = fit.FixestFormula

mm = model_matrix_fixest(FixestFormula, data)
mm
```

.. deprecated::
    This function will be deprecated in a future version.
    Use `pyfixest.estimation.formula.model_matrix.create_model_matrix()` with a `Formula` object instead.
    See https://pyfixest.org/reference/estimation.formula.model_matrix.ModelMatrix.html

---

# reference/estimation.formula.factor_interaction.factor_interaction.html

# estimation.formula.factor_interaction.factor_interaction { #pyfixest.estimation.formula.factor_interaction.factor_interaction }

```python
estimation.formula.factor_interaction.factor_interaction(
    data,
    var2=None,
    *,
    ref=None,
    ref2=None,
    bin=None,
    bin2=None,
)
```

Fixest-style i() operator for categorical encoding with interactions.

Args:
    data: The categorical variable
    var2: Optional second variable for interaction (continuous or categorical)
    ref: Reference level to drop from data
    ref2: Reference level to drop from var2 (if categorical)
    bin: Dict mapping new_level -> [old_levels] for binning

Naming convention (matches R fixest):
    i(cyl)           -> cyl::4, cyl::6, cyl::8
    i(cyl, ref=4)    -> cyl::6, cyl::8
    i(cyl, wt)       -> cyl::4:wt, cyl::6:wt, cyl::8:wt
    i(cyl, wt, ref=4) -> cyl::6:wt, cyl::8:wt

---

# reference/estimation.formula.model_matrix.ModelMatrix.html

# estimation.formula.model_matrix.ModelMatrix { #pyfixest.estimation.formula.model_matrix.ModelMatrix }

```python
estimation.formula.model_matrix.ModelMatrix(
    model_matrix,
    drop_rows,
    drop_singletons=True,
    drop_intercept=False,
)
```

A wrapper around formulaic.ModelMatrix for the specification of PyFixest models.

This class organizes and processes model matrices for econometric estimation,
extracting dependent and independent variables, fixed effects, instrumental
variables, and weights. It handles missing data, singleton observations,
and ensures proper formatting for estimation procedures.

## Attributes {.doc-section .doc-section-attributes}

| Name          | Type                 | Description                                                         |
|---------------|----------------------|---------------------------------------------------------------------|
| dependent     | pd.DataFrame         | The dependent variable(s) (left-hand side of the main equation).    |
| independent   | pd.DataFrame         | The independent variable(s) (right-hand side of the main equation). |
| fixed_effects | pd.DataFrame or None | Fixed effects variables, encoded as integers.                       |
| endogenous    | pd.DataFrame or None | Endogenous variables in instrumental variable specifications.       |
| instruments   | pd.DataFrame or None | Instrumental variables for IV estimation.                           |
| weights       | pd.DataFrame or None | Observation weights for weighted estimation.                        |
| model_spec    | formulaic.ModelSpec  | The underlying formulaic model specification.                       |
| na_index      | frozenset\[int\]     | Indices of rows that were dropped.                                  |

---

# reference/estimation.formula.parse.Formula.html

# estimation.formula.parse.Formula { #pyfixest.estimation.formula.parse.Formula }

```python
estimation.formula.parse.Formula(
    _second_stage,
    _fixed_effects=None,
    _first_stage=None,
)
```

A formulaic-compliant formula.

## Attributes

| Name | Description |
| --- | --- |
| [endogenous](#pyfixest.estimation.formula.parse.Formula.endogenous) | Endogenous variables of an instrumental variable specification. |
| [exogenous](#pyfixest.estimation.formula.parse.Formula.exogenous) | Exogenous aka covariates aka independent variables. |
| [first_stage](#pyfixest.estimation.formula.parse.Formula.first_stage) | The first stage formula of an instrumental variable specification. |
| [fixed_effects](#pyfixest.estimation.formula.parse.Formula.fixed_effects) | The fixed effects of a formula. |
| [formula](#pyfixest.estimation.formula.parse.Formula.formula) | Full fixest-style formula. |
| [second_stage](#pyfixest.estimation.formula.parse.Formula.second_stage) | The second stage formula. |

## Methods

| Name | Description |
| --- | --- |
| [parse](#pyfixest.estimation.formula.parse.Formula.parse) | Parse fixest-style formula. In case of multiple estimation syntax, |
| [parse_to_dict](#pyfixest.estimation.formula.parse.Formula.parse_to_dict) | Group parsed formulas into dictionary keyed by fixed effects. |

### parse { #pyfixest.estimation.formula.parse.Formula.parse }

```python
estimation.formula.parse.Formula.parse(formula)
```

Parse fixest-style formula. In case of multiple estimation syntax,
returns a list of multiple regression formulas.

### parse_to_dict { #pyfixest.estimation.formula.parse.Formula.parse_to_dict }

```python
estimation.formula.parse.Formula.parse_to_dict(formula)
```

Group parsed formulas into dictionary keyed by fixed effects.

---

# reference/estimation.internals.demean_.demean.html

# estimation.internals.demean_.demean { #pyfixest.estimation.internals.demean_.demean }

```python
estimation.internals.demean_.demean(
    x,
    flist,
    weights,
    tol=1e-08,
    maxiter=100000,
)
```

Demean an array.

Workhorse for demeaning an input array `x` based on the specified fixed
effects and weights via the alternating projections algorithm.

## Parameters {.doc-section .doc-section-parameters}

| Name    | Type          | Description                                                                                                    | Default    |
|---------|---------------|----------------------------------------------------------------------------------------------------------------|------------|
| x       | numpy.ndarray | Input array of shape (n_samples, n_features). Needs to be of type float.                                       | _required_ |
| flist   | numpy.ndarray | Array of shape (n_samples, n_factors) specifying the fixed effects. Needs to already be converted to integers. | _required_ |
| weights | numpy.ndarray | Array of shape (n_samples,) specifying the weights.                                                            | _required_ |
| tol     | float         | Tolerance criterion for convergence. Defaults to 1e-08.                                                        | `1e-08`    |
| maxiter | int           | Maximum number of iterations. Defaults to 100_000.                                                             | `100000`   |

## Returns {.doc-section .doc-section-returns}

| Name   | Type                         | Description                                                                                                                                   |
|--------|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|
|        | tuple\[numpy.ndarray, bool\] | A tuple containing the demeaned array of shape (n_samples, n_features) and a boolean indicating whether the algorithm converged successfully. |

## Examples {.doc-section .doc-section-examples}

```{python}
import numpy as np
import pyfixest as pf
from pyfixest.utils.dgps import get_blw
from pyfixest.estimation.internals.demean_ import demean
from formulaic import model_matrix

fml = "y ~ treat | state + year"

data = get_blw()
data.head()

Y, rhs = model_matrix(fml, data)
X = rhs[0].drop(columns="Intercept")
fe = rhs[1].drop(columns="Intercept")
YX = np.concatenate([Y, X], axis=1)

# to numpy
Y = Y.to_numpy()
X = X.to_numpy()
YX = np.concatenate([Y, X], axis=1)
fe = fe.to_numpy().astype(int)  # demean requires fixed effects as ints!

YX_demeaned, success = demean(YX, fe, weights = np.ones(YX.shape[0]))
Y_demeaned = YX_demeaned[:, 0]
X_demeaned = YX_demeaned[:, 1:]

print(np.linalg.lstsq(X_demeaned, Y_demeaned, rcond=None)[0])
print(pf.feols(fml, data).coef())
```

---

# reference/estimation.internals.detect_singletons_.detect_singletons.html

# estimation.internals.detect_singletons_.detect_singletons { #pyfixest.estimation.internals.detect_singletons_.detect_singletons }

```python
estimation.internals.detect_singletons_.detect_singletons(ids)
```

Detect singleton fixed effects in a dataset.

This function iterates over the columns of a 2D numpy array representing
fixed effects to identify singleton fixed effects.
An observation is considered a singleton if it is the only one in its group
(fixed effect identifier).

## Parameters {.doc-section .doc-section-parameters}

| Name   | Type       | Description                                                                                                                                                           | Default    |
|--------|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| ids    | np.ndarray | A 2D numpy array representing fixed effects, with a shape of (n_samples, n_features). Elements should be non-negative integers representing fixed effect identifiers. | _required_ |

## Returns {.doc-section .doc-section-returns}

| Name   | Type          | Description                                                                                         |
|--------|---------------|-----------------------------------------------------------------------------------------------------|
|        | numpy.ndarray | A boolean array of shape (n_samples,), indicating which observations have a singleton fixed effect. |

## Notes {.doc-section .doc-section-notes}

The algorithm iterates over columns to identify fixed effects. After each
column is processed, it updates the record of non-singleton rows. This approach
accounts for the possibility that removing an observation in one column can
lead to the emergence of new singletons in subsequent columns.

For performance reasons, the input array should be in column-major order.
Operating on a row-major array can lead to significant performance losses.

---

# reference/estimation.models.fegaussian_.Fegaussian.html

# estimation.models.fegaussian_.Fegaussian { #pyfixest.estimation.models.fegaussian_.Fegaussian }

```python
estimation.models.fegaussian_.Fegaussian(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    tol,
    maxiter,
    solver,
    store_data=True,
    copy_data=True,
    lean=False,
    sample_split_var=None,
    sample_split_value=None,
    separation_check=None,
    context=0,
    demeaner_backend='numba',
    accelerate=True,
)
```

Class for the estimation of a fixed-effects GLM with normal errors.

---

# reference/estimation.models.feglm_.Feglm.html

# estimation.models.feglm_.Feglm { #pyfixest.estimation.models.feglm_.Feglm }

```python
estimation.models.feglm_.Feglm(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    tol,
    maxiter,
    solver,
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    sample_split_var=None,
    sample_split_value=None,
    separation_check=None,
    context=0,
    accelerate=True,
)
```

Abstract base class for the estimation of a fixed-effects GLM model.

## Methods

| Name | Description |
| --- | --- |
| [get_fit](#pyfixest.estimation.models.feglm_.Feglm.get_fit) | Fit the GLM model via iterated weighted least squares. |
| [predict](#pyfixest.estimation.models.feglm_.Feglm.predict) | Return predicted values from regression model. |
| [prepare_model_matrix](#pyfixest.estimation.models.feglm_.Feglm.prepare_model_matrix) | Prepare model inputs for estimation. |
| [residualize](#pyfixest.estimation.models.feglm_.Feglm.residualize) | Residualize v and X by flist using weights. |
| [to_array](#pyfixest.estimation.models.feglm_.Feglm.to_array) | Turn estimation DataFrames to np arrays. |

### get_fit { #pyfixest.estimation.models.feglm_.Feglm.get_fit }

```python
estimation.models.feglm_.Feglm.get_fit()
```

Fit the GLM model via iterated weighted least squares.

The implementation follows ideas developed in
- Bergé (2018): https://ideas.repec.org/p/luc/wpaper/18-13.html
- Correia, Guimaraes, Zylkin (2019): https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909691
- Stamann (2018): https://arxiv.org/pdf/1707.01815

### predict { #pyfixest.estimation.models.feglm_.Feglm.predict }

```python
estimation.models.feglm_.Feglm.predict(
    newdata=None,
    atol=1e-06,
    btol=1e-06,
    type='link',
    se_fit=False,
    interval=None,
    alpha=0.05,
)
```

Return predicted values from regression model.

Return a flat np.array with predicted values of the regression model.
If new fixed effect levels are introduced in `newdata`, predicted values
for such observations
will be set to NaN.

#### Parameters {.doc-section .doc-section-parameters}

| Name     | Type                               | Description                                                                                                                                                                                                                                                                                                        | Default   |
|----------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| newdata  | Union\[None, pd.DataFrame\]        | A pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.                                                                                                                                                                                          | `None`    |
| atol     | Float                              | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                     | `1e-6`    |
| btol     | Float                              | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                             | `1e-6`    |
| type     | str                                | The type of prediction to be computed. Can be either "response" (default) or "link". If type="response", the output is at the level of the response variable, i.e., it is the expected predictor E(Y\|X). If "link", the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta. | `'link'`  |
| atol     | Float                              | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                          | `1e-6`    |
| btol     | Float                              | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                  | `1e-6`    |
| se_fit   | Optional\[bool\]                   | If True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.                                                                                                                                                              | `False`   |
| interval | Optional\[PredictionErrorOptions\] | The type of interval to compute. Can be either 'prediction' or None.                                                                                                                                                                                                                                               | `None`    |
| alpha    | float                              | The alpha level for the confidence interval. Defaults to 0.05. Only used if interval = "prediction" is not None.                                                                                                                                                                                                   | `0.05`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                              | Description                                                                                                                                                                                                                        |
|--------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | Union\[np.ndarray, pd.DataFrame\] | Returns a pd.Dataframe with columns "fit", "se_fit" and CIs if argument "interval=prediction". Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument "se_fit=True". |

### prepare_model_matrix { #pyfixest.estimation.models.feglm_.Feglm.prepare_model_matrix }

```python
estimation.models.feglm_.Feglm.prepare_model_matrix()
```

Prepare model inputs for estimation.

### residualize { #pyfixest.estimation.models.feglm_.Feglm.residualize }

```python
estimation.models.feglm_.Feglm.residualize(v, X, flist, weights, tol, maxiter)
```

Residualize v and X by flist using weights.

### to_array { #pyfixest.estimation.models.feglm_.Feglm.to_array }

```python
estimation.models.feglm_.Feglm.to_array()
```

Turn estimation DataFrames to np arrays.

---

# reference/estimation.models.feiv_.Feiv.html

# estimation.models.feiv_.Feiv { #pyfixest.estimation.models.feiv_.Feiv }

```python
estimation.models.feiv_.Feiv(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    solver='scipy.linalg.solve',
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    context=0,
    sample_split_var=None,
    sample_split_value=None,
)
```

Non user-facing class to estimate an IV model using a 2SLS estimator.

Inherits from the Feols class. Users should not directly instantiate this class,
but rather use the [feols()](/reference/estimation.api.feols.feols.qmd) function. Note that
no demeaning is performed in this class: demeaning is performed in the
FixestMulti class (to allow for caching of demeaned variables for multiple
estimation).

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type                                                                                                               | Description                                                                                               | Default                |
|------------------|--------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|------------------------|
| Y                | np.ndarray                                                                                                         | Dependent variable, a two-dimensional np.array.                                                           | _required_             |
| X                | np.ndarray                                                                                                         | Independent variables, a two-dimensional np.array.                                                        | _required_             |
| endgvar          | np.ndarray                                                                                                         | Endogenous Indenpendent variables, a two-dimensional np.array.                                            | _required_             |
| Z                | np.ndarray                                                                                                         | Instruments, a two-dimensional np.array.                                                                  | _required_             |
| weights          | np.ndarray                                                                                                         | Weights, a one-dimensional np.array.                                                                      | _required_             |
| coefnames_x      | list                                                                                                               | Names of the coefficients of X.                                                                           | _required_             |
| coefnames_z      | list                                                                                                               | Names of the coefficients of Z.                                                                           | _required_             |
| collin_tol       | float                                                                                                              | Tolerance for collinearity check.                                                                         | _required_             |
| solver           | Literal\[\'np.linalg.lstsq\', \'np.linalg.solve\', \'scipy.linalg.solve\', \'scipy.sparse.linalg.lsqr\', \'jax\'\] | "scipy.sparse.linalg.lsqr", "jax"], default is "scipy.linalg.solve". Solver to use for the estimation.    | `'scipy.linalg.solve'` |
| demeaner_backend | DemeanerBackendOptions                                                                                             | The backend to use for demeaning. Can be either "numba", "jax", or "rust". Defaults to "numba".           | `'numba'`              |
| weights_name     | Optional\[str\]                                                                                                    | Name of the weights variable.                                                                             | _required_             |
| weights_type     | Optional\[str\]                                                                                                    | Type of the weights variable. Either "aweights" for analytic weights or "fweights" for frequency weights. | _required_             |

## Attributes {.doc-section .doc-section-attributes}

| Name                    | Type         | Description                                                                                                                                                                                                                                                                                                                                                                                                   |
|-------------------------|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| _Z                      | np.ndarray   | Processed instruments after handling multicollinearity.                                                                                                                                                                                                                                                                                                                                                       |
| _weights_type_feiv      | str          | Type of the weights variable defined in Feiv class. Either "aweights" for analytic weights or "fweights" for frequency weights.                                                                                                                                                                                                                                                                               |
| _coefnames_z            | list         | Names of coefficients for Z after handling multicollinearity.                                                                                                                                                                                                                                                                                                                                                 |
| _collin_vars_z          | list         | Variables identified as collinear in Z.                                                                                                                                                                                                                                                                                                                                                                       |
| _collin_index_z         | list         | Indices of collinear variables in Z.                                                                                                                                                                                                                                                                                                                                                                          |
| _is_iv                  | bool         | Indicator if instrumental variables are used.                                                                                                                                                                                                                                                                                                                                                                 |
| _support_crv3_inference | bool         | Indicator for supporting CRV3 inference.                                                                                                                                                                                                                                                                                                                                                                      |
| _support_iid_inference  | bool         | Indicator for supporting IID inference.                                                                                                                                                                                                                                                                                                                                                                       |
| _tZX                    | np.ndarray   | Transpose of Z times X.                                                                                                                                                                                                                                                                                                                                                                                       |
| _tXZ                    | np.ndarray   | Transpose of X times Z.                                                                                                                                                                                                                                                                                                                                                                                       |
| _tZy                    | np.ndarray   | Transpose of Z times Y.                                                                                                                                                                                                                                                                                                                                                                                       |
| _tZZinv                 | np.ndarray   | Inverse of transpose of Z times Z.                                                                                                                                                                                                                                                                                                                                                                            |
| _beta_hat               | np.ndarray   | Estimated regression coefficients.                                                                                                                                                                                                                                                                                                                                                                            |
| _Y_hat_link             | np.ndarray   | Predicted values of the regression model.                                                                                                                                                                                                                                                                                                                                                                     |
| _u_hat                  | np.ndarray   | Residuals of the regression model.                                                                                                                                                                                                                                                                                                                                                                            |
| _scores                 | np.ndarray   | Scores used in the regression.                                                                                                                                                                                                                                                                                                                                                                                |
| _hessian                | np.ndarray   | Hessian matrix used in the regression.                                                                                                                                                                                                                                                                                                                                                                        |
| _bread                  | np.ndarray   | Bread matrix used in the regression.                                                                                                                                                                                                                                                                                                                                                                          |
| _pi_hat                 | np.ndarray   | Estimated coefficients from 1st stage regression                                                                                                                                                                                                                                                                                                                                                              |
| _X_hat                  | np.ndarray   | Predicted values of the 1st stage regression                                                                                                                                                                                                                                                                                                                                                                  |
| _v_hat                  | np.ndarray   | Residuals of the 1st stage regression                                                                                                                                                                                                                                                                                                                                                                         |
| _model_1st_stage        | Any          | feols object of 1st stage regression. It contains various results and diagnostics from the fixed effects OLS regression.                                                                                                                                                                                                                                                                                      |
| _endogvar_1st_stage     | np.ndarray   | Unweihgted Endogenous independent variable vector                                                                                                                                                                                                                                                                                                                                                             |
| _Z_1st_stage            | np.ndarray   | Unweighted instruments vector to be used for 1st stage                                                                                                                                                                                                                                                                                                                                                        |
| _non_exo_instruments    | list         | List of instruments name excluding exogenous independent vars.                                                                                                                                                                                                                                                                                                                                                |
| __p_iv                  | scalar       | Number of instruments listed in _non_exo_instruments                                                                                                                                                                                                                                                                                                                                                          |
| _f_stat_1st_stage       | scalar       | F-statistics of First Stage regression for evaluation of IV weakness. The computed F-statistics test the following null hypothesis : # H0 : beta_{z_1} = 0 & ... & beta_{z_{p_iv}} = 0 where z_1, ..., z_{p_iv} # are the instrument variables # H1 : H0 does not hold Note that this F-statistics is adjusted to heteroskedasticity / clusters if users set specification of variance-covariance matrix type |
| _eff_F                  | scalar       | Effective F-statistics of first stage regression as in Olea and Pflueger 2013                                                                                                                                                                                                                                                                                                                                 |
| _data                   | pd.DataFrame | The data frame used in the estimation. None if arguments `lean = True` or `store_data = False`.                                                                                                                                                                                                                                                                                                               |

## Raises {.doc-section .doc-section-raises}

| Name   | Type       | Description                          |
|--------|------------|--------------------------------------|
|        | ValueError | If Z is not a two-dimensional array. |

## Methods

| Name | Description |
| --- | --- |
| [IV_Diag](#pyfixest.estimation.models.feiv_.Feiv.IV_Diag) | Implement IV diagnostic tests. |
| [IV_weakness_test](#pyfixest.estimation.models.feiv_.Feiv.IV_weakness_test) | Implement IV weakness test (F-test). |
| [demean](#pyfixest.estimation.models.feiv_.Feiv.demean) | Demean instruments and endogeneous variable. |
| [drop_multicol_vars](#pyfixest.estimation.models.feiv_.Feiv.drop_multicol_vars) | Drop multicollinear variables in matrix of instruments Z. |
| [eff_F](#pyfixest.estimation.models.feiv_.Feiv.eff_F) | Compute Effective F stat (Olea and Pflueger 2013). |
| [first_stage](#pyfixest.estimation.models.feiv_.Feiv.first_stage) | Implement First stage regression. |
| [get_fit](#pyfixest.estimation.models.feiv_.Feiv.get_fit) | Fit a IV model using a 2SLS estimator. |
| [to_array](#pyfixest.estimation.models.feiv_.Feiv.to_array) | Transform estimation DataFrames to arrays. |
| [wls_transform](#pyfixest.estimation.models.feiv_.Feiv.wls_transform) | Transform variables for WLS estimation. |

### IV_Diag { #pyfixest.estimation.models.feiv_.Feiv.IV_Diag }

```python
estimation.models.feiv_.Feiv.IV_Diag(statistics=None)
```

Implement IV diagnostic tests.

#### Notes {.doc-section .doc-section-notes}

This method covers diagnostic tests related with IV regression.
We currently have IV weak tests only. More test will be updated
in future updates!

#### Parameters {.doc-section .doc-section-parameters}

| Name       | Type        | Description                      | Default   |
|------------|-------------|----------------------------------|-----------|
| statistics | list\[str\] | List of IV diagnostic statistics | `None`    |

#### Example {.doc-section .doc-section-example}

The following is an example usage of this method:

    ```{python}

    import numpy as np
    import pandas as pd
    from pyfixest.estimation import feols

    # Set random seed for reproducibility
    np.random.seed(1)

    # Number of observations
    n = 1000

    # Simulate the data
    # Instrumental variable
    z = np.random.binomial(1, 0.5, size=n)
    z2 = np.random.binomial(1, 0.5, size=n)

    # Endogenous variable
    d = 0.5 * z + 1.5 * z2 + np.random.normal(size=n)

    # Control variables
    c1 = np.random.normal(size=n)
    c2 = np.random.normal(size=n)

    # Outcome variable
    y = 1.0 + 1.5 * d + 0.8 * c1 + 0.5 * c2 + np.random.normal(size=n)

    # Cluster variable
    cluster = np.random.randint(1, 50, size=n)
    weights = np.random.uniform(1, 3, size=n)

    # Create a DataFrame
    data = pd.DataFrame({
        'd': d,
        'y': y,
        'z': z,
        'z2': z2,
        'c1': c1,
        'c2': c2,
        'cluster': cluster,
        'weights': weights
    })

    vcov_detail = "iid"

    # Fit OLS model
    fit_ols = feols("y ~ 1 + d + c1 + c2", data=data, vcov=vcov_detail)

    # Fit IV model
    fit_iv = feols("y ~ 1 + c1 + c2 | d ~ z", data=data,
             vcov=vcov_detail,
             weights="weights")
    fit_iv.first_stage()
    F_stat_pf = fit_iv._f_stat_1st_stage
    fit_iv.IV_Diag()
    F_stat_eff_pf = fit_iv._eff_F

    print("(Unadjusted) F stat :", F_stat_pf)
    print("Effective F stat :", F_stat_eff_pf)

    ```

### IV_weakness_test { #pyfixest.estimation.models.feiv_.Feiv.IV_weakness_test }

```python
estimation.models.feiv_.Feiv.IV_weakness_test(iv_diag_statistics=None)
```

Implement IV weakness test (F-test).

This method covers hetero-robust and clustered-robust F statistics.
It produces two statistics:

- self._f_stat_1st_stage: F statistics of first stage regression
- self._eff_F: Effective F statistics (Olea and Pflueger 2013)
               of first stage regression

#### Notes {.doc-section .doc-section-notes}

"self._f_stat_1st_stage" is adjusted to the specification of vcov.
If vcov_detail = "iid", F statistics is not adjusted,
otherwise it is always adjusted.

#### Parameters {.doc-section .doc-section-parameters}

| Name               | Type   | Description                    | Default   |
|--------------------|--------|--------------------------------|-----------|
| iv_diag_statistics | list   | List of IV weakness statistics | `None`    |

### demean { #pyfixest.estimation.models.feiv_.Feiv.demean }

```python
estimation.models.feiv_.Feiv.demean()
```

Demean instruments and endogeneous variable.

### drop_multicol_vars { #pyfixest.estimation.models.feiv_.Feiv.drop_multicol_vars }

```python
estimation.models.feiv_.Feiv.drop_multicol_vars()
```

Drop multicollinear variables in matrix of instruments Z.

### eff_F { #pyfixest.estimation.models.feiv_.Feiv.eff_F }

```python
estimation.models.feiv_.Feiv.eff_F()
```

Compute Effective F stat (Olea and Pflueger 2013).

### first_stage { #pyfixest.estimation.models.feiv_.Feiv.first_stage }

```python
estimation.models.feiv_.Feiv.first_stage()
```

Implement First stage regression.

### get_fit { #pyfixest.estimation.models.feiv_.Feiv.get_fit }

```python
estimation.models.feiv_.Feiv.get_fit()
```

Fit a IV model using a 2SLS estimator.

### to_array { #pyfixest.estimation.models.feiv_.Feiv.to_array }

```python
estimation.models.feiv_.Feiv.to_array()
```

Transform estimation DataFrames to arrays.

### wls_transform { #pyfixest.estimation.models.feiv_.Feiv.wls_transform }

```python
estimation.models.feiv_.Feiv.wls_transform()
```

Transform variables for WLS estimation.

---

# reference/estimation.models.felogit_.Felogit.html

# estimation.models.felogit_.Felogit { #pyfixest.estimation.models.felogit_.Felogit }

```python
estimation.models.felogit_.Felogit(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    tol,
    maxiter,
    solver,
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    context=0,
    sample_split_var=None,
    sample_split_value=None,
    separation_check=None,
    accelerate=True,
)
```

Class for the estimation of a fixed-effects logit model.

---

# reference/estimation.models.feols_.Feols.html

# estimation.models.feols_.Feols { #pyfixest.estimation.models.feols_.Feols }

```python
estimation.models.feols_.Feols(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    solver='np.linalg.solve',
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    context=0,
    sample_split_var=None,
    sample_split_value=None,
)
```

Non user-facing class to estimate a linear regression via OLS.

Users should not directly instantiate this class,
but rather use the [feols()](/reference/estimation.api.feols.feols.qmd) function. Note that
no demeaning is performed in this class: demeaning is performed in the
FixestMulti class (to allow for caching of demeaned variables for multiple
estimation).

## Parameters {.doc-section .doc-section-parameters}

| Name         | Type                       | Description                                                                                                                                                                                                                                                          | Default             |
|--------------|----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|
| Y            | np.ndarray                 | Dependent variable, a two-dimensional numpy array.                                                                                                                                                                                                                   | _required_          |
| X            | np.ndarray                 | Independent variables, a two-dimensional numpy array.                                                                                                                                                                                                                | _required_          |
| weights      | np.ndarray                 | Weights, a one-dimensional numpy array.                                                                                                                                                                                                                              | _required_          |
| collin_tol   | float                      | Tolerance level for collinearity checks.                                                                                                                                                                                                                             | _required_          |
| coefnames    | list\[str\]                | Names of the coefficients (of the design matrix X).                                                                                                                                                                                                                  | _required_          |
| weights_name | Optional\[str\]            | Name of the weights variable.                                                                                                                                                                                                                                        | _required_          |
| weights_type | Optional\[str\]            | Type of the weights variable. Either "aweights" for analytic weights or "fweights" for frequency weights.                                                                                                                                                            | _required_          |
| solver       | str, optional.             | The solver to use for the regression. Can be "np.linalg.lstsq", "np.linalg.solve", "scipy.linalg.solve", "scipy.sparse.linalg.lsqr" and "jax". Defaults to "scipy.linalg.solve".                                                                                     | `'np.linalg.solve'` |
| context      | int or Mapping\[str, Any\] | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment. | `0`                 |

## Attributes {.doc-section .doc-section-attributes}

| Name                         | Type                                                                       | Description                                                                                                                                                                                                                                                                                            |
|------------------------------|----------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| _method                      | str                                                                        | Specifies the method used for regression, set to "feols".                                                                                                                                                                                                                                              |
| _is_iv                       | bool                                                                       | Indicates whether instrumental variables are used, initialized as False.                                                                                                                                                                                                                               |
| _Y                           | np.ndarray                                                                 | The demeaned dependent variable, a two-dimensional numpy array.                                                                                                                                                                                                                                        |
| _X                           | np.ndarray                                                                 | The demeaned independent variables, a two-dimensional numpy array.                                                                                                                                                                                                                                     |
| _X_is_empty                  | bool                                                                       | Indicates whether the X array is empty.                                                                                                                                                                                                                                                                |
| _collin_tol                  | float                                                                      | Tolerance level for collinearity checks.                                                                                                                                                                                                                                                               |
| _coefnames                   | list                                                                       | Names of the coefficients (of the design matrix X).                                                                                                                                                                                                                                                    |
| _collin_vars                 | list                                                                       | Variables identified as collinear.                                                                                                                                                                                                                                                                     |
| _collin_index                | list                                                                       | Indices of collinear variables.                                                                                                                                                                                                                                                                        |
| _Z                           | np.ndarray                                                                 | Alias for the _X array, used for calculations.                                                                                                                                                                                                                                                         |
| _solver                      | str                                                                        | The solver used for the regression.                                                                                                                                                                                                                                                                    |
| _weights                     | np.ndarray                                                                 | Array of weights for each observation.                                                                                                                                                                                                                                                                 |
| _N                           | int                                                                        | Number of observations.                                                                                                                                                                                                                                                                                |
| _k                           | int                                                                        | Number of independent variables (or features).                                                                                                                                                                                                                                                         |
| _support_crv3_inference      | bool                                                                       | Indicates support for CRV3 inference.                                                                                                                                                                                                                                                                  |
| _data                        | Any                                                                        | Data used in the regression, to be enriched outside of the class.                                                                                                                                                                                                                                      |
| _fml                         | Any                                                                        | Formula used in the regression, to be enriched outside of the class.                                                                                                                                                                                                                                   |
| _has_fixef                   | bool                                                                       | Indicates whether fixed effects are used.                                                                                                                                                                                                                                                              |
| _fixef                       | Any                                                                        | Fixed effects used in the regression.                                                                                                                                                                                                                                                                  |
| _icovars                     | Any                                                                        | Internal covariates, to be enriched outside of the class.                                                                                                                                                                                                                                              |
| _ssc_dict                    | dict                                                                       | dictionary for sum of squares and cross products matrices.                                                                                                                                                                                                                                             |
| _tZX                         | np.ndarray                                                                 | Transpose of Z multiplied by X, set in get_fit().                                                                                                                                                                                                                                                      |
| _tXZ                         | np.ndarray                                                                 | Transpose of X multiplied by Z, set in get_fit().                                                                                                                                                                                                                                                      |
| _tZy                         | np.ndarray                                                                 | Transpose of Z multiplied by Y, set in get_fit().                                                                                                                                                                                                                                                      |
| _tZZinv                      | np.ndarray                                                                 | Inverse of the transpose of Z multiplied by Z, set in get_fit().                                                                                                                                                                                                                                       |
| _beta_hat                    | np.ndarray                                                                 | Estimated regression coefficients.                                                                                                                                                                                                                                                                     |
| _Y_hat_link                  | np.ndarray                                                                 | Prediction at the level of the explanatory variable, i.e., the linear predictor X @ beta.                                                                                                                                                                                                              |
| _Y_hat_response              | np.ndarray                                                                 | Prediction at the level of the response variable, i.e., the expected predictor E(Y\|X).                                                                                                                                                                                                                |
| _u_hat                       | np.ndarray                                                                 | Residuals of the regression model.                                                                                                                                                                                                                                                                     |
| _scores                      | np.ndarray                                                                 | Scores used in the regression analysis.                                                                                                                                                                                                                                                                |
| _hessian                     | np.ndarray                                                                 | Hessian matrix used in the regression.                                                                                                                                                                                                                                                                 |
| _bread                       | np.ndarray                                                                 | Bread matrix, used in calculating the variance-covariance matrix.                                                                                                                                                                                                                                      |
| _vcov_type                   | Any                                                                        | Type of variance-covariance matrix used.                                                                                                                                                                                                                                                               |
| _vcov_type_detail            | Any                                                                        | Detailed specification of the variance-covariance matrix type.                                                                                                                                                                                                                                         |
| _is_clustered                | bool                                                                       | Indicates if clustering is used in the variance-covariance calculation.                                                                                                                                                                                                                                |
| _clustervar                  | Any                                                                        | Variable used for clustering in the variance-covariance calculation.                                                                                                                                                                                                                                   |
| _G                           | Any                                                                        | Group information used in clustering.                                                                                                                                                                                                                                                                  |
| _ssc                         | Any                                                                        | Sum of squares and cross products matrix.                                                                                                                                                                                                                                                              |
| _vcov                        | np.ndarray                                                                 | Variance-covariance matrix of the estimated coefficients.                                                                                                                                                                                                                                              |
| _se                          | np.ndarray                                                                 | Standard errors of the estimated coefficients.                                                                                                                                                                                                                                                         |
| _tstat                       | np.ndarray                                                                 | T-statistics of the estimated coefficients.                                                                                                                                                                                                                                                            |
| _pvalue                      | np.ndarray                                                                 | P-values associated with the t-statistics.                                                                                                                                                                                                                                                             |
| _conf_int                    | np.ndarray                                                                 | Confidence intervals for the estimated coefficients.                                                                                                                                                                                                                                                   |
| _F_stat                      | Any                                                                        | F-statistic for the model, set in get_Ftest().                                                                                                                                                                                                                                                         |
| _fixef_dict                  | dict                                                                       | dictionary containing fixed effects estimates.                                                                                                                                                                                                                                                         |
| _alpha                       | pd.DataFrame                                                               | A DataFrame with the estimated fixed effects.                                                                                                                                                                                                                                                          |
| _sumFE                       | np.ndarray                                                                 | Sum of all fixed effects for each observation.                                                                                                                                                                                                                                                         |
| _rmse                        | float                                                                      | Root mean squared error of the model.                                                                                                                                                                                                                                                                  |
| _r2                          | float                                                                      | R-squared value of the model.                                                                                                                                                                                                                                                                          |
| _r2_within                   | float                                                                      | R-squared value computed on demeaned dependent variable.                                                                                                                                                                                                                                               |
| _adj_r2                      | float                                                                      | Adjusted R-squared value of the model.                                                                                                                                                                                                                                                                 |
| _adj_r2_within               | float                                                                      | Adjusted R-squared value computed on demeaned dependent variable.                                                                                                                                                                                                                                      |
| _solver                      | Literal\[\"np.linalg.lstsq\", \"np.linalg.solve\", \"scipy.linalg.solve\", | "scipy.sparse.linalg.lsqr", "jax"], default is "scipy.linalg.solve". Solver to use for the estimation.                                                                                                                                                                                                 |
| _demeaner_backend            | DemeanerBackendOptions                                                     |                                                                                                                                                                                                                                                                                                        |
| _data                        | pd.DataFrame                                                               | The data frame used in the estimation. None if arguments `lean = True` or `store_data = False`.                                                                                                                                                                                                        |
| _model_name                  | str                                                                        | The name of the model. Usually just the formula string. If split estimation is used, the model name will include the split variable and value.                                                                                                                                                         |
| _model_name_plot             | str                                                                        | The name of the model used when plotting and summarizing models. Usually identical to `_model_name`. This might be different when pf.summary() or pf.coefplot() are called and models with identical _model_name attributes are passed. In this case, the _model_name_plot attribute will be modified. |
| _quantile                    | Optional\[float\]                                                          | The quantile used for quantile regression. None if not a quantile regression.                                                                                                                                                                                                                          |
| # special for did            |                                                                            |                                                                                                                                                                                                                                                                                                        |
| _res_cohort_eventtime_dict   | Optional\[dict\[str, Any\]\]                                               |                                                                                                                                                                                                                                                                                                        |
| _yname                       | Optional\[str\]                                                            |                                                                                                                                                                                                                                                                                                        |
| _gname                       | Optional\[str\]                                                            |                                                                                                                                                                                                                                                                                                        |
| _tname                       | Optional\[str\]                                                            |                                                                                                                                                                                                                                                                                                        |
| _idname                      | Optional\[str\]                                                            |                                                                                                                                                                                                                                                                                                        |
| _att                         | Optional\[Any\]                                                            |                                                                                                                                                                                                                                                                                                        |
| test_treatment_heterogeneity | Callable\[..., Any\]                                                       |                                                                                                                                                                                                                                                                                                        |
| aggregate                    | Callable\[..., Any\]                                                       |                                                                                                                                                                                                                                                                                                        |
| iplot_aggregate              | Callable\[..., Any\]                                                       |                                                                                                                                                                                                                                                                                                        |

## Methods

| Name | Description |
| --- | --- |
| [add_fixest_multi_context](#pyfixest.estimation.models.feols_.Feols.add_fixest_multi_context) | Enrich Feols object. |
| [ccv](#pyfixest.estimation.models.feols_.Feols.ccv) | Compute the Causal Cluster Variance following Abadie et al (QJE 2023). |
| [decompose](#pyfixest.estimation.models.feols_.Feols.decompose) | Implement the Gelbach (2016) decomposition method for mediation analysis. |
| [demean](#pyfixest.estimation.models.feols_.Feols.demean) | Demean the dependent variable and covariates by the fixed effect(s). |
| [drop_multicol_vars](#pyfixest.estimation.models.feols_.Feols.drop_multicol_vars) | Detect and drop multicollinear variables. |
| [fixef](#pyfixest.estimation.models.feols_.Feols.fixef) | Compute the coefficients of (swept out) fixed effects for a regression model. |
| [get_fit](#pyfixest.estimation.models.feols_.Feols.get_fit) | Fit an OLS model. |
| [plot_ritest](#pyfixest.estimation.models.feols_.Feols.plot_ritest) | Plot the distribution of the Randomization Inference Statistics. |
| [predict](#pyfixest.estimation.models.feols_.Feols.predict) | Predict values of the model on new data. |
| [prepare_model_matrix](#pyfixest.estimation.models.feols_.Feols.prepare_model_matrix) | Prepare model matrices for estimation. |
| [ritest](#pyfixest.estimation.models.feols_.Feols.ritest) | Conduct Randomization Inference (RI) test against a null hypothesis of |
| [to_array](#pyfixest.estimation.models.feols_.Feols.to_array) | Convert estimation data frames to np arrays. |
| [update](#pyfixest.estimation.models.feols_.Feols.update) | Update coefficients for new observations using Sherman-Morrison formula. |
| [vcov](#pyfixest.estimation.models.feols_.Feols.vcov) | Compute covariance matrices for an estimated regression model. |
| [wald_test](#pyfixest.estimation.models.feols_.Feols.wald_test) | Conduct Wald test. |
| [wildboottest](#pyfixest.estimation.models.feols_.Feols.wildboottest) | Run a wild cluster bootstrap based on an object of type "Feols". |
| [wls_transform](#pyfixest.estimation.models.feols_.Feols.wls_transform) | Transform model matrices for WLS Estimation. |

### add_fixest_multi_context { #pyfixest.estimation.models.feols_.Feols.add_fixest_multi_context }

```python
estimation.models.feols_.Feols.add_fixest_multi_context(
    depvar,
    Y,
    _data,
    _ssc_dict,
    _k_fe,
    fval,
    store_data,
)
```

Enrich Feols object.

Enrich an instance of `Feols` Class with additional
attributes set in the `FixestMulti` class.

#### Parameters {.doc-section .doc-section-parameters}

| Name          | Type          | Description                                                             | Default    |
|---------------|---------------|-------------------------------------------------------------------------|------------|
| FixestFormula | FixestFormula | The formula(s) used for estimation encoded in a `FixestFormula` object. | _required_ |
| depvar        | str           | The dependent variable of the regression model.                         | _required_ |
| Y             | pd.Series     | The dependent variable of the regression model.                         | _required_ |
| _data         | pd.DataFrame  | The data used for estimation.                                           | _required_ |
| _ssc_dict     | dict          | A dictionary with the sum of squares and cross products matrices.       | _required_ |
| _k_fe         | int           | The number of fixed effects.                                            | _required_ |
| fval          | str           | The fixed effects formula.                                              | _required_ |
| store_data    | bool          | Indicates whether to save the data used for estimation in the object    | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description   |
|--------|--------|---------------|
|        | None   |               |

### ccv { #pyfixest.estimation.models.feols_.Feols.ccv }

```python
estimation.models.feols_.Feols.ccv(
    treatment,
    cluster=None,
    seed=None,
    n_splits=8,
    pk=1,
    qk=1,
)
```

Compute the Causal Cluster Variance following Abadie et al (QJE 2023).

#### Parameters {.doc-section .doc-section-parameters}

| Name      | Type   | Description                                                                                                                                         | Default    |
|-----------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| treatment |        | The name of the treatment variable.                                                                                                                 | _required_ |
| cluster   | str    | The name of the cluster variable. None by default. If None, uses the cluster variable from the model fit.                                           | `None`     |
| seed      | int    | An integer to set the random seed. Defaults to None.                                                                                                | `None`     |
| n_splits  | int    | The number of splits to use in the cross-fitting procedure. Defaults to 8.                                                                          | `8`        |
| pk        | float  | The proportion of sampled clusters. Defaults to 1, which corresponds to all clusters of the population being sampled.                               | `1`        |
| qk        | float  | The proportion of sampled observations within each cluster. Defaults to 1, which corresponds to all observations within each cluster being sampled. | `1`        |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type         | Description                                                                                     |
|--------|--------------|-------------------------------------------------------------------------------------------------|
|        | pd.DataFrame | A DataFrame with inference based on the "Causal Cluster Variance" and "regular" CRV1 inference. |

#### Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf
import numpy as np

data = pf.get_data()
data["D"] = np.random.choice([0, 1], size=data.shape[0])

fit = pf.feols("Y ~ D", data=data, vcov={"CRV1": "group_id"})
fit.ccv(treatment="D", pk=0.05, qk=0.5, n_splits=8, seed=123).head()
```

### decompose { #pyfixest.estimation.models.feols_.Feols.decompose }

```python
estimation.models.feols_.Feols.decompose(
    param=None,
    x1_vars=None,
    decomp_var=None,
    type='gelbach',
    cluster=None,
    combine_covariates=None,
    reps=1000,
    seed=None,
    nthreads=None,
    agg_first=None,
    only_coef=False,
    digits=4,
)
```

Implement the Gelbach (2016) decomposition method for mediation analysis.

Compares a short model `depvar on param` with the long model
specified in the original feols() call.

For details, take a look at
"When do covariates matter?" by Gelbach (2016, JoLe). You can find
an ungated version of the paper on SSRN under the following link:
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737 .

When the initial regression is weighted, weights are interpreted as frequency
weights. Inference is not yet supported for weighted models.

#### Parameters {.doc-section .doc-section-parameters}

| Name               | Type                                 | Description                                                                                                                                                                                                                                                                                                | Default     |
|--------------------|--------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|
| param              | str                                  | The name of the focal covariate whose effect is to be decomposed into direct and indirect components with respect to the rest of the right-hand side.                                                                                                                                                      | `None`      |
| x1_vars            | list\[str\]                          | A list of covariates that are included in both the baseline and the full regressions.                                                                                                                                                                                                                      | `None`      |
| decomp_var         | str                                  | The name of the focal covariate whose effect is to be decomposed into direct and indirect components with respect to the rest of the right-hand side.                                                                                                                                                      | `None`      |
| type               | str                                  | The type of decomposition method to use. Defaults to "gelbach", which currently is the only supported option.                                                                                                                                                                                              | `'gelbach'` |
| cluster            | Optional\[str\]                      | The name of the cluster variable. If None, uses the cluster variable from the model fit. Defaults to None.                                                                                                                                                                                                 | `None`      |
| combine_covariates | Optional\[dict\[str, list\[str\]\]\] | A dictionary that specifies which covariates to combine into groups. See the example for how to use this argument. Defaults to None.                                                                                                                                                                       | `None`      |
| reps               | int                                  | The number of bootstrap iterations to run. Defaults to 1000.                                                                                                                                                                                                                                               | `1000`      |
| seed               | int                                  | An integer to set the random seed. Defaults to None.                                                                                                                                                                                                                                                       | `None`      |
| nthreads           | int                                  | The number of threads to use for the bootstrap. Defaults to None. If None, uses all available threads minus one.                                                                                                                                                                                           | `None`      |
| agg_first          | bool                                 | If True, use the 'aggregate first' algorithm described in Gelbach (2016). False by default, unless combine_covariates is provided. Recommended to set to True if combine_covariates is argument is provided. As a rule of thumb, the more covariates are combined, the larger the performance improvement. | `None`      |
| only_coef          | bool                                 | Indicates whether to compute inference for the decomposition. Defaults to False. If True, skips the inference step and only returns the decomposition results.                                                                                                                                             | `False`     |
| digits             | int                                  | The number of digits to round the results to. Defaults to 4.                                                                                                                                                                                                                                               | `4`         |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                 | Description                                                                                                                 |
|--------|----------------------|-----------------------------------------------------------------------------------------------------------------------------|
|        | GelbachDecomposition | A GelbachDecomposition object with the decomposition results. Use `tidy()` and `etable()` to access the estimation results. |

#### Examples {.doc-section .doc-section-examples}

```{python}
import re
import pyfixest as pf
from pyfixest.utils.dgps import gelbach_data

data = gelbach_data(nobs = 1000)
fit = pf.feols("y ~ x1 + x21 + x22 + x23", data=data)

# simple decomposition
gb = fit.decompose(decomp_var = "x1", reps = 10, nthreads = 1)
type(gb)

gb.tidy()
gb = fit.decompose(decomp_var = "x1", reps = 10, nthreads = 1, x1_vars = ["x21"])
# combine covariates
gb = fit.decompose(decomp_var = "x1", reps = 10, nthreads = 1, combine_covariates = {"g1": ["x21", "x22"], "g2": ["x23"]})
# supress inference
gb = fit.decompose(decomp_var = "x1", reps = 10, nthreads = 1, combine_covariates = {"g1": ["x21", "x22"], "g2": ["x23"]}, only_coef = True)
# print results
gb.etable()

# group covariates via regex
res = fit.decompose(decomp_var="x1", combine_covariates={"g1": re.compile("x2[1-2]"), "g2": re.compile("x23")})
```

### demean { #pyfixest.estimation.models.feols_.Feols.demean }

```python
estimation.models.feols_.Feols.demean()
```

Demean the dependent variable and covariates by the fixed effect(s).

### drop_multicol_vars { #pyfixest.estimation.models.feols_.Feols.drop_multicol_vars }

```python
estimation.models.feols_.Feols.drop_multicol_vars()
```

Detect and drop multicollinear variables.

### fixef { #pyfixest.estimation.models.feols_.Feols.fixef }

```python
estimation.models.feols_.Feols.fixef(atol=1e-06, btol=1e-06)
```

Compute the coefficients of (swept out) fixed effects for a regression model.

This method creates the following attributes:
- `_alpha` (pd.DataFrame): A DataFrame with the estimated fixed effects.
- `_sumFE` (np.array): An array with the sum of fixed effects for each
observation (i = 1, ..., N).

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type   | Description                                                                                                                                            | Default   |
|--------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| atol   | Float  | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html         | `1e-6`    |
| btol   | Float  | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html | `1e-6`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                            | Description                                    |
|--------|---------------------------------|------------------------------------------------|
|        | dict\[str, dict\[str, float\]\] | A dictionary with the estimated fixed effects. |

### get_fit { #pyfixest.estimation.models.feols_.Feols.get_fit }

```python
estimation.models.feols_.Feols.get_fit()
```

Fit an OLS model.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description   |
|--------|--------|---------------|
|        | None   |               |

### plot_ritest { #pyfixest.estimation.models.feols_.Feols.plot_ritest }

```python
estimation.models.feols_.Feols.plot_ritest(plot_backend='lets_plot')
```

Plot the distribution of the Randomization Inference Statistics.

#### Parameters {.doc-section .doc-section-parameters}

| Name         | Type   | Description                                                                                     | Default       |
|--------------|--------|-------------------------------------------------------------------------------------------------|---------------|
| plot_backend | str    | The plotting backend to use. Defaults to "lets_plot". Alternatively, "matplotlib" is available. | `'lets_plot'` |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                        | Description   |
|--------|-----------------------------------------------------------------------------|---------------|
|        | A lets_plot or matplotlib figure with the distribution of the Randomization |               |
|        | Inference Statistics.                                                       |               |

### predict { #pyfixest.estimation.models.feols_.Feols.predict }

```python
estimation.models.feols_.Feols.predict(
    newdata=None,
    atol=1e-06,
    btol=1e-06,
    type='link',
    se_fit=False,
    interval=None,
    alpha=0.05,
)
```

Predict values of the model on new data.

Return a flat np.array with predicted values of the regression model.
If new fixed effect levels are introduced in `newdata`, predicted values
for such observations will be set to NaN.

#### Parameters {.doc-section .doc-section-parameters}

| Name     | Type                               | Description                                                                                                                                                    | Default   |
|----------|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| newdata  | DataFrameType                      | A narwhals compatible DataFrame (polars, pandas, duckdb, etc). If None (default), the data used for fitting the model is used.                                 | `None`    |
| type     | str                                | The type of prediction to be computed. Can be either "response" (default) or "link". For linear models, both are identical.                                    | `'link'`  |
| atol     | Float                              | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html                 | `1e-6`    |
| btol     | Float                              | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html         | `1e-6`    |
| type     | PredictionType                     | The type of prediction to be made. Can be either 'link' or 'response'.  Defaults to 'link'. 'link' and 'response' lead to identical results for linear models. | `'link'`  |
| se_fit   | Optional\[bool\]                   | If True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.          | `False`   |
| interval | Optional\[PredictionErrorOptions\] | The type of interval to compute. Can be either 'prediction' or None.                                                                                           | `None`    |
| alpha    | float                              | The alpha level for the confidence interval. Defaults to 0.05. Only used if interval = "prediction" is not None.                                               | `0.05`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                              | Description                                                                                                                                                                                                                        |
|--------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | Union\[np.ndarray, pd.DataFrame\] | Returns a pd.Dataframe with columns "fit", "se_fit" and CIs if argument "interval=prediction". Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument "se_fit=True". |

### prepare_model_matrix { #pyfixest.estimation.models.feols_.Feols.prepare_model_matrix }

```python
estimation.models.feols_.Feols.prepare_model_matrix()
```

Prepare model matrices for estimation.

### ritest { #pyfixest.estimation.models.feols_.Feols.ritest }

```python
estimation.models.feols_.Feols.ritest(
    resampvar,
    cluster=None,
    reps=100,
    type='randomization-c',
    rng=None,
    choose_algorithm='auto',
    store_ritest_statistics=False,
    level=0.95,
)
```

Conduct Randomization Inference (RI) test against a null hypothesis of
`resampvar = 0`.

#### Parameters {.doc-section .doc-section-parameters}

| Name                    | Type                | Description                                                                                                                                                                                                                                             | Default             |
|-------------------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|
| resampvar               | str                 | The name of the variable to be resampled.                                                                                                                                                                                                               | _required_          |
| cluster                 | str                 | The name of the cluster variable in case of cluster random assignment. If provided, `resampvar` is held constant within each `cluster`. Defaults to None.                                                                                               | `None`              |
| reps                    | int                 | The number of randomization iterations. Defaults to 100.                                                                                                                                                                                                | `100`               |
| type                    | str                 | The type of the randomization inference test. Can be "randomization-c" or "randomization-t". Note that the "randomization-c" is much faster, while the "randomization-t" is recommended by Wu & Ding (JASA, 2021).                                      | `'randomization-c'` |
| rng                     | np.random.Generator | A random number generator. Defaults to None.                                                                                                                                                                                                            | `None`              |
| choose_algorithm        | str                 | The algorithm to use for the computation. Defaults to "auto". The alternative is "fast" and "slow", and should only be used for running CI tests. Ironically, this argument is not tested for any input errors from the user! So please don't use it =) | `'auto'`            |
| include_plot            |                     | Whether to include a plot of the distribution p-values. Defaults to False.                                                                                                                                                                              | _required_          |
| store_ritest_statistics | bool                | Whether to store the simulated statistics of the RI procedure. Defaults to False. If True, stores the simulated statistics in the model object via the `ritest_statistics` attribute as a numpy array.                                                  | `False`             |
| level                   | float               | The level for the confidence interval of the randomization inference p-value. Defaults to 0.95.                                                                                                                                                         | `0.95`              |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                                        | Description   |
|--------|-----------------------------------------------------------------------------|---------------|
|        | A pd.Series with the regression coefficient of `resampvar` and the p-value  |               |
|        | of the RI test. Additionally, reports the standard error and the confidence |               |
|        | interval of the p-value.                                                    |               |

#### Examples {.doc-section .doc-section-examples}

```{python}

#| echo: true
#| results: asis
#| include: true

import pyfixest as pf
data = pf.get_data()
fit = pf.feols("Y ~ X1 + X2", data=data)

# Conduct a randomization inference test for the coefficient of X1
fit.ritest("X1", reps=1000)

# use randomization-t instead of randomization-c
fit.ritest("X1", reps=1000, type="randomization-t")

# store statistics for plotting
fit.ritest("X1", reps=1000, store_ritest_statistics=True)
```

### to_array { #pyfixest.estimation.models.feols_.Feols.to_array }

```python
estimation.models.feols_.Feols.to_array()
```

Convert estimation data frames to np arrays.

### update { #pyfixest.estimation.models.feols_.Feols.update }

```python
estimation.models.feols_.Feols.update(X_new, y_new, inplace=False)
```

Update coefficients for new observations using Sherman-Morrison formula.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type       | Description          |
|--------|------------|----------------------|
|        | np.ndarray | Updated coefficients |

### vcov { #pyfixest.estimation.models.feols_.Feols.vcov }

```python
estimation.models.feols_.Feols.vcov(vcov, vcov_kwargs=None, data=None)
```

Compute covariance matrices for an estimated regression model.

#### Parameters {.doc-section .doc-section-parameters}

| Name        | Type                           | Description                                                                                                                                                                                                                                                                                                                                                                              | Default    |
|-------------|--------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| vcov        | Union\[str, dict\[str, str\]\] | A string or dictionary specifying the type of variance-covariance matrix to use for inference. If a string, it can be one of "iid", "hetero", "HC1", "HC2", "HC3", "NW", "DK". If a dictionary, it should have the format {"CRV1": "clustervar"} for CRV1 inference or {"CRV3": "clustervar"} for CRV3 inference. Note that CRV3 inference is currently not supported for IV estimation. | _required_ |
| vcov_kwargs | Optional\[dict\[str, any\]\]   | Additional keyword arguments for the variance-covariance matrix.                                                                                                                                                                                                                                                                                                                         | `None`     |
| data        | Optional\[DataFrameType\]      | The data used for estimation. If None, tries to fetch the data from the model object. Defaults to None.                                                                                                                                                                                                                                                                                  | `None`     |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                                                                                         |
|--------|--------|-----------------------------------------------------------------------------------------------------|
|        | Feols  | An instance of class [Feols](/reference/estimation.models.feols_.Feols.qmd) with updated inference. |

### wald_test { #pyfixest.estimation.models.feols_.Feols.wald_test }

```python
estimation.models.feols_.Feols.wald_test(R=None, q=None, distribution='F')
```

Conduct Wald test.

Compute a Wald test for a linear hypothesis of the form R * beta = q.
where R is m x k matrix, beta is a k x 1 vector of coefficients,
and q is m x 1 vector.
By default, tests the joint null hypothesis that all coefficients are zero.

This method producues the following attriutes

_dfd : int
    degree of freedom in denominator
_dfn : int
    degree of freedom in numerator
_wald_statistic : scalar
    Wald-statistics computed for hypothesis testing
_f_statistic : scalar
    Wald-statistics(when R is an indentity matrix, and q being zero vector)
    computed for hypothesis testing
_p_value : scalar
    corresponding p-value for statistics

#### Parameters {.doc-section .doc-section-parameters}

| Name         | Type         | Description                                                                            | Default   |
|--------------|--------------|----------------------------------------------------------------------------------------|-----------|
| R            | array - like | The matrix R of the linear hypothesis. If None, defaults to an identity matrix.        | `None`    |
| q            | array - like | The vector q of the linear hypothesis. If None, defaults to a vector of zeros.         | `None`    |
| distribution | str          | The distribution to use for the p-value. Can be either "F" or "chi2". Defaults to "F". | `'F'`     |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type      | Description                                      |
|--------|-----------|--------------------------------------------------|
|        | pd.Series | A pd.Series with the Wald statistic and p-value. |

#### Examples {.doc-section .doc-section-examples}

```{python}
import numpy as np
import pandas as pd
import pyfixest as pf

data = pf.get_data()
fit = pf.feols("Y ~ X1 + X2| f1", data, vcov={"CRV1": "f1"}, ssc=pf.ssc(k_adj=False))

R = np.array([[1,-1]] )
q = np.array([0.0])

# Wald test
fit.wald_test(R=R, q=q, distribution = "chi2")
f_stat = fit._f_statistic
p_stat = fit._p_value

print(f"Python f_stat: {f_stat}")
print(f"Python p_stat: {p_stat}")
```

### wildboottest { #pyfixest.estimation.models.feols_.Feols.wildboottest }

```python
estimation.models.feols_.Feols.wildboottest(
    reps,
    cluster=None,
    param=None,
    weights_type='rademacher',
    impose_null=True,
    bootstrap_type='11',
    seed=None,
    k_adj=True,
    G_adj=True,
    parallel=False,
    return_bootstrapped_t_stats=False,
)
```

Run a wild cluster bootstrap based on an object of type "Feols".

#### Parameters {.doc-section .doc-section-parameters}

| Name                        | Type               | Description                                                                                                                                                                                                               | Default        |
|-----------------------------|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|
| reps                        | int                | The number of bootstrap iterations to run.                                                                                                                                                                                | _required_     |
| cluster                     | Union\[str, None\] | The variable used for clustering. Defaults to None. If None, then uses the variable specified in the model's `clustervar` attribute. If no `_clustervar` attribute is found, runs a heteroskedasticity- robust bootstrap. | `None`         |
| param                       | Union\[str, None\] | A string of length one, containing the test parameter of interest. Defaults to None.                                                                                                                                      | `None`         |
| weights_type                | str                | The type of bootstrap weights. Options are 'rademacher', 'mammen', 'webb', or 'normal'. Defaults to 'rademacher'.                                                                                                         | `'rademacher'` |
| impose_null                 | bool               | Indicates whether to impose the null hypothesis on the bootstrap DGP. Defaults to True.                                                                                                                                   | `True`         |
| bootstrap_type              | str                | A string of length one to choose the bootstrap type. Options are '11', '31', '13', or '33'. Defaults to '11'.                                                                                                             | `'11'`         |
| seed                        | Union\[int, None\] | An option to provide a random seed. Defaults to None.                                                                                                                                                                     | `None`         |
| k_adj                       | bool               | Indicates whether to apply a small sample adjustment for the number of observations and covariates. Defaults to True.                                                                                                     | `True`         |
| G_adj                       | bool               | Indicates whether to apply a small sample adjustment for the number of clusters. Defaults to True.                                                                                                                        | `True`         |
| parallel                    | bool               | Indicates whether to run the bootstrap in parallel. Defaults to False.                                                                                                                                                    | `False`        |
| seed                        | Union\[str, None\] | An option to provide a random seed. Defaults to None.                                                                                                                                                                     | `None`         |
| return_bootstrapped_t_stats | bool, optional:    | If True, the method returns a tuple of the regular output and the bootstrapped t-stats. Defaults to False.                                                                                                                | `False`        |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type         | Description                                                                                                                                                                                                                                                                                                                                 |
|--------|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | pd.DataFrame | A DataFrame with the original, non-bootstrapped t-statistic and bootstrapped p-value, along with the bootstrap type, inference type (HC vs CRV), and whether the null hypothesis was imposed on the bootstrap DGP. If `return_bootstrapped_t_stats` is True, the method returns a tuple of the regular output and the bootstrapped t-stats. |

#### Examples {.doc-section .doc-section-examples}

```{python}
#| echo: true
#| results: asis
#| include: true

import re
import pyfixest as pf

data = pf.get_data()
fit = pf.feols("Y ~ X1 + X2 | f1", data)

fit.wildboottest(
    param = "X1",
    reps=1000,
    seed = 822
)

fit.wildboottest(
    param = "X1",
    reps=1000,
    seed = 822,
    bootstrap_type = "31"
)

```

### wls_transform { #pyfixest.estimation.models.feols_.Feols.wls_transform }

```python
estimation.models.feols_.Feols.wls_transform()
```

Transform model matrices for WLS Estimation.

---

# reference/estimation.models.feols_compressed_.FeolsCompressed.html

# estimation.models.feols_compressed_.FeolsCompressed { #pyfixest.estimation.models.feols_compressed_.FeolsCompressed }

```python
estimation.models.feols_compressed_.FeolsCompressed(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    solver='np.linalg.solve',
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    context=0,
    sample_split_var=None,
    sample_split_value=None,
    reps=None,
    seed=None,
)
```

Non-user-facing class for compressed regression with fixed effects.

See the paper "You only compress once" by Wong et al (https://arxiv.org/abs/2102.11297) for
details on regression compression.

## Parameters {.doc-section .doc-section-parameters}

| Name                 | Type                            | Description                                                                                                                                                                                                                                                          | Default             |
|----------------------|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|
| FixestFormula        | FixestFormula                   | The formula object.                                                                                                                                                                                                                                                  | _required_          |
| data                 | pd.DataFrame                    | The data.                                                                                                                                                                                                                                                            | _required_          |
| ssc_dict             | dict\[str, Union\[str, bool\]\] | The ssc dictionary.                                                                                                                                                                                                                                                  | _required_          |
| drop_singletons      | bool                            | Whether to drop columns with singleton fixed effects.                                                                                                                                                                                                                | _required_          |
| drop_intercept       | bool                            | Whether to include an intercept.                                                                                                                                                                                                                                     | _required_          |
| weights              | Optional\[str\]                 | The column name of the weights. None if no weights are used. For this method, weights needs to be None.                                                                                                                                                              | _required_          |
| weights_type         | Optional\[str\]                 | The type of weights. For this method, weights_type needs to be 'fweights'.                                                                                                                                                                                           | _required_          |
| collin_tol           | float                           | The tolerance level for collinearity.                                                                                                                                                                                                                                | _required_          |
| fixef_tol            | float                           | The tolerance level for the fixed effects.                                                                                                                                                                                                                           | _required_          |
| fixef_maxiter        | int                             | The maximum iterations for the demeaning algorithm.                                                                                                                                                                                                                  | _required_          |
| lookup_demeaned_data | dict\[str, pd.DataFrame\]       | The lookup table for demeaned data.                                                                                                                                                                                                                                  | _required_          |
| solver               | SolverOptions                   | The solver to use.                                                                                                                                                                                                                                                   | `'np.linalg.solve'` |
| store_data           | bool                            | Whether to store the data.                                                                                                                                                                                                                                           | `True`              |
| copy_data            | bool                            | Whether to copy the data.                                                                                                                                                                                                                                            | `True`              |
| lean                 | bool                            | Whether to keep memory-heavy objects as attributes or not.                                                                                                                                                                                                           | `False`             |
| context              | int or Mapping\[str, Any\]      | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment. | `0`                 |
| reps                 | int                             | The number of bootstrap repetitions. Default is 100. Only used for CRV1 inference, where a wild cluster bootstrap is used.                                                                                                                                           | `None`              |
| seed                 | Optional\[int\]                 | The seed for the random number generator. Only relevant for CRV1 inference, where a wild cluster bootstrap is used.                                                                                                                                                  | `None`              |

## Methods

| Name | Description |
| --- | --- |
| [demean](#pyfixest.estimation.models.feols_compressed_.FeolsCompressed.demean) | Compression 'handles demeaning' via Mundlak transform. |
| [predict](#pyfixest.estimation.models.feols_compressed_.FeolsCompressed.predict) | Compute predicted values. |
| [prepare_model_matrix](#pyfixest.estimation.models.feols_compressed_.FeolsCompressed.prepare_model_matrix) | Prepare model inputs for estimation. |
| [vcov](#pyfixest.estimation.models.feols_compressed_.FeolsCompressed.vcov) | Compute the variance-covariance matrix for the compressed regression. |

### demean { #pyfixest.estimation.models.feols_compressed_.FeolsCompressed.demean }

```python
estimation.models.feols_compressed_.FeolsCompressed.demean()
```

Compression 'handles demeaning' via Mundlak transform.

### predict { #pyfixest.estimation.models.feols_compressed_.FeolsCompressed.predict }

```python
estimation.models.feols_compressed_.FeolsCompressed.predict(
    newdata=None,
    atol=1e-06,
    btol=1e-06,
    type='link',
    se_fit=False,
    interval=None,
    alpha=0.05,
)
```

Compute predicted values.

#### Parameters {.doc-section .doc-section-parameters}

| Name     | Type                               | Description                                                                                                                                           | Default   |
|----------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| newdata  | Optional\[DataFrameType\]          | The new data. If None, makes a prediction based on the uncompressed data set.                                                                         | `None`    |
| atol     | float                              | The absolute tolerance.                                                                                                                               | `1e-06`   |
| btol     | float                              | The relative tolerance.                                                                                                                               | `1e-06`   |
| type     | str                                | The type of prediction.                                                                                                                               | `'link'`  |
| se_fit   | Optional\[bool\]                   | If True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False. | `False`   |
| interval | Optional\[PredictionErrorOptions\] | The type of interval to compute. Can be either 'prediction' or None.                                                                                  | `None`    |
| alpha    | float                              | The alpha level for the confidence interval. Defaults to 0.05. Only used if interval = "prediction" is not None.                                      | `0.05`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                              | Description                                                                                                                                                                                                                        |
|--------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | Union\[np.ndarray, pd.DataFrame\] | Returns a pd.Dataframe with columns "fit", "se_fit" and CIs if argument "interval=prediction". Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument "se_fit=True". |

### prepare_model_matrix { #pyfixest.estimation.models.feols_compressed_.FeolsCompressed.prepare_model_matrix }

```python
estimation.models.feols_compressed_.FeolsCompressed.prepare_model_matrix()
```

Prepare model inputs for estimation.

### vcov { #pyfixest.estimation.models.feols_compressed_.FeolsCompressed.vcov }

```python
estimation.models.feols_compressed_.FeolsCompressed.vcov(
    vcov,
    vcov_kwargs=None,
    data=None,
)
```

Compute the variance-covariance matrix for the compressed regression.

---

# reference/estimation.models.fepois_.Fepois.html

# estimation.models.fepois_.Fepois { #pyfixest.estimation.models.fepois_.Fepois }

```python
estimation.models.fepois_.Fepois(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    tol,
    maxiter,
    solver='np.linalg.solve',
    demeaner_backend='numba',
    context=0,
    store_data=True,
    copy_data=True,
    lean=False,
    sample_split_var=None,
    sample_split_value=None,
    separation_check=None,
)
```

Estimate a Poisson regression model.

Non user-facing class to estimate a Poisson regression model via Iterated
Weighted Least Squares (IWLS).

Inherits from the Feols class. Users should not directly instantiate this class,
but rather use the [fepois()](/reference/estimation.api.fepois.fepois.qmd) function.
Note that no demeaning is performed in this class: demeaning is performed in the
FixestMulti class (to allow for caching of demeaned variables for multiple estimation).

The method implements the algorithm from Stata's `ppmlhdfe` module.

## Attributes {.doc-section .doc-section-attributes}

| Name             | Type                             | Description                                                                                                                                                                                                                                                          |
|------------------|----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| _Y               | np.ndarray                       | The demeaned dependent variable, a two-dimensional numpy array.                                                                                                                                                                                                      |
| _X               | np.ndarray                       | The demeaned independent variables, a two-dimensional numpy array.                                                                                                                                                                                                   |
| _fe              | np.ndarray                       | Fixed effects, a two-dimensional numpy array or None.                                                                                                                                                                                                                |
| weights          | np.ndarray                       | Weights, a one-dimensional numpy array or None.                                                                                                                                                                                                                      |
| coefnames        | list\[str\]                      | Names of the coefficients in the design matrix X.                                                                                                                                                                                                                    |
| drop_singletons  | bool                             | Whether to drop singleton fixed effects.                                                                                                                                                                                                                             |
| collin_tol       | float                            | Tolerance level for the detection of collinearity.                                                                                                                                                                                                                   |
| maxiter          | Optional\[int\], default=25      | Maximum number of iterations for the IRLS algorithm.                                                                                                                                                                                                                 |
| tol              | Optional\[float\], default=1e-08 | Tolerance level for the convergence of the IRLS algorithm.                                                                                                                                                                                                           |
| solver           | str, optional.                   | The solver to use for the regression. Can be "np.linalg.lstsq", "np.linalg.solve", "scipy.linalg.solve", "scipy.sparse.linalg.lsqr" and "jax". Defaults to "scipy.linalg.solve".                                                                                     |
| demeaner_backend | DemeanerBackendOptions.          | The backend used for demeaning.                                                                                                                                                                                                                                      |
| fixef_tol        | float, default = 1e-06.          | Tolerance level for the convergence of the demeaning algorithm.                                                                                                                                                                                                      |
| context          | int or Mapping\[str, Any\]       | A dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment. |
| weights_name     | Optional\[str\]                  | Name of the weights variable.                                                                                                                                                                                                                                        |
| weights_type     | Optional\[str\]                  | Type of weights variable.                                                                                                                                                                                                                                            |
| _data            | pd.DataFrame                     | The data frame used in the estimation. None if arguments `lean = True` or `store_data = False`.                                                                                                                                                                      |

## Methods

| Name | Description |
| --- | --- |
| [get_fit](#pyfixest.estimation.models.fepois_.Fepois.get_fit) | Fit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS). |
| [predict](#pyfixest.estimation.models.fepois_.Fepois.predict) | Return predicted values from regression model. |
| [prepare_model_matrix](#pyfixest.estimation.models.fepois_.Fepois.prepare_model_matrix) | Prepare model inputs for estimation. |
| [resid](#pyfixest.estimation.models.fepois_.Fepois.resid) | Return residuals from regression model. |
| [to_array](#pyfixest.estimation.models.fepois_.Fepois.to_array) | Turn estimation DataFrames to np arrays. |

### get_fit { #pyfixest.estimation.models.fepois_.Fepois.get_fit }

```python
estimation.models.fepois_.Fepois.get_fit()
```

Fit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS).

#### Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description   |
|--------|--------|---------------|
|        | None   |               |

#### Attributes {.doc-section .doc-section-attributes}

| Name     | Type       | Description                                                                     |
|----------|------------|---------------------------------------------------------------------------------|
| beta_hat | np.ndarray | Estimated coefficients.                                                         |
| Y_hat    | np.ndarray | Estimated dependent variable.                                                   |
| u_hat    | np.ndarray | Estimated residuals.                                                            |
| weights  | np.ndarray | Weights (from the last iteration of the IRLS algorithm).                        |
| X        | np.ndarray | Demeaned independent variables (from the last iteration of the IRLS algorithm). |
| Z        | np.ndarray | Demeaned independent variables (from the last iteration of the IRLS algorithm). |
| Y        | np.ndarray | Demeaned dependent variable (from the last iteration of the IRLS algorithm).    |

### predict { #pyfixest.estimation.models.fepois_.Fepois.predict }

```python
estimation.models.fepois_.Fepois.predict(
    newdata=None,
    atol=1e-06,
    btol=1e-06,
    type='link',
    se_fit=False,
    interval=None,
    alpha=0.05,
)
```

Return predicted values from regression model.

Return a flat np.array with predicted values of the regression model.
If new fixed effect levels are introduced in `newdata`, predicted values
for such observations
will be set to NaN.

#### Parameters {.doc-section .doc-section-parameters}

| Name     | Type                               | Description                                                                                                                                                                                                                                                                                                        | Default   |
|----------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|
| newdata  | Union\[None, pd.DataFrame\]        | A pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.                                                                                                                                                                                          | `None`    |
| atol     | Float                              | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                     | `1e-6`    |
| btol     | Float                              | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/     scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                             | `1e-6`    |
| type     | str                                | The type of prediction to be computed. Can be either "response" (default) or "link". If type="response", the output is at the level of the response variable, i.e., it is the expected predictor E(Y\|X). If "link", the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta. | `'link'`  |
| atol     | Float                              | Stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                          | `1e-6`    |
| btol     | Float                              | Another stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html                                                                                                                                                                  | `1e-6`    |
| se_fit   | Optional\[bool\]                   | If True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.                                                                                                                                                              | `False`   |
| interval | Optional\[PredictionErrorOptions\] | The type of interval to compute. Can be either 'prediction' or None.                                                                                                                                                                                                                                               | `None`    |
| alpha    | float                              | The alpha level for the confidence interval. Defaults to 0.05. Only used if interval = "prediction" is not None.                                                                                                                                                                                                   | `0.05`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                              | Description                                                                                                                                                                                                                        |
|--------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|        | Union\[np.ndarray, pd.DataFrame\] | Returns a pd.Dataframe with columns "fit", "se_fit" and CIs if argument "interval=prediction". Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument "se_fit=True". |

### prepare_model_matrix { #pyfixest.estimation.models.fepois_.Fepois.prepare_model_matrix }

```python
estimation.models.fepois_.Fepois.prepare_model_matrix()
```

Prepare model inputs for estimation.

### resid { #pyfixest.estimation.models.fepois_.Fepois.resid }

```python
estimation.models.fepois_.Fepois.resid(type='response')
```

Return residuals from regression model.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type   | Description                                                                            | Default      |
|--------|--------|----------------------------------------------------------------------------------------|--------------|
| type   | str    | The type of residuals to be computed. Can be either "response" (default) or "working". | `'response'` |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type       | Description                                              |
|--------|------------|----------------------------------------------------------|
|        | np.ndarray | A flat array with the residuals of the regression model. |

### to_array { #pyfixest.estimation.models.fepois_.Fepois.to_array }

```python
estimation.models.fepois_.Fepois.to_array()
```

Turn estimation DataFrames to np arrays.

---

# reference/estimation.models.feprobit_.Feprobit.html

# estimation.models.feprobit_.Feprobit { #pyfixest.estimation.models.feprobit_.Feprobit }

```python
estimation.models.feprobit_.Feprobit(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    tol,
    maxiter,
    solver,
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    sample_split_var=None,
    sample_split_value=None,
    separation_check=None,
    context=0,
    accelerate=True,
)
```

Class for the estimation of a fixed-effects probit model.

---

# reference/estimation.post_estimation.multcomp.bonferroni.html

# estimation.post_estimation.multcomp.bonferroni { #pyfixest.estimation.post_estimation.multcomp.bonferroni }

```python
estimation.post_estimation.multcomp.bonferroni(models, param)
```

Compute Bonferroni adjusted p-values for multiple hypothesis testing.

For each model, it is assumed that tests to adjust are of the form
"param = 0".

## Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                     | Description                                              | Default    |
|--------|--------------------------------------------------------------------------|----------------------------------------------------------|------------|
| models | A supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of | Feols, Fepois & Feiv models.                             | _required_ |
| param  | str                                                                      | The parameter for which the p-values should be adjusted. | _required_ |

## Returns {.doc-section .doc-section-returns}

| Name   | Type         | Description                                                                               |
|--------|--------------|-------------------------------------------------------------------------------------------|
|        | pd.DataFrame | A DataFrame containing estimation statistics, including the Bonferroni adjusted p-values. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf
from pyfixest.utils import get_data

data = get_data().dropna()
fit1 = pf.feols("Y ~ X1", data=data)
fit2 = pf.feols("Y ~ X1 + X2", data=data)
bonf_df = pf.bonferroni([fit1, fit2], param="X1")
bonf_df
```

---

# reference/estimation.post_estimation.multcomp.rwolf.html

# estimation.post_estimation.multcomp.rwolf { #pyfixest.estimation.post_estimation.multcomp.rwolf }

```python
estimation.post_estimation.multcomp.rwolf(
    models,
    param,
    reps,
    seed,
    sampling_method='wild-bootstrap',
)
```

Compute Romano-Wolf adjusted p-values for multiple hypothesis testing.

For each model, it is assumed that tests to adjust are of the form
"param = 0". This function uses the `wildboottest()` method for running the
bootstrap, hence models of type `Feiv` or `Fepois` are not supported.

## Parameters {.doc-section .doc-section-parameters}

| Name            | Type                         | Description                                                                                                                               | Default            |
|-----------------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------|
| models          | list\[Feols\] or FixestMulti | A list of models for which the p-values should be computed, or a FixestMulti object. Models of type `Feiv` or `Fepois` are not supported. | _required_         |
| param           | str                          | The parameter for which the p-values should be computed.                                                                                  | _required_         |
| reps            | int                          | The number of bootstrap replications.                                                                                                     | _required_         |
| seed            | int                          | The seed for the random number generator.                                                                                                 | _required_         |
| sampling_method | str                          | Sampling method for computing resampled statistics. Users can choose either bootstrap('wild-bootstrap') or randomization inference('ri')  | `'wild-bootstrap'` |

## Returns {.doc-section .doc-section-returns}

| Name   | Type         | Description                                                                                |
|--------|--------------|--------------------------------------------------------------------------------------------|
|        | pd.DataFrame | A DataFrame containing estimation statistics, including the Romano-Wolf adjusted p-values. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf
from pyfixest.utils import get_data

data = get_data().dropna()
fit = pf.feols("Y ~ Y2 + X1 + X2", data=data)
pf.rwolf(fit, "X1", reps=9999, seed=123)

fit1 = pf.feols("Y ~ X1", data=data)
fit2 = pf.feols("Y ~ X1 + X2", data=data)
rwolf_df = pf.rwolf([fit1, fit2], "X1", reps=9999, seed=123)

# use randomization inference - dontrun as too slow
# rwolf_df = pf.rwolf([fit1, fit2], "X1", reps=9999, seed=123, sampling_method = "ri")

rwolf_df
```

---

# reference/estimation.quantreg.quantreg_.Quantreg.html

# estimation.quantreg.quantreg_.Quantreg { #pyfixest.estimation.quantreg.quantreg_.Quantreg }

```python
estimation.quantreg.quantreg_.Quantreg(
    FixestFormula,
    data,
    ssc_dict,
    drop_singletons,
    drop_intercept,
    weights,
    weights_type,
    collin_tol,
    fixef_tol,
    fixef_maxiter,
    lookup_demeaned_data,
    solver='np.linalg.solve',
    demeaner_backend='numba',
    store_data=True,
    copy_data=True,
    lean=False,
    context=0,
    sample_split_var=None,
    sample_split_value=None,
    quantile=0.5,
    method='fn',
    quantile_tol=1e-06,
    quantile_maxiter=None,
    seed=None,
)
```

Quantile regression model.

## Attributes

| Name | Description |
| --- | --- |
| [objective_value](#pyfixest.estimation.quantreg.quantreg_.Quantreg.objective_value) | Compute the total loss of the quantile regression model. |

## Methods

| Name | Description |
| --- | --- |
| [fit_qreg_fn](#pyfixest.estimation.quantreg.quantreg_.Quantreg.fit_qreg_fn) | Fit a quantile regression model using the Frisch-Newton Interior Point Solver. |
| [fit_qreg_pfn](#pyfixest.estimation.quantreg.quantreg_.Quantreg.fit_qreg_pfn) | Fit a quantile regression model using the Frisch-Newton Interior Point Solver with pre-processing. |
| [get_fit](#pyfixest.estimation.quantreg.quantreg_.Quantreg.get_fit) | Fit a quantile regression model using the interior point method. |
| [get_performance](#pyfixest.estimation.quantreg.quantreg_.Quantreg.get_performance) | Compute performance metrics for the quantile regression model. |
| [prepare_model_matrix](#pyfixest.estimation.quantreg.quantreg_.Quantreg.prepare_model_matrix) | Prepare model inputs for estimation. |
| [to_array](#pyfixest.estimation.quantreg.quantreg_.Quantreg.to_array) | Turn estimation DataFrames to np arrays. |

### fit_qreg_fn { #pyfixest.estimation.quantreg.quantreg_.Quantreg.fit_qreg_fn }

```python
estimation.quantreg.quantreg_.Quantreg.fit_qreg_fn(
    X,
    Y,
    q,
    tol=None,
    maxiter=None,
    beta_init=None,
)
```

Fit a quantile regression model using the Frisch-Newton Interior Point Solver.

### fit_qreg_pfn { #pyfixest.estimation.quantreg.quantreg_.Quantreg.fit_qreg_pfn }

```python
estimation.quantreg.quantreg_.Quantreg.fit_qreg_pfn(
    X,
    Y,
    q,
    m=None,
    tol=None,
    maxiter=None,
    beta_init=None,
    rng=None,
    eta=None,
)
```

Fit a quantile regression model using the Frisch-Newton Interior Point Solver with pre-processing.

### get_fit { #pyfixest.estimation.quantreg.quantreg_.Quantreg.get_fit }

```python
estimation.quantreg.quantreg_.Quantreg.get_fit()
```

Fit a quantile regression model using the interior point method.

### get_performance { #pyfixest.estimation.quantreg.quantreg_.Quantreg.get_performance }

```python
estimation.quantreg.quantreg_.Quantreg.get_performance()
```

Compute performance metrics for the quantile regression model.

### prepare_model_matrix { #pyfixest.estimation.quantreg.quantreg_.Quantreg.prepare_model_matrix }

```python
estimation.quantreg.quantreg_.Quantreg.prepare_model_matrix()
```

Prepare model inputs for estimation.

### to_array { #pyfixest.estimation.quantreg.quantreg_.Quantreg.to_array }

```python
estimation.quantreg.quantreg_.Quantreg.to_array()
```

Turn estimation DataFrames to np arrays.

---

# reference/index.html

# PyFixest Function Reference {.doc .doc-index}

## Estimation Functions

User facing estimation functions


| | |
| --- | --- |
| [estimation.api.feols.feols](estimation.api.feols.feols.qmd#pyfixest.estimation.api.feols.feols) | Estimate a linear regression models with fixed effects using fixest formula syntax. |
| [estimation.api.fepois.fepois](estimation.api.fepois.fepois.qmd#pyfixest.estimation.api.fepois.fepois) | Estimate Poisson regression model with fixed effects using the `ppmlhdfe` algorithm. |
| [estimation.api.feglm.feglm](estimation.api.feglm.feglm.qmd#pyfixest.estimation.api.feglm.feglm) | Estimate GLM regression models with fixed effects. |
| [estimation.api.quantreg.quantreg](estimation.api.quantreg.quantreg.qmd#pyfixest.estimation.api.quantreg.quantreg) | Fit a quantile regression model using the interior point algorithm from Portnoy and Koenker (1997). |
| [did.estimation.did2s](did.estimation.did2s.qmd#pyfixest.did.estimation.did2s) | Estimate a Difference-in-Differences model using Gardner's two-step DID2S estimator. |
| [did.estimation.lpdid](did.estimation.lpdid.qmd#pyfixest.did.estimation.lpdid) | Local projections approach to estimation. |
| [did.estimation.event_study](did.estimation.event_study.qmd#pyfixest.did.estimation.event_study) | Estimate Event Study Model. |
| [estimation.post_estimation.multcomp.bonferroni](estimation.post_estimation.multcomp.bonferroni.qmd#pyfixest.estimation.post_estimation.multcomp.bonferroni) | Compute Bonferroni adjusted p-values for multiple hypothesis testing. |
| [estimation.post_estimation.multcomp.rwolf](estimation.post_estimation.multcomp.rwolf.qmd#pyfixest.estimation.post_estimation.multcomp.rwolf) | Compute Romano-Wolf adjusted p-values for multiple hypothesis testing. |

## Estimation Classes

Details on Methods and Attributes


| | |
| --- | --- |
| [estimation.models.feols_.Feols](estimation.models.feols_.Feols.qmd#pyfixest.estimation.models.feols_.Feols) | Non user-facing class to estimate a linear regression via OLS. |
| [estimation.models.fepois_.Fepois](estimation.models.fepois_.Fepois.qmd#pyfixest.estimation.models.fepois_.Fepois) | Estimate a Poisson regression model. |
| [estimation.models.feiv_.Feiv](estimation.models.feiv_.Feiv.qmd#pyfixest.estimation.models.feiv_.Feiv) | Non user-facing class to estimate an IV model using a 2SLS estimator. |
| [estimation.models.feglm_.Feglm](estimation.models.feglm_.Feglm.qmd#pyfixest.estimation.models.feglm_.Feglm) | Abstract base class for the estimation of a fixed-effects GLM model. |
| [estimation.models.felogit_.Felogit](estimation.models.felogit_.Felogit.qmd#pyfixest.estimation.models.felogit_.Felogit) | Class for the estimation of a fixed-effects logit model. |
| [estimation.models.feprobit_.Feprobit](estimation.models.feprobit_.Feprobit.qmd#pyfixest.estimation.models.feprobit_.Feprobit) | Class for the estimation of a fixed-effects probit model. |
| [estimation.models.fegaussian_.Fegaussian](estimation.models.fegaussian_.Fegaussian.qmd#pyfixest.estimation.models.fegaussian_.Fegaussian) | Class for the estimation of a fixed-effects GLM with normal errors. |
| [estimation.models.feols_compressed_.FeolsCompressed](estimation.models.feols_compressed_.FeolsCompressed.qmd#pyfixest.estimation.models.feols_compressed_.FeolsCompressed) | Non-user-facing class for compressed regression with fixed effects. |
| [estimation.FixestMulti_.FixestMulti](estimation.FixestMulti_.FixestMulti.qmd#pyfixest.estimation.FixestMulti_.FixestMulti) | A class to estimate multiple regression models with fixed effects. |
| [estimation.quantreg.quantreg_.Quantreg](estimation.quantreg.quantreg_.Quantreg.qmd#pyfixest.estimation.quantreg.quantreg_.Quantreg) | Quantile regression model. |

## Summarize and Visualize

Post-Processing of Estimation Results


| | |
| --- | --- |
| [did.visualize.panelview](did.visualize.panelview.qmd#pyfixest.did.visualize.panelview) | Generate a panel view of the treatment variable over time for each unit. |
| [report.summary](report.summary.qmd#pyfixest.report.summary) | Print a summary of estimation results for each estimated model. |
| [report.etable](report.etable.qmd#pyfixest.report.etable) | Generate a table summarizing the results of multiple regression models. |
| [report.dtable](report.dtable.qmd#pyfixest.report.dtable) | Generate descriptive statistics tables and create a booktab style table in |
| [report.coefplot](report.coefplot.qmd#pyfixest.report.coefplot) | Plot model coefficients with confidence intervals. |
| [report.iplot](report.iplot.qmd#pyfixest.report.iplot) | Plot model coefficients for variables interacted via "i()" syntax, with |
| [did.visualize.panelview](did.visualize.panelview.qmd#pyfixest.did.visualize.panelview) | Generate a panel view of the treatment variable over time for each unit. |

## Formula Parsing & Model Matrix

Internal APIs for formula parsing and model matrix construction


| | |
| --- | --- |
| [estimation.formula.parse.Formula](estimation.formula.parse.Formula.qmd#pyfixest.estimation.formula.parse.Formula) | A formulaic-compliant formula. |
| [estimation.formula.model_matrix.ModelMatrix](estimation.formula.model_matrix.ModelMatrix.qmd#pyfixest.estimation.formula.model_matrix.ModelMatrix) | A wrapper around formulaic.ModelMatrix for the specification of PyFixest models. |
| [estimation.formula.factor_interaction.factor_interaction](estimation.formula.factor_interaction.factor_interaction.qmd#pyfixest.estimation.formula.factor_interaction.factor_interaction) | Fixest-style i() operator for categorical encoding with interactions. |

## Misc / Utilities

PyFixest internals and utilities


| | |
| --- | --- |
| [estimation.internals.demean_.demean](estimation.internals.demean_.demean.qmd#pyfixest.estimation.internals.demean_.demean) | Demean an array. |
| [estimation.internals.detect_singletons_.detect_singletons](estimation.internals.detect_singletons_.detect_singletons.qmd#pyfixest.estimation.internals.detect_singletons_.detect_singletons) | Detect singleton fixed effects in a dataset. |
| [estimation.deprecated.model_matrix_fixest_.model_matrix_fixest](estimation.deprecated.model_matrix_fixest_.model_matrix_fixest.qmd#pyfixest.estimation.deprecated.model_matrix_fixest_.model_matrix_fixest) | Create model matrices for fixed effects estimation. |

---

# reference/report.coefplot.html

# report.coefplot { #pyfixest.report.coefplot }

```python
report.coefplot(
    models,
    alpha=0.05,
    figsize=None,
    yintercept=0,
    xintercept=None,
    rotate_xticks=0,
    title=None,
    coord_flip=True,
    keep=None,
    drop=None,
    exact_match=False,
    plot_backend='lets_plot' if _HAS_LETS_PLOT else 'matplotlib',
    labels=None,
    joint=None,
    seed=None,
    ax=None,
    rename_models=None,
)
```

Plot model coefficients with confidence intervals.

## Parameters {.doc-section .doc-section-parameters}

| Name          | Type                                                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Default                                           |
|---------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|
| models        | A supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of | Feols, Fepois & Feiv models.                                                                                                                                                                                                                                                                                                                                                                                                                                                | _required_                                        |
| figsize       | tuple or None                                                            | The size of the figure. If None, the default size is used.                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`                                            |
| alpha         | float                                                                    | The significance level for the confidence intervals.                                                                                                                                                                                                                                                                                                                                                                                                                        | `0.05`                                            |
| yintercept    | float or None                                                            | The value at which to draw a horizontal line on the plot. Default is 0.                                                                                                                                                                                                                                                                                                                                                                                                     | `0`                                               |
| xintercept    | float or None                                                            | The value at which to draw a vertical line on the plot. Default is None.                                                                                                                                                                                                                                                                                                                                                                                                    | `None`                                            |
| rotate_xticks | float                                                                    | The angle in degrees to rotate the xticks labels. Default is 0 (no rotation).                                                                                                                                                                                                                                                                                                                                                                                               | `0`                                               |
| title         | str                                                                      | The title of the plot.                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`                                            |
| coord_flip    | bool                                                                     | Whether to flip the coordinates of the plot. Default is True.                                                                                                                                                                                                                                                                                                                                                                                                               | `True`                                            |
| keep          | Optional\[Union\[list, str\]\]                                           | The pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients.     "age",            # would keep all coefficients containing age     r"^tr",           # would keep all coefficients starting with tr     r"\\d$",          # would keep all coefficients ending with number Output will be in the order of the patterns. | `None`                                            |
| drop          | Optional\[Union\[list, str\]\]                                           | The pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for `keep`. Default is keeping all coefficients. Parameter `keep` and `drop` can be used simultaneously.                                                                                                                                                                                                                              | `None`                                            |
| exact_match   | bool                                                                     | Whether to use exact match for `keep` and `drop`. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.                                                                                                                                                                                                                                                                                              | `False`                                           |
| plot_backend  | str                                                                      | The plotting backend to use. Options are "lets_plot" (default if installed) and "matplotlib". If "lets_plot" is specified but not installed, an ImportError will be raised with instructions to install it or use "matplotlib" instead.                                                                                                                                                                                                                                     | `'lets_plot' if _HAS_LETS_PLOT else 'matplotlib'` |
| rename_models | dict                                                                     | A dictionary to rename the models. The keys are the original model names and the values the new names.                                                                                                                                                                                                                                                                                                                                                                      | `None`                                            |
| labels        | Optional\[dict\]                                                         | A dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the `_coefnames` attribute of the model. The values in the dictionary are the new  names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The renaming is applied after the selection of the coefficients via `keep` and `drop`.         | `None`                                            |
| joint         | Optional\[Union\[str, bool\]\]                                           | Whether to plot simultaneous confidence bands for the coefficients. If True, simultaneous confidence bands are plotted. If False, "standard" confidence intervals are plotted. If "both", both are plotted in one figure. Default is None, which returns the standard confidence intervals. Note that this option is not available for objects of type `FixestMulti`, i.e. multiple estimation.                                                                             | `None`                                            |
| seed          | Optional\[int\]                                                          | The seed for the random number generator. Default is None. Only required / used when `joint` is True.                                                                                                                                                                                                                                                                                                                                                                       | `None`                                            |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                               |
|--------|--------|-------------------------------------------|
|        | object | A plot figure from the specified backend. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf

df = pf.get_data()
fit1 = pf.feols("Y ~ i(f1)", data = df)
fit2 = pf.feols("Y ~ i(f1) + X2", data = df)
fit3 = pf.feols("Y ~ i(f1) + X2 | f1", data = df)

pf.iplot([fit1, fit2, fit3])
pf.iplot(
    models = [fit1, fit2, fit3],
    rename_models = {
        fit1._model_name_plot: "Model 1",
        fit2._model_name_plot: "Model 2",
        fit3._model_name_plot: "Model 3"
    },
)
pf.iplot(
    models = [fit1, fit2, fit3],
    rename_models = {
        "Y~i(f1)": "Model 1",
        "Y~i(f1)+X2": "Model 2",
        "Y~i(f1)+X2|f1": "Model 3"
    },
)
pf.iplot([fit1], joint = "both")

```

---

# reference/report.dtable.html

# report.dtable { #pyfixest.report.dtable }

```python
report.dtable(
    df,
    vars,
    stats=None,
    bycol=None,
    byrow=None,
    type='gt',
    labels=None,
    stats_labels=None,
    digits=2,
    notes='',
    counts_row_below=False,
    hide_stats=False,
    **kwargs,
)
```

Generate descriptive statistics tables and create a booktab style table in
the desired format (gt or tex).

.. deprecated:: 0.41.0
    This function is deprecated and will be removed in a future version.
    Please use `maketables.DTable()` directly instead.
    See https://py-econometrics.github.io/maketables/ for documentation.

## Parameters {.doc-section .doc-section-parameters}

| Name             | Type         | Description                                                                                                                                                                                               | Default    |
|------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| df               | pd.DataFrame | DataFrame containing the table to be displayed.                                                                                                                                                           | _required_ |
| vars             | list         | List of variables to be included in the table.                                                                                                                                                            | _required_ |
| stats            | list         | List of statistics to be calculated. The default is None, that sets ['count','mean', 'std']. All pandas aggregation functions are supported.                                                              | `None`     |
| bycol            | list         | List of variables to be used to group the data by columns. The default is None.                                                                                                                           | `None`     |
| byrow            | str          | Variable to be used to group the data by rows. The default is None.                                                                                                                                       | `None`     |
| type             | str          | Type of table to be created. The default is 'gt'. Type can be 'gt' for great_tables, 'tex' for LaTeX or 'df' for dataframe.                                                                               | `'gt'`     |
| labels           | dict         | Dictionary containing the labels for the variables. The default is None.                                                                                                                                  | `None`     |
| stats_labels     | dict         | Dictionary containing the labels for the statistics. The default is None. The function uses a default labeling which will be replaced by the labels in the dictionary.                                    | `None`     |
| digits           | int          | Number of decimal places to round the statistics to. The default is 2.                                                                                                                                    | `2`        |
| notes            | str          | Table notes to be displayed at the bottom of the table.                                                                                                                                                   | `''`       |
| counts_row_below | bool         | Whether to display the number of observations at the bottom of the table. Will only be carried out when each var has the same number of obs and when byrow is None. The default is False                  | `False`    |
| hide_stats       | bool         | Whether to hide the names of the statistics in the table header. When stats are hidden and the user provides no notes string the labels of the stats are listed in the table notes. The default is False. | `False`    |
| kwargs           | dict         | Additional arguments to be passed to maketables.DTable.                                                                                                                                                   | `{}`       |

## Returns {.doc-section .doc-section-returns}

| Name   | Type                             | Description   |
|--------|----------------------------------|---------------|
|        | A table in the specified format. |               |

## Examples {.doc-section .doc-section-examples}

For more examples, take a look at the [regression tables and summary statistics vignette](https://pyfixest.org/table-layout.html).

```{python}
import pyfixest as pf

# load data
df = pf.get_data()
pf.dtable(df, vars = ["Y", "X1", "X2", "f1"])
```

---

# reference/report.etable.html

# report.etable { #pyfixest.report.etable }

```python
report.etable(
    models,
    type='gt',
    signif_code=None,
    coef_fmt='b \n (se)',
    custom_stats=None,
    custom_model_stats=None,
    keep=None,
    drop=None,
    exact_match=False,
    labels=None,
    cat_template=None,
    show_fe=True,
    felabels=None,
    fe_present='x',
    fe_absent='-',
    notes='',
    model_heads=None,
    head_order='dh',
    file_name=None,
    **kwargs,
)
```

Generate a table summarizing the results of multiple regression models.

This function uses the maketables package internally to create publication-ready
regression tables. It supports various output formats including HTML (via Great Tables),
markdown, and LaTeX.

## Parameters {.doc-section .doc-section-parameters}

| Name                          | Type                                                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Default       |
|-------------------------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|
| models                        | A supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of | Feols, Fepois & Feiv models. The models to be summarized in the table.                                                                                                                                                                                                                                                                                                                                                                                                      | _required_    |
| type                          | str                                                                      | Type of output. Either "df" for pandas DataFrame, "md" for markdown, "gt" for great_tables, or "tex" for LaTeX table. Default is "gt".                                                                                                                                                                                                                                                                                                                                      | `'gt'`        |
| signif_code                   | list                                                                     | Significance levels for the stars. Default is None, which sets [0.001, 0.01, 0.05]. If None, no stars are printed.                                                                                                                                                                                                                                                                                                                                                          | `None`        |
| coef_fmt                      | str                                                                      | The format of the coefficient (b), standard error (se), t-stats (t), and p-value (p). Default is `"b \n (se)"`. Spaces ` `, parentheses `()`, brackets `[]`, newlines `\n` are supported.                                                                                                                                                                                                                                                                                   | `'b \n (se)'` |
| custom_stats                  | Optional\[dict\]                                                         | A dictionary of custom statistics that can be used in the coef_fmt string to be displayed in the coefficuent cells analogously to "b", "se" etc. The keys are the names of the custom statistics, and the values are lists of lists, where each inner list contains the custom statistic values for all coefficients each model. Note that "b", "se", "t", or "p" are reserved and cannot be used as keys.                                                                  | `None`        |
| custom_model_stats            | Optional\[dict\]                                                         | A dictionary of custom model statistics or model information displayed in a new line in the bottom panel of the table. The keys are the names of the statistics (i.e. entry in the first column) and the values are a lists of the same length as the number of models. Default is None.                                                                                                                                                                                    | `None`        |
| keep                          | Optional\[Union\[list, str\]\]                                           | The pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients.     "age",            # would keep all coefficients containing age     r"^tr",           # would keep all coefficients starting with tr     r"\\d$",          # would keep all coefficients ending with number Output will be in the order of the patterns. | `None`        |
| drop                          | Optional\[Union\[list, str\]\]                                           | The pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for `keep`. Default is keeping all coefficients. Parameter `keep` and `drop` can be used simultaneously.                                                                                                                                                                                                                              | `None`        |
| exact_match                   | Optional\[bool\]                                                         | Whether to use exact match for `keep` and `drop`. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.                                                                                                                                                                                                                                                                                              | `False`       |
| labels                        | Optional\[dict\]                                                         | A dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the `_coefnames` attribute of the model. The values in the dictionary are the new names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The command is applied after the `keep` and `drop` commands.                                    | `None`        |
| cat_template                  | Optional\[str\]                                                          | Template to relabel categorical variables. None by default, which applies no relabeling. Other options include combinations of "{variable}" and "{value}", e.g. "{variable}::{value}" to mimic fixest encoding. But "{variable}--{value}" or "{variable}{value}" or just "{value}" are also possible.                                                                                                                                                                       | `None`        |
| show_fe                       | Optional\[bool\]                                                         | Whether to show the rows with fixed effects markers. Default is True.                                                                                                                                                                                                                                                                                                                                                                                                       | `True`        |
| felabels                      | Optional\[dict\]                                                         | A dictionary to relabel the fixed effects. Only needed if you want to relabel the FE lines with a different label than the one specied for the respective variable in the labels dictionary. The command is applied after the `keep` and `drop` commands.                                                                                                                                                                                                                   | `None`        |
| fe_present                    | str                                                                      | Symbol to use when a fixed effect is present in a model. Default is "x". Common alternatives include "Y", "YES", "✓", "✅", or any custom string.                                                                                                                                                                                                                                                                                                                           | `'x'`         |
| fe_absent                     | str                                                                      | Symbol to use when a fixed effect is absent from a model. Default is "-". Common alternatives include "N", "NO", "✗", "", or any custom string.                                                                                                                                                                                                                                                                                                                             | `'-'`         |
| digits                        |                                                                          | The number of digits to round to.                                                                                                                                                                                                                                                                                                                                                                                                                                           | _required_    |
| thousands_sep                 |                                                                          | The thousands separator. Default is False.                                                                                                                                                                                                                                                                                                                                                                                                                                  | _required_    |
| scientific_notation           |                                                                          | Whether to use scientific notation. Default is True.                                                                                                                                                                                                                                                                                                                                                                                                                        | _required_    |
| scientific_notation_threshold |                                                                          | The threshold for using scientific notation. Default is 10_000.                                                                                                                                                                                                                                                                                                                                                                                                             | _required_    |
| notes                         | str                                                                      | Custom table notes. Default shows the significance levels and the format of the coefficient cell.                                                                                                                                                                                                                                                                                                                                                                           | `''`          |
| model_heads                   | Optional\[list\]                                                         | Add custom headlines to models when output as df or latex. Length of list must correspond to number of models. Default is None.                                                                                                                                                                                                                                                                                                                                             | `None`        |
| head_order                    | Optional\[str\]                                                          | String to determine the display of the table header when output as df or latex. Allowed values are "dh", "hd", "d", "h", or "". When head_order is "dh", the dependent variable is displayed first, followed by the custom model_heads (provided the user has specified them). With "hd" it is the other way around. When head_order is "d", only the dependent variable and model numbers are displayed and with "" only the model numbers. Default is "dh".               | `'dh'`        |
| file_name                     | Optional\[str\]                                                          | The name/path of the file to save the LaTeX table to. Default is None.                                                                                                                                                                                                                                                                                                                                                                                                      | `None`        |

## Returns {.doc-section .doc-section-returns}

| Name   | Type             | Description                                                                                                                               |
|--------|------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
|        | pandas.DataFrame | A styled DataFrame with the coefficients and standard errors of the models. When output is "tex", the LaTeX code is returned as a string. |

## Examples {.doc-section .doc-section-examples}

For more examples, take a look at the [regression tables and summary statistics vignette](https://pyfixest.org/table-layout.html).

```{python}
import pyfixest as pf

# load data
df = pf.get_data()
fit1 = pf.feols("Y~X1 + X2 | f1", df)
fit2 = pf.feols("Y~X1 + X2 | f1 + f2", df)

pf.etable([fit1, fit2])
```

---

# reference/report.iplot.html

# report.iplot { #pyfixest.report.iplot }

```python
report.iplot(
    models,
    alpha=0.05,
    figsize=None,
    yintercept=None,
    xintercept=None,
    rotate_xticks=0,
    title=None,
    coord_flip=True,
    keep=None,
    drop=None,
    exact_match=False,
    plot_backend='lets_plot' if _HAS_LETS_PLOT else 'matplotlib',
    labels=None,
    cat_template=None,
    rename_models=None,
    ax=None,
    joint=None,
    seed=None,
)
```

Plot model coefficients for variables interacted via "i()" syntax, with
confidence intervals.

## Parameters {.doc-section .doc-section-parameters}

| Name          | Type                                                                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Default                                           |
|---------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|
| models        | A supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of | Feols, Fepois & Feiv models.                                                                                                                                                                                                                                                                                                                                                                                                                                                | _required_                                        |
| figsize       | tuple or None                                                            | The size of the figure. If None, the default size is used.                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`                                            |
| alpha         | float                                                                    | The significance level for the confidence intervals.                                                                                                                                                                                                                                                                                                                                                                                                                        | `0.05`                                            |
| yintercept    | int or None                                                              | The value at which to draw a horizontal line on the plot.                                                                                                                                                                                                                                                                                                                                                                                                                   | `None`                                            |
| xintercept    | int or None                                                              | The value at which to draw a vertical line on the plot.                                                                                                                                                                                                                                                                                                                                                                                                                     | `None`                                            |
| rotate_xticks | float                                                                    | The angle in degrees to rotate the xticks labels. Default is 0 (no rotation).                                                                                                                                                                                                                                                                                                                                                                                               | `0`                                               |
| title         | str                                                                      | The title of the plot.                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `None`                                            |
| coord_flip    | bool                                                                     | Whether to flip the coordinates of the plot. Default is True.                                                                                                                                                                                                                                                                                                                                                                                                               | `True`                                            |
| keep          | Optional\[Union\[list, str\]\]                                           | The pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients.     "age",            # would keep all coefficients containing age     r"^tr",           # would keep all coefficients starting with tr     r"\\d$",          # would keep all coefficients ending with number Output will be in the order of the patterns. | `None`                                            |
| drop          | Optional\[Union\[list, str\]\]                                           | The pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for `keep`. Default is keeping all coefficients. Parameter `keep` and `drop` can be used simultaneously.                                                                                                                                                                                                                              | `None`                                            |
| exact_match   | bool                                                                     | Whether to use exact match for `keep` and `drop`. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.                                                                                                                                                                                                                                                                                              | `False`                                           |
| plot_backend  | str                                                                      | The plotting backend to use. Options are "lets_plot" (default if installed) and "matplotlib". If "lets_plot" is specified but not installed, an ImportError will be raised with instructions to install it or use "matplotlib" instead.                                                                                                                                                                                                                                     | `'lets_plot' if _HAS_LETS_PLOT else 'matplotlib'` |
| rename_models | dict                                                                     | A dictionary to rename the models. The keys are the original model names and the values the new names.                                                                                                                                                                                                                                                                                                                                                                      | `None`                                            |
| labels        | Optional\[dict\]                                                         | A dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the `_coefnames` attribute of the model. The values in the dictionary are the new  names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The renaming is applied after the selection of the coefficients via `keep` and `drop`.         | `None`                                            |
| cat_template  | Optional\[str\]                                                          | Template to relabel categorical variables. None by default, which applies no relabeling. Other options include combinations of "{variable}" and "{value}", e.g. "{variable}::{value}" to mimic fixest encoding. But "{variable}--{value}" or "{variable}{value}" or just "{value}" are also possible.                                                                                                                                                                       | `None`                                            |
| joint         | Optional\[Union\[str, bool\]\]                                           | Whether to plot simultaneous confidence bands for the coefficients. If True, simultaneous confidence bands are plotted. If False, "standard" confidence intervals are plotted. If "both", both are plotted in one figure. Default is None, which returns the standard confidence intervals. Note that this option is not available for objects of type `FixestMulti`, i.e. multiple estimation.                                                                             | `None`                                            |
| seed          | Optional\[int\]                                                          | The seed for the random number generator. Default is None. Only required / used when `joint` is True.                                                                                                                                                                                                                                                                                                                                                                       | `None`                                            |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description                               |
|--------|--------|-------------------------------------------|
|        | object | A plot figure from the specified backend. |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf
from pyfixest.report.utils import rename_categoricals

df = pf.get_data()
fit1 = pf.feols("Y ~ i(f1)", data = df)
fit2 = pf.feols("Y ~ i(f1) + X2", data = df)
fit3 = pf.feols("Y ~ i(f1) + X2 | f2", data = df)

pf.iplot([fit1, fit2, fit3], labels = rename_categoricals(fit1._coefnames))
pf.iplot(
    models = [fit1, fit2, fit3],
    labels = rename_categoricals(fit1._coefnames)
)
pf.iplot(
    models = [fit1, fit2, fit3],
    rename_models = {
        fit1._model_name_plot: "Model 1",
        fit2._model_name_plot: "Model 2",
        fit3._model_name_plot: "Model 3"
    },
)
pf.iplot(
    models = [fit1, fit2, fit3],
    rename_models = {
        "Y~i(f1)": "Model 1",
        "Y~i(f1)+X2": "Model 2",
        "Y~i(f1)+X2|f2": "Model 3"
    },
)
pf.iplot([fit1], joint = "both")
```

---

# reference/report.summary.html

# report.summary { #pyfixest.report.summary }

```python
report.summary(models, digits=3)
```

Print a summary of estimation results for each estimated model.

For each model, this method prints a header indicating the fixed-effects and the
dependent variable, followed by a table of coefficient estimates with standard
errors, t-values, and p-values.

## Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                     | Description                                                                    | Default    |
|--------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------|------------|
| models | A supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of | Feols, Fepois & Feiv models.                                                   | _required_ |
| digits | int                                                                      | The number of decimal places to round the summary statistics to. Default is 3. | `3`        |

## Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description   |
|--------|--------|---------------|
|        | None   |               |

## Examples {.doc-section .doc-section-examples}

```{python}
import pyfixest as pf

# load data
df = pf.get_data()
fit1 = pf.feols("Y~X1 + X2 | f1", df)
fit2 = pf.feols("Y~X1 + X2 | f1 + f2", df)
fit3 = pf.feols("Y~X1 + X2 | f1 + f2 + f3", df)

pf.summary([fit1, fit2, fit3])
```

---

# regression_decomposition.html

# How much do Covariates Matter?

## Motivation

In regression analyses, we often wonder about "how much covariates matter?" for explaining the relationship between a target variable $D$ and an outcome variable $Y$. 

For example, we might start analysing the gender wage gap with a simple regression model as `log(wage) on gender`. But arguably, men and women differ in many socio-economic characteristics: they might have different (average) levels of education or career experience, and they might work in different industries and select into different higher- or lower-paying industries. So which fraction of the gender wage gap can be explained by these observable characteristics? 

In this notebook, we will compute and decompose the gender wage gab based on a subset of the PSID data set using a method commonly known as the 
"Gelbach Decomposition" ([Gelbach, JoLE 2016](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)). 

We start with loading a subset of the PSID data provided by the AER R package. 

```python
import re

import pandas as pd

import pyfixest as pf

psid = pd.read_csv(
    "https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/refs/heads/master/csv/AER/PSID7682.csv"
)
psid["experience"] = pd.Categorical(psid["experience"])
psid["year"] = pd.Categorical(psid["year"])
psid.head()
```

```text
   rownames experience  weeks occupation industry south smsa married gender  \
0         1          3     32      white       no   yes   no     yes   male   
1         2          4     43      white       no   yes   no     yes   male   
2         3          5     40      white       no   yes   no     yes   male   
3         4          6     39      white       no   yes   no     yes   male   
4         5          7     42      white      yes   yes   no     yes   male   

  union  education ethnicity  wage  year  id  
0    no          9     other   260  1976   1  
1    no          9     other   305  1977   1  
2    no          9     other   402  1978   1  
3    no          9     other   402  1979   1  
4    no          9     other   429  1980   1  
```

Computing a first correlation between gender and wage, we find that males earn on average 0.474 log points
more than women. 

```python
fit_base = pf.feols("log(wage) ~ gender", data=psid, vcov="hetero")
fit_base.summary()
```

```text
['###\n', '\n', 'Estimation:  OLS\n', 'Dep. var.: log(wage), Fixed effects: 0\n', 'Inference:  hetero\n', 'Observations:  4165\n', '\n', '| Coefficient    |   Estimate |   Std. Error |   t value |   Pr(>|t|) |   2.5% |   97.5% |\n', '|:---------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n', '| Intercept      |      6.255 |        0.020 |   320.714 |      0.000 |  6.217 |   6.294 |\n', '| gender[T.male] |      0.474 |        0.021 |    22.818 |      0.000 |  0.434 |   0.515 |\n', '---\n', 'RMSE: 0.436 R2: 0.106 \n']
```

To examine the impact of observable on the relationship between wage and gender, a common strategy in applied research is to incrementally add a set of covariates to the baseline regression model of `log(wage) on gender`. Here, we will incrementally add the following covariates:  

- education,
- experience
- occupation, 
- industry 
- year 
- ethnicity

We can do so by using **multiple estimation syntax**: 

```python
fit_stepwise1 = pf.feols(
    "log(wage) ~ gender + csw0(education, experience, occupation, industry, year, ethnicity)",
    data=psid,
)
pf.etable(fit_stepwise1)
```

```text
GT(_tbl_data=   level_0             level_1                      0                      1  \
0     coef      gender[T.male]  0.474*** <br> (0.021)  0.474*** <br> (0.019)   
1     coef           education                         0.065*** <br> (0.002)   
2     coef     experience[T.2]                                                 
3     coef     experience[T.3]                                                 
4     coef     experience[T.4]                                                 
..     ...                 ...                    ...                    ...   
61    coef           Intercept  6.255*** <br> (0.020)  5.419*** <br> (0.034)   
62   stats        Observations                   4165                   4165   
63   stats           S.E. type                    iid                    iid   
64   stats       R<sup>2</sup>                  0.106                  0.260   
65   stats  Adj. R<sup>2</sup>                  0.105                  0.260   

                        2                      3                      4  \
0   0.425*** <br> (0.018)  0.444*** <br> (0.018)  0.428*** <br> (0.018)   
1   0.075*** <br> (0.002)  0.061*** <br> (0.003)  0.063*** <br> (0.003)   
2      0.147 <br> (0.156)     0.156 <br> (0.155)     0.158 <br> (0.154)   
3      0.273 <br> (0.139)    0.284* <br> (0.138)    0.278* <br> (0.138)   
4    0.377** <br> (0.137)   0.389** <br> (0.136)   0.385** <br> (0.135)   
..                    ...                    ...                    ...   
61  4.566*** <br> (0.134)  4.664*** <br> (0.133)  4.636*** <br> (0.133)   
62                   4165                   4165                   4165   
63                    iid                    iid                    iid   
64                  0.376                  0.387                  0.391   
65                  0.368                  0.379                  0.383   

                        5                      6  
0   0.432*** <br> (0.016)  0.410*** <br> (0.017)  
1   0.060*** <br> (0.002)  0.059*** <br> (0.002)  
2      0.124 <br> (0.137)     0.128 <br> (0.136)  
3      0.232 <br> (0.122)    0.241* <br> (0.122)  
4     0.287* <br> (0.121)    0.298* <br> (0.120)  
..                    ...                    ...  
61  4.672*** <br> (0.118)  4.570*** <br> (0.118)  
62                   4165                   4165  
63                    iid                    iid  
64                  0.520                  0.525  
65                  0.513                  0.518  

[66 rows x 9 columns], _body=<great_tables._gt_data.Body object at 0x00000242B838F770>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None), ColInfo(var='2', type=<ColInfoTypeEnum.default: 1>, column_label='(3)', column_align='center', column_width=None), ColInfo(var='3', type=<ColInfoTypeEnum.default: 1>, column_label='(4)', column_align='center', column_width=None), ColInfo(var='4', type=<ColInfoTypeEnum.default: 1>, column_label='(5)', column_align='center', column_width=None), ColInfo(var='5', type=<ColInfoTypeEnum.default: 1>, column_label='(6)', column_align='center', column_width=None), ColInfo(var='6', type=<ColInfoTypeEnum.default: 1>, column_label='(7)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B80BD2B0>, _spanners=Spanners([SpannerInfo(spanner_id='log(wage)', spanner_level=1, spanner_label='log(wage)', spanner_units=None, spanner_pattern=None, vars=['0', '1', '2', '3', '4', '5', '6'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_n
...[truncated]...
```

Because the table is so long that it's hard to see anything, we restrict it to display only a few variables: 

```python
pf.etable(fit_stepwise1, keep=["gender", "ethnicity", "education"])
```

```text
GT(_tbl_data=  level_0             level_1                      0                      1  \
0    coef      gender[T.male]  0.474*** <br> (0.021)  0.474*** <br> (0.019)   
1    coef  ethnicity[T.other]                                                 
2    coef           education                         0.065*** <br> (0.002)   
3   stats        Observations                   4165                   4165   
4   stats           S.E. type                    iid                    iid   
5   stats       R<sup>2</sup>                  0.106                  0.260   
6   stats  Adj. R<sup>2</sup>                  0.105                  0.260   

                       2                      3                      4  \
0  0.425*** <br> (0.018)  0.444*** <br> (0.018)  0.428*** <br> (0.018)   
1                                                                        
2  0.075*** <br> (0.002)  0.061*** <br> (0.003)  0.063*** <br> (0.003)   
3                   4165                   4165                   4165   
4                    iid                    iid                    iid   
5                  0.376                  0.387                  0.391   
6                  0.368                  0.379                  0.383   

                       5                      6  
0  0.432*** <br> (0.016)  0.410*** <br> (0.017)  
1                         0.133*** <br> (0.020)  
2  0.060*** <br> (0.002)  0.059*** <br> (0.002)  
3                   4165                   4165  
4                    iid                    iid  
5                  0.520                  0.525  
6                  0.513                  0.518  , _body=<great_tables._gt_data.Body object at 0x00000242B8167440>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None), ColInfo(var='2', type=<ColInfoTypeEnum.default: 1>, column_label='(3)', column_align='center', column_width=None), ColInfo(var='3', type=<ColInfoTypeEnum.default: 1>, column_label='(4)', column_align='center', column_width=None), ColInfo(var='4', type=<ColInfoTypeEnum.default: 1>, column_label='(5)', column_align='center', column_width=None), ColInfo(var='5', type=<ColInfoTypeEnum.default: 1>, column_label='(6)', column_align='center', column_width=None), ColInfo(var='6', type=<ColInfoTypeEnum.default: 1>, column_label='(7)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B82DA3C0>, _spanners=Spanners([SpannerInfo(spanner_id='log(wage)', spanner_level=1, spanner_label='log(wage)', spanner_units=None, spanner_pattern=None, vars=['0', '1', '2', '3', '4', '5', '6'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B82DA8D0>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_a
...[truncated]...
```

We see that the coefficient on gender is roughly the same in all models. Tentatively, we might already conclude that the observable characteristics in the data do not explain a large part of the gender wage gap. 

But how much do differences in education matter? We have computed 6 additional models that contain education as a covariate. The obtained point estimates 
vary between $0.059$ and $0.075$. Which of these numbers should we report?  

Additionally, note that while we have only computed 6 additional models with covariates, the number of possible models is much larger. 
If I did the math correctly, simply by additively and incrementally adding covariates, we could have computed $57$ different models (not all of which would have included `education` as a control).

As it turns out, different models **will lead to different point estimates**. The order of incrementally adding covariates **might** impact our conclusion. To illustrate this, we keep the same ordering as before, but start with `ethnicity` as our first variable: 

```python
fit_stepwise2 = pf.feols(
    "log(wage) ~ gender + csw0(ethnicity, education, experience, occupation, industry, year)",
    data=psid,
)
pf.etable(fit_stepwise2, keep=["gender", "ethnicity", "education"])
```

```text
GT(_tbl_data=  level_0             level_1                      0                      1  \
0    coef      gender[T.male]  0.474*** <br> (0.021)  0.436*** <br> (0.022)   
1    coef  ethnicity[T.other]                         0.227*** <br> (0.026)   
2    coef           education                                                 
3   stats        Observations                   4165                   4165   
4   stats           S.E. type                    iid                    iid   
5   stats       R<sup>2</sup>                  0.106                  0.121   
6   stats  Adj. R<sup>2</sup>                  0.105                  0.121   

                       2                      3                      4  \
0  0.450*** <br> (0.020)  0.399*** <br> (0.019)  0.418*** <br> (0.019)   
1  0.141*** <br> (0.024)  0.158*** <br> (0.023)  0.151*** <br> (0.022)   
2  0.064*** <br> (0.002)  0.074*** <br> (0.002)  0.060*** <br> (0.003)   
3                   4165                   4165                   4165   
4                    iid                    iid                    iid   
5                  0.266                  0.383                  0.394   
6                  0.266                  0.375                  0.386   

                       5                      6  
0  0.404*** <br> (0.019)  0.410*** <br> (0.017)  
1  0.146*** <br> (0.022)  0.133*** <br> (0.020)  
2  0.062*** <br> (0.003)  0.059*** <br> (0.002)  
3                   4165                   4165  
4                    iid                    iid  
5                  0.397                  0.525  
6                  0.389                  0.518  , _body=<great_tables._gt_data.Body object at 0x00000242B7FE3B60>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='0', type=<ColInfoTypeEnum.default: 1>, column_label='(1)', column_align='center', column_width=None), ColInfo(var='1', type=<ColInfoTypeEnum.default: 1>, column_label='(2)', column_align='center', column_width=None), ColInfo(var='2', type=<ColInfoTypeEnum.default: 1>, column_label='(3)', column_align='center', column_width=None), ColInfo(var='3', type=<ColInfoTypeEnum.default: 1>, column_label='(4)', column_align='center', column_width=None), ColInfo(var='4', type=<ColInfoTypeEnum.default: 1>, column_label='(5)', column_align='center', column_width=None), ColInfo(var='5', type=<ColInfoTypeEnum.default: 1>, column_label='(6)', column_align='center', column_width=None), ColInfo(var='6', type=<ColInfoTypeEnum.default: 1>, column_label='(7)', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B81EF4D0>, _spanners=Spanners([SpannerInfo(spanner_id='log(wage)', spanner_level=1, spanner_label='log(wage)', spanner_units=None, spanner_pattern=None, vars=['0', '1', '2', '3', '4', '5', '6'], built=None)]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['Significance levels: * p < 0.05, ** p < 0.01, *** p < 0.001. Format of coefficient cell:\nCoefficient \n (Std. Error)'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B81ED640>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_a
...[truncated]...
```

We obtain 5 new coefficients on `education` that vary between 0.074 and 0.059. 

So, which share of the "raw" gender wage gap can be attributed to differences in education between men and women? Should we report a statisics based on the 0.075 estimate? Or 
on the 0.059 estimate? Which value should we pick? 

To help us with this problem, Gelbach (2016, JoLE) develops a decomposition procedure building on the omitted variable bias formula that produces a single value for the contribution of a given covariate, say education, to the gender wage gap.

## Notation and Gelbach's Algorithm

Before we dive into a code example, let us first introduce the notation and Gelbach's algorithm. We are interested in "decomposing" the effect of 
a variable $X_{1} \in \mathbb{R}$ on an outcome $Y \in \mathbb{R}$ into a part explained by covariates $X_{2} \in \mathbb{R}^{k_{2}}$ and an unexplained part. 

Thus we can specify two regression models: 

- The **short** model 
    $$
        Y = X_{1} \beta_{1} + u_{1}
    $$

- the **long** (or full) model 

    $$
        Y = X_{1} \beta_{1} + X_{2} \beta_{2} + e
    $$

By fitting the **short** regression, we obtain an estimate $\hat{\beta}_{1}$, which we will denote as the **direct effect**, and by estimating the **long** regression, we obtain an estimate of the regression coefficients $\hat{\beta}_{2} \in \mathbb{R}^{k_2}$. We will denote the estimate on $X_1$ in the long regression as the **full** effect. 

We can then compute the contribution of an individual covariate $\hat{\delta}_{k}$ via the following algorithm: 

- Step 1: we compute coefficients from $k_{2}$ auxiliary regression models $\hat{\Gamma}$ as 
    $$
        \hat{\Gamma} = (X_{1}'X_{1})^{-1} X_{1}'X_{2}
    $$

    In words, we regress the target variable $X_{1}$ on each covariate in $X_{2}$. In practice, we can easily do this in one line of code via `scipy.linalg.lstsq()`.

- Step 2: We can compute the total effect **explained** by the covariates, which we denote by $\delta$, as 

    $$
        \hat{\delta} = \sum_{k=1}^{k_2} \hat{\Gamma}_{k} \hat{\beta}_{2,k}
    $$

    where $\hat{\Gamma}_{k}$ are the coefficients from an auxiliary regression $X_1$ on covariate $X_{2,k}$ and $\hat{\beta}_{2,k}$ is the associated estimate on $X_{2,k}$ from the **full** model. 

    The individual **contribution of covariate $k$** is then defined as 

    $$
        \hat{\delta}_{k} = \hat{\Gamma}_{k} \hat{\beta}_{2,k}.
    $$


After having obtained $\delta_{k}$ for each auxiliary variable $k$, we can easily aggregate multiple variables into a single groups of interest. For example, if $X_{2}$ contains a set of dummies from industry fixed effects, we could compute the explained part of "industry" by summing over all the dummies: 

$$
        \hat{\delta}_{\textit{industry}} = \sum_{k \in \textit{industry dummies}} \hat{\Gamma}_{k} \hat{\beta}_{2,k}
$$

## `PyFixest` Example

To employ Gelbach's decomposition in `pyfixest`, we start with the **full** regression model that contains **all variables of interest**: 

```python
fit_full = pf.feols(
    "log(wage) ~ gender + ethnicity + education + experience + occupation + industry +year",
    data=psid,
)
```

After fitting the **full model**, we can run the decomposition procedure by calling the `decompose()` method. The only required argument is to specify the focal variable `decomp_var`, which in this case is "gender". Inference is conducted via a non-parametric bootstrap and can optionally be turned off.

```python
gb = fit_full.decompose(decomp_var="gender[T.male]", digits=5)
```

```text
['100%|██████████| 1000/1000 [00:58<00:00, 17.00it/s]\n']
```

As before, this produces a pretty big output table that reports 
- the **direct effect** of the regression of `log(wage) ~ gender`
- the **full effect** of gender on log wage using the **full regression** with all control variables
- the **explained effect** as the difference between the full and direct effect
- a **single scalar value** for the individual contributions of a covariate to overall **explained effect** 

For our example at hand, the additional covariates only explain a tiny fraction of the differences in log wages between men and women - 0.064 points. 
Of these, around one third can be attributed to ethnicity, 0.00064 to years of eduaction, etc. 

Note that for now, we ask `etable()` to only report effects in levels. By switching to `panels = "all"`, we would also report normalized coefficient; but we decided not to do so here as otherwise the table would have turned out even longer than it already has. 

```python
gb.etable(
    panels="levels",
)
```

```text
GT(_tbl_data=            level_0             level_1 Initial Difference  \
0    Levels (units)      gender[T.male]              0.474   
1    Levels (units)                         [0.469, 0.469]   
2    Levels (units)  ethnicity[T.other]                  -   
3    Levels (units)                                      -   
4    Levels (units)           education                  -   
..              ...                 ...                ...   
117  Levels (units)                                      -   
118  Levels (units)        year[T.1981]                  -   
119  Levels (units)                                      -   
120  Levels (units)        year[T.1982]                  -   
121  Levels (units)                                      -   

    Adjusted Difference Explained Difference  
0                 0.410                0.064  
1        [0.409, 0.409]       [0.060, 0.060]  
2                     -                0.023  
3                     -       [0.021, 0.021]  
4                     -                0.001  
..                  ...                  ...  
117                   -     [-0.005, -0.005]  
118                   -                0.000  
119                   -       [0.008, 0.008]  
120                   -                0.000  
121                   -       [0.000, 0.000]  

[122 rows x 5 columns], _body=<great_tables._gt_data.Body object at 0x00000242B826B170>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='Initial Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Initial Difference', column_align='center', column_width=None), ColInfo(var='Adjusted Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Adjusted Difference', column_align='center', column_width=None), ColInfo(var='Explained Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Explained Difference', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B8268CB0>, _spanners=Spanners([]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['\n            Decomposition variable: gender[T.male].\n        \n                CIs are computed using B = 1000 bootstrap replications\n             using iid sampling.Col 1: Raw Difference - Coefficient on gender[T.male] in short regression .\nCol 2: Adjusted Difference - Coefficient on gender[T.male] in long regression.\nCol 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression.\nPanel 1: Levels (units).'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B80F00E0>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', valu
...[truncated]...
```

Note: we can also return the decomposition results as a pd.DataFrame via the `tidy` method.

```python
gb.tidy().head()
```

```text
                    coefficients  ci_lower  ci_upper          panels
direct_effect           0.474466  0.469132  0.469132  Levels (units)
full_effect             0.410338  0.409346  0.409346  Levels (units)
explained_effect        0.064129  0.059785  0.059785  Levels (units)
unexplained_effect      0.410338  0.409346  0.409346  Levels (units)
ethnicity[T.other]      0.022749  0.021393  0.021393  Levels (units)
```

Because experience is a categorical variable, the table gets pretty unhandy: we produce one estimate for "each" level. Luckily, Gelbach's decomposition 
allows us to group individual contributions into a single number. In the `decompose()` method, we can combine variables via the `combine_covariates` argument: 

```python
gb2 = fit_full.decompose(
    decomp_var="gender[T.male]",
    combine_covariates={
        "experience": re.compile("experience"),
        "occupation": re.compile("occupation"),
        "industry": re.compile("industry"),
        "year": re.compile("year"),
        "ethnicity": re.compile("ethnicity"),
    },
    only_coef=True,  # suppress bootstrap for inference
)
```

We now report a single value for "experience", which explains a good chunk - around half - of the explained part of the gender wage gap. 

```python
gb2.etable(
    panels="levels",
)
```

```text
GT(_tbl_data=          level_0         level_1 Initial Difference Adjusted Difference  \
0  Levels (units)  gender[T.male]              0.474               0.410   
1  Levels (units)      experience                  -                   -   
2  Levels (units)      occupation                  -                   -   
3  Levels (units)        industry                  -                   -   
4  Levels (units)            year                  -                   -   
5  Levels (units)       ethnicity                  -                   -   

  Explained Difference  
0                0.063  
1                0.039  
2               -0.016  
3                0.018  
4                0.000  
5                0.023  , _body=<great_tables._gt_data.Body object at 0x00000242B81DE390>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='Initial Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Initial Difference', column_align='center', column_width=None), ColInfo(var='Adjusted Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Adjusted Difference', column_align='center', column_width=None), ColInfo(var='Explained Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Explained Difference', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B7FC1BE0>, _spanners=Spanners([]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['\n            Decomposition variable: gender[T.male].\n        Col 1: Raw Difference - Coefficient on gender[T.male] in short regression .\nCol 2: Adjusted Difference - Coefficient on gender[T.male] in long regression.\nCol 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression.\nPanel 1: Levels (units).'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B826B9E0>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo(scss=True, category='table', type='value', value='solid'), table_border_top_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_top_color=OptionsInfo(scss=True, category='table', type='value', value='#A8A8A8'), table_border_right_style=OptionsInfo(scss=True, category='table', ty
...[truncated]...
```

We can aggregate even more to "individual level" and "job" level variables: 

```python
gb3 = fit_full.decompose(
    decomp_var="gender[T.male]",
    combine_covariates={
        "job": re.compile(r".*(occupation|industry).*"),
        "personal": re.compile(r".*(education|experience|ethnicity).*"),
        "year": re.compile("year"),
    },
    only_coef=True,  # suppress inference
)
```

```python
gb3.etable(panels="levels")
```

```text
GT(_tbl_data=          level_0         level_1 Initial Difference Adjusted Difference  \
0  Levels (units)  gender[T.male]              0.474               0.410   
1  Levels (units)             job                  -                   -   
2  Levels (units)        personal                  -                   -   
3  Levels (units)            year                  -                   -   

  Explained Difference  
0                0.064  
1                0.002  
2                0.062  
3                0.000  , _body=<great_tables._gt_data.Body object at 0x00000242B8298500>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='Initial Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Initial Difference', column_align='center', column_width=None), ColInfo(var='Adjusted Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Adjusted Difference', column_align='center', column_width=None), ColInfo(var='Explained Difference', type=<ColInfoTypeEnum.default: 1>, column_label='Explained Difference', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B82C0CB0>, _spanners=Spanners([]), _heading=Heading(title=None, subtitle=None, preheader=None), _stubhead=None, _source_notes=['\n            Decomposition variable: gender[T.male].\n        Col 1: Raw Difference - Coefficient on gender[T.male] in short regression .\nCol 2: Adjusted Difference - Coefficient on gender[T.male] in long regression.\nCol 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression.\nPanel 1: Levels (units).'], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B829AA20>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo(scss=True, category='table', type='value', value='solid'), table_border_top_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_top_color=OptionsInfo(scss=True, category='table', type='value', value='#A8A8A8'), table_border_right_style=OptionsInfo(scss=True, category='table', type='value', value='none'), table_border_right_width=OptionsInfo(scss=True, category='table', type='px', value='2px'), table_border_right_color=OptionsInfo(scss=True, category='table', type='value', valu
...[truncated]...
```

We can easily change multiple aspects of the GT table that `etable` returns.
If we set `panels = "all"`, `etable()` will also report normalized coefficients. 

```python
gb3.etable(
    column_heads=["Column A", "Column B", "Column C"],
    panel_heads=["Panel A", "Panel B", "Panel C"],
    panels="all",
    add_notes="We add more notes.",
    caption="Gelbach Decomposition",
)
```

```text
GT(_tbl_data=    level_0         level_1 Column A Column B Column C
0   Panel A  gender[T.male]    0.474    0.410    0.064
1   Panel A             job        -        -    0.002
2   Panel A        personal        -        -    0.062
3   Panel A            year        -        -    0.000
4   Panel B  gender[T.male]    1.000    0.865    0.135
5   Panel B             job        -        -    0.004
6   Panel B        personal        -        -    0.132
7   Panel B            year        -        -    0.000
8   Panel C  gender[T.male]        -        -    1.000
9   Panel C             job        -        -    0.026
10  Panel C        personal        -        -    0.974
11  Panel C            year        -        -    0.000, _body=<great_tables._gt_data.Body object at 0x00000242B8298530>, _boxhead=Boxhead([ColInfo(var='level_0', type=<ColInfoTypeEnum.row_group: 3>, column_label='level_0', column_align='center', column_width=None), ColInfo(var='level_1', type=<ColInfoTypeEnum.stub: 2>, column_label='level_1', column_align='center', column_width=None), ColInfo(var='Column A', type=<ColInfoTypeEnum.default: 1>, column_label='Column A', column_align='center', column_width=None), ColInfo(var='Column B', type=<ColInfoTypeEnum.default: 1>, column_label='Column B', column_align='center', column_width=None), ColInfo(var='Column C', type=<ColInfoTypeEnum.default: 1>, column_label='Column C', column_align='center', column_width=None)]), _stub=<great_tables._gt_data.Stub object at 0x00000242B828C200>, _spanners=Spanners([]), _heading=Heading(title='Gelbach Decomposition', subtitle=None, preheader=None), _stubhead=None, _source_notes=['\n            Decomposition variable: gender[T.male].\n        Col 1: Raw Difference - Coefficient on gender[T.male] in short regression .\nCol 2: Adjusted Difference - Coefficient on gender[T.male] in long regression.\nCol 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression.\nPanel 1: Levels (units).\nPanel 2: Share of Full Effect: Levels normalized by coefficient of the short regression.\nPanel 3: Share of Explained Effect: Levels normalized by coefficient of the long regression.\n            We add more notes.\n            '], _footnotes=[], _styles=[], _locale=<great_tables._gt_data.Locale object at 0x00000242B828F290>, _formats=[], _substitutions=[], _options=Options(table_id=OptionsInfo(scss=False, category='table', type='value', value=None), table_caption=OptionsInfo(scss=False, category='table', type='value', value=None), table_width=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_layout=OptionsInfo(scss=True, category='table', type='value', value='fixed'), table_margin_left=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_margin_right=OptionsInfo(scss=True, category='table', type='px', value='auto'), table_background_color=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_additional_css=OptionsInfo(scss=False, category='table', type='values', value=[]), table_font_names=OptionsInfo(scss=False, category='table', type='values', value=['-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', 'Helvetica Neue', 'Fira Sans', 'Droid Sans', 'Arial', 'sans-serif']), table_font_size=OptionsInfo(scss=True, category='table', type='px', value='16px'), table_font_weight=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_style=OptionsInfo(scss=True, category='table', type='value', value='normal'), table_font_color=OptionsInfo(scss=True, category='table', type='value', value='#333333'), table_font_color_light=OptionsInfo(scss=True, category='table', type='value', value='#FFFFFF'), table_border_top_include=OptionsInfo(scss=False, category='table', type='boolean', value=True), table_border_top_style=OptionsInfo(scss=True, category='table', type='value', value='hidden'), table_border_top_width=OptionsInfo(scss=True, category='table'
...[truncated]...
```

We can visualise the Gelbach decomposition using the `coefplot` method.

```python
gb3.coefplot()
```

```text
<Figure size 1200x800 with 1 Axes>
```

## Literature 

- ["When do Covariates Matter? And Which Ones, and How Much?" by Gelbach, Jonah B. (2016), Journal of Labor Economics](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)

---

# replicating-the-effect.html

This notebook replicates code examples from Nick Huntington-Klein's book on causal inference, [The Effect](https://theeffectbook.net/).


```{python}
from causaldata import Mroz, gapminder, organ_donations, restaurant_inspections

import pyfixest as pf

%load_ext watermark
%watermark --iversions
```


## Chapter 4: Describing Relationships


```{python}
# Read in data
dt = Mroz.load_pandas().data
# Keep just working women
dt = dt.query("lfp")
# Create unlogged earnings
dt.loc[:, "earn"] = dt["lwg"].apply("exp")

# 5. Run multiple linear regression models by succesively adding controls
fit = pf.feols(fml="lwg ~ csw(inc, wc, k5)", data=dt, vcov="iid")
pf.etable(fit)
```

## Chapter 13: Regression

### Example 1


```{python}
res = restaurant_inspections.load_pandas().data
res.inspection_score = res.inspection_score.astype(float)
res.NumberofLocations = res.NumberofLocations.astype(float)
res.dtypes

fit = pf.feols(fml="inspection_score ~ NumberofLocations", data=res)
pf.etable([fit])
```


### Example 2


```{python}
df = restaurant_inspections.load_pandas().data

fit1 = pf.feols(
    fml="inspection_score ~ NumberofLocations + I(NumberofLocations^2) + Year", data=df
)
fit2 = pf.feols(fml="inspection_score ~ NumberofLocations*Weekend + Year", data=df)

pf.etable([fit1, fit2])
```



### Example 3: HC Standard Errors


```{python}
pf.feols(fml="inspection_score ~ Year + Weekend", data=df, vcov="HC3").summary()
```


### Example 4: Clustered Standard Errors


```{python}
pf.feols(
    fml="inspection_score ~ Year + Weekend", data=df, vcov={"CRV1": "Weekend"}
).tidy()
```

### Example 5: Bootstrap Inference


```{python}
fit = pf.feols(fml="inspection_score ~ Year + Weekend", data=df)
fit.wildboottest(reps=999, param="Year")
```


## Chapter 16: Fixed Effects

### Example 1

tba

### Example 2


```{python}
gm = gapminder.load_pandas().data
gm["logGDPpercap"] = gm["gdpPercap"].apply("log")

fit = pf.feols(fml="lifeExp ~ C(country) + np.log(gdpPercap)", data=gm)
fit.tidy().head()
```


### Example 3: TWFE


```{python}
# Set our individual and time (index) for our data
fit = pf.feols(fml="lifeExp ~ np.log(gdpPercap) | country + year", data=gm)
fit.summary()
```


## Chapter 18: Difference-in-Differences

### Example 1


```{python}
od = organ_donations.load_pandas().data

# Create Treatment Variable
od["California"] = od["State"] == "California"
od["After"] = od["Quarter_Num"] > 3
od["Treated"] = 1 * (od["California"] & od["After"])

did = pf.feols(fml="Rate ~ Treated | State + Quarter", data=od)
did.summary()
```


### Example 3: Dynamic Treatment Effect


```{python}
od = organ_donations.load_pandas().data

# Create Treatment Variable
od["California"] = od["State"] == "California"
# od["Quarter_Num"] = pd.Categorical(od.Quarter_Num)
od["California"] = od.California.astype(float)

did2 = pf.feols(
    fml="Rate ~ i(Quarter_Num, California,ref=3) | State + Quarter_Num", data=od
)

did2.tidy()
```

---

# resources.html

A few classes and textbooks have adopted PyFixest for teaching. And for core textbooks in causal inference that have not (yet =) ) , we have (or are planning to) write PyFixest translations (there are also multiple current "good first issues" for this if you'd like to help us).

## Textbooks

- Data Analysis for Business, Economics, and Policy ("Gabor's Data Analysis"). [Data and code](https://gabors-data-analysis.com/data-and-code/) and [Jupyter Notebooks on github](https://github.com/gabors-data-analysis/da_case_studies)
- Coding for Economists: [Regression](https://aeturrell.github.io/coding-for-economists/econmt-regression.html)
- Tidy Finance comes with a [chapter on fixed effects](https://www.tidy-finance.org/python/fixed-effects-and-clustered-standard-errors.html) and clustered standard errors and [Differene-in-Differences estimation](https://www.tidy-finance.org/python/difference-in-differences.html) that uses pyfixest
- The Effect (first edition): [our PyFixest translation](https://pyfixest.org/replicating-the-effect.html)
- The Panel Data Chapter in [the Mixtape](https://mixtape.scunning.com/08-panel_data#data-exercise-survey-of-adult-service-providers). You can find a PyFixest translation [here](https://pyfixest.org/replicating-the-mixtape.html)

Textbooks / textbook chapters that we still want to cover:

- The Difference-in-Differences chapter in the Mixtape ([github issue here](https://github.com/py-econometrics/pyfixest/issues/998))
- All of the Python translation of Ding's textbook on causal inference ([github issue here](https://github.com/py-econometrics/pyfixest/issues/957))
- The "Brave and True" chapters on [Dummy Regression](https://matheusfacure.github.io/python-causality-handbook/06-Grouped-and-Dummy-Regression.html), [Instrumental Variables](https://matheusfacure.github.io/python-causality-handbook/08-Instrumental-Variables.html), [Difference-in-Differences](https://matheusfacure.github.io/python-causality-handbook/13-Difference-in-Differences.html) and [Panel Data and Fixed Effects](https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html).

## Classes

If you are teaching with pyfixest, we'd love to hear from you!

- Econometrics II (taught by Vladislav Morozov at UBonn): Great intro to fixed effects estimation theory. Slides on fixed effects [here](https://vladislav-morozov.github.io/econometrics-2/slides/panel/fe.html#/title-slide), full class notes [here](https://vladislav-morozov.github.io/econometrics-2/), [github repository](https://github.com/vladislav-morozov/econometrics-2)
- Empirical Economics (taught at University of Utrecht 2025-2026) - MSc class in empirical economics.
- ECON 526 - MA-level course in quantitative economics, data science, and causal inference in economics, taught at the University of Brisith Columbia. [Class notes here](https://github.com/ubcecon/ECON526/tree/main_2025)


## Blog Posts, Notebooks, Videos

If you've written a blog post that illustrates how to use pyfixest, please let us know, we'd love to link to it.

- PyData Berlin Presentation (2024) on PyFixest: [link](https://www.youtube.com/watch?v=kSQxGGA7Rr4)

---

# skills.html

# Skills

This page provides a ready-to-use skill file for analytics projects that use PyFixest. The goal is to make LLM-assisted analysis more reliable by giving the model a concise, authoritative reference for PyFixest usage.

## How To Use

1. Copy the skill content below into a file named `SKILL.md` or `Agent.md` (or a tool-specific skill file) in your analytics project.
2. Ensure your LLM tool is configured to read project skills.
3. Use the formula and inference guidelines below as the authoritative PyFixest reference for the model.

## Skill File (PyFixest)

```markdown
# PyFixest Skill

## Core API (4 functions)

- `pyfixest.feols(fml, data, vcov, weights, ssc, fixef_rm, ...)`: OLS/WLS/IV with fixed effects.
- `pyfixest.fepois(fml, data, vcov, ...)`: Poisson regression with fixed effects.
- `pyfixest.feglm(fml, data, family, vcov, ...)`: GLM regression (family: "logit", "probit", "gaussian") with fixed effects.
- `pyfixest.quantreg(fml, data, quantile, ...)`: Quantile regression via interior point solver.

## Formula Syntax

Formulas follow fixest syntax and are split into 1–3 parts by `|`:

- One-part: `"Y ~ X1 + X2"` (no fixed effects, no IV)
- Two-part: `"Y ~ X1 + X2 | FE1 + FE2"` (fixed effects)
- Two-part IV: `"Y ~ X1 + X2 | X_endog ~ Z1 + Z2"` (IV without fixed effects)
- Three-part IV: `"Y ~ X1 + X2 | FE1 + FE2 | X_endog ~ Z1 + Z2"` (IV with fixed effects)

IV behavior:
- The IV part must be `endogenous ~ instruments`.
- Exogenous variables from the second-stage RHS are automatically added to the first stage.
- Endogenous variables are automatically added to the second stage.
- Multiple endogenous variables are not supported.

Other syntax:
- Multiple depvars are expanded to multiple estimations: `"Y1 + Y2 ~ X1"` behaves like `"sw(Y1, Y2) ~ X1"`.
- `i()` creates indicator expansions and interactions. It expands categorical variables into dummies and can interact a categorical variable with a numeric or another categorical variable.
  Examples: `i(cat)`, `i(cat, ref="Base")`, `i(cat, x)` (numeric `x` gives varying slopes; categorical `x` gives cat-by-x indicators), `i(cat1, cat2, ref2="Base")`.
  Example (cat × numeric): `Y ~ i(industry, exposure)` creates industry-specific slopes on `exposure`.
  Example (cat × cat): `Y ~ i(state, year, ref2=2000)` creates state-by-year indicators with 2000 as the base year.
- Standard interactions work as well. `X1 * X2` expands to `X1 + X2 + X1:X2`, while `X1:X2` is the interaction term only.
- Interacted FEs: `"Y ~ X1 | FE1 ^ FE2"` (creates a combined FE).

### Multiple Estimation Operators

Operators can appear anywhere in the formula (RHS, fixed effects, IV parts). They can be combined; expansion is recursive and produces all combinations. Note that multiple estimation can be significantly faster than independent model calls due to internal optimisations.

`sw` (sequential stepwise):
- `y ~ x1 + sw(x2, x3)` → `y ~ x1 + x2` and `y ~ x1 + x3`

`sw0` (sequential stepwise with zero step):
- `y ~ x1 + sw0(x2, x3)` → `y ~ x1`, `y ~ x1 + x2`, `y ~ x1 + x3`

`csw` (cumulative stepwise):
- `y ~ x1 + csw(x2, x3)` → `y ~ x1 + x2`, `y ~ x1 + x2 + x3`

`csw0` (cumulative stepwise with zero step):
- `y ~ x1 + csw0(x2, x3)` → `y ~ x1`, `y ~ x1 + x2`, `y ~ x1 + x2 + x3`

`mvsw` (multiverse stepwise):
- `y ~ mvsw(x1, x2, x3)` → all non-empty combinations plus the zero step:
  `y ~ 1`, `y ~ x1`, `y ~ x2`, `y ~ x3`, `y ~ x1 + x2`, `y ~ x1 + x3`, `y ~ x2 + x3`, `y ~ x1 + x2 + x3`

Combining operators example:
- `y ~ csw(x1, x2) + sw(z1, z2)` expands to:
  `y ~ x1 + z1`, `y ~ x1 + z2`, `y ~ x1 + x2 + z1`, `y ~ x1 + x2 + z2`

You can run regressions for subsamples by using the `split` and `fsplit` arguments, where both split by the
provided variable, but `fsplit` also provides a fit for the full sample.

## Inference (vcov)

Pass to `vcov`:

- `"iid"` — IID errors
- `"hetero"` — HC1 heteroskedasticity-robust (alias: `"HC1"`, `"HC2"`, `"HC3"`)
- `{"CRV1": "cluster_var"}` — Cluster-robust variance
- `{"CRV3": "cluster_var"}` — Leave-one-cluster-out jackknife
- `"nw"` — Newey-West HAC (requires panel_id and time_id)
- `"dk"` — Driscoll-Kraay HAC (requires panel_id and time_id)

## Post Processing

Model objects support:

- `.summary()` — Print regression summary
- `.tidy()` — Tidy DataFrame of coefficients, SEs, t-stats, p-values, CIs
- `.coef()` — Coefficient values
- `.se()` — Standard errors
- `.pvalue()` — P-values
- `.confint()` — Confidence intervals
- `.predict(newdata)` — Predictions
- `.resid()` — Residuals
- `.vcov()` — Variance-covariance matrix
- `.wildboottest(param, reps, seed)` — Wild cluster bootstrap inference
- `.ccv(treatment, pk, qk, ...)` — Causal cluster variance estimator
- `.ritest(param, reps, ...)` — Randomization inference
- `.decompose(param, x1_vars, type, ...)` — Gelbach (2016) decomposition

## etable Basics

For regression tables, use `pf.etable()`.

- Build tables: `pf.etable([fit1, fit2, ...])` or `pf.etable(pf.feols("Y~csw(X1,X2)", data))`.
- Output formats: `type="gt"` (default), `"md"`, `"tex"`, `"df"`.
- Keep/drop variables: `keep="X1"` or `drop=["X2"]`.
- Labels and fixed effects labels: `labels={...}`, `felabels={...}`.
- Show p-values or CIs: `coef_fmt="b (se) [p]"` shows the coefficient - b; standard error in parentheses, pvalue in paranteres.
- Title/caption: `caption="Regression Results"`.
- Rename variables: `labels={"X1": "Age", "X2": "Schooling"}`.
- Column headers for dependent variables: `model_heads=[...]` and `head_order="hd"`/`"dh"` to control header order (headlines vs depvars).
```

---

# ssc.html

The `fixest` R package provides various options for small sample corrections. While it has an excellent [vignette](https://cran.r-project.org/web/packages/fixest/vignettes/standard_errors.html) on the topic, reproducing its behavior in `pyfixest` took more time than expected. So that future developers (and my future self) can stay sane, I’ve compiled all of my hard-earned understanding of how small sample adjustments work in `fixest` and how they are implemented in `pyfixest` in this document.

In both `fixest` and `pyfixest`, small sample corrections are controlled by the `ssc` function. In `pyfixest`, `ssc` accepts four arguments: `k_adj`, `G_adj`, `k_fixef` and `G_df`.

Based on these inputs, the adjusted variance-covariance matrix is computed as:

```
vcov_k_adj = adj_val(N, df_k) if k_adj else 1
          * G_adj_val(G, G_df) if G_adj else 1
          * vcov
```

Where:

- **`k_adj`**: Enables or disables the first scalar adjustment.
- **`G_adj`**: Enables or disables the second scalar adjustment. This argument is only relevant / applied to clustered errors.
- **`vcov`**: The unadjusted variance-covariance matrix.
- **`df_k`**: The number of estimated parameters considered in the first adjustment. Impacts `adj_val`.
- **`k_fixef`**: Determines how `df_k` is computed (how fixed effects are counted).
- **`G_df`**: Determines how `G_adj_val` is computed (only relevant for multi-way clustering).
- **`G`**: The number of unique clusters (`G = N` for heteroskedastic errors).

Outside of this formula, we have **`df_t`**, which is the degrees of freedom used for p-values and confidence intervals:

- `df_t = N - df_k` for IID or heteroskedastic errors.
- `df_t = G - 1` for clustered errors.

---

# Small Sample Adjustments

## `k_adj = True`

If `k_adj = True`, the adjustment factor is:

`adj_val = (N - 1) / (N - df_k)`

If `k_adj = False`, no adjustment is applied.

Note that for `k_adj = True` and heteroskedastic errors, the applied correction is `N / (N-k)`.

---

## `k_fixef`

The `k_fixef` argument controls how fixed effects contribute to `df_k`, and thus to `adj_val`. It supports three options:

- **`"none"`**
- **`"full"`**
- **`"nonnested"`**

### `k_fixef = "none"`

Fixed effects are ignored when counting parameters:

- **Example**:
  - `Y ~ X1 | f1` → `k = 1`
  - `Y ~ X1 + X2 | f1` → `k = 2`

### `k_fixef = "full"`

Fixed effects are fully counted. For `n_fe` total fixed effects and each fixed effect `f_i`, we set `df_k = k + k_fe`,


- If there is **more than one** fixed effect, we drop one level from each fixed effects except the first (to avoid multicollinearity)
  `k_fe = sum_{i=1}^{n_fe} levels(f_i) - (n_fe - 1)`

- If there is **only one** fixed effect:
  `k_fe = sum_{i=1}^{n_fe} levels(f_i) = levels(f_1)`

### `k_fixef = "nonnested"`

Fixed effects may be **nested** within cluster variables (e.g., district FEs nested in state clusters). If `k_fixef = "nonnested"`, nested fixed effects do not count toward `k_fe`:

`k_fe = sum_{i=1}^{n_fe} levels(f_i) - k_fe_nested - (n_fe - 1)`

where `k_fe_nested` is the count of nested fixed effects. For cluster fixed effects, `k_fe_nested = G`, the number of clusters.

> ⚠️ *Note:* If you already subtracted a level from a nested FE, you may need to add it back.

---

## `G_adj`

This argument is only relevant for clustered errors.

If `G_adj = True`, we apply a second correction:

`G_df_val = G / (G - 1)`

Where:

- `G` is the number of clusters for clustered errors, or `N` for heteroskedastic errors.
- This follows the approach in R’s `sandwich` package, interpreting heteroskedastic errors as “singleton clusters.”

> *Tip:* If `G_adj = True` for IID errors, `G_df_val` defaults to `1`. For *heteroskedastic erros*, despite its name, `G_adj=True` will apply an adjustment of (N-1) / N, as there are $G = N$ singleton clusters.

---

## `G_df`

Relevant only for **multi-way clustering**. Two-way clustering, for example, can be written as:

`vcov = ssc_A * vcov_A + ssc_B * vcov_B - ssc_AB * vcov_AB`

where `A` and `B` are clustering dimensions, with `G_AB > G_A > G_B`.

- If `G_df = "min"`, then G is set to the minimum value of `G_A`, `G_B`, and `G_AB`.
- If `G_df = "conventional"`, each clustering dimension uses its own cluster count (`G_A`, `G_B`, etc.) for its respective adjustment.

---

# More on Inference

For computing critical values:

- **OLS and IV**: use t-statistics with `df_t = N - df_k` (non-clustered) or `df_t = G - 1` (clustered).
- **GLMs**: use z-statistics (normal approximation).

For multi-way clustering:

- **Two-way**: `df_t = min(G_1 - 1, G_2 - 1)`
- **Three-way**: `df_t = min(G_1 - 1, G_2 - 1, G_3 - 1)` *(not currently supported)*

See [this implementation](https://github.com/py-econometrics/pyfixest/blob/864da9c0d1797aff70e3f5b420e4c73f7256642d/pyfixest/estimation/feols_.py#L851) for details.

# In Code

All of the above logic is implemented [here](https://github.com/py-econometrics/pyfixest/blob/69acf9d22eab4300853d80264ee6d01bc4bdcb35/pyfixest/utils/utils.py#L108).

---

# stata-2-pyfixest.html

# How to Get Started

This guide will focus on how to replicate the regression results you would get in Stata
with the Python package `pyfixest` and assumes you know how to do things like install
Python packages and load data into Pandas.  For a broader introduction to doing
econmetrics in Python you might check out Arthur Turrell's
[Coding for Economist](https://aeturrell.github.io/coding-for-economists/intro.html),
which includes a section on
[Coming from Stata](https://aeturrell.github.io/coding-for-economists/coming-from-stata.html),
or [Tidy Finance with Python](https://www.tidy-finance.org/python/) by Christopher
Scheuch, Stefan Voigt, Patrick Weiss, and Christoph Frey. You can also check out the fixest section
of [stata2r](https://stata2r.github.io/fixest/) as there is a lot of overlap between fixest and PyFixest
syntax.

# Data

`pyfixest` includes a function to generate a dataset for testing.

```{python}
import pyfixest as pf
df = pf.get_data()
```

If you want to use the same dataset in Stata, you can save the data to your home
directory as a .dta file with

```{python, eval = FALSE}
import os
df.to_stata(os.path.join(os.path.expanduser("~"), "pyfixest-data.dta"))
```

and then load the data in Stata with

```stata
cd ~
use pyfixest-data.dta
```

# Basic OLS

To do a basic linear regression in `pyfixest` you would simply use

```{python}
fit1 = pf.feols("Y ~ X1", data = df)
fit2 = pf.feols("Y ~ X1 + X2", data = df)
```

which is equivalent to

```stata
reg Y X1

*       Source |       SS           df       MS      Number of obs   =       998
* -------------+----------------------------------   F(1, 996)       =    139.30
*        Model |  650.171727         1  650.171727   Prob > F        =    0.0000
*     Residual |  4648.80985       996  4.66747977   R-squared       =    0.1227
* -------------+----------------------------------   Adj R-squared   =    0.1218
*        Total |  5298.98157       997  5.31492635   Root MSE        =    2.1604
*
* ------------------------------------------------------------------------------
*            Y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -1.000086   .0847353   -11.80   0.000    -1.166366   -.8338056
*        _cons |   .9185178   .1118212     8.21   0.000     .6990856     1.13795
* ------------------------------------------------------------------------------

reg y X1 X2

*       Source |       SS           df       MS      Number of obs   =       998
* -------------+----------------------------------   F(2, 995)       =    106.99
*        Model |  937.866146         2  468.933073   Prob > F        =    0.0000
*     Residual |  4361.11543       995  4.38303058   R-squared       =    0.1770
* -------------+----------------------------------   Adj R-squared   =    0.1753
*        Total |  5298.98157       997  5.31492635   Root MSE        =    2.0936
*
* ------------------------------------------------------------------------------
*            Y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9929358   .0821175   -12.09   0.000    -1.154079   -.8317925
*           X2 |  -.1763424    .021766    -8.10   0.000    -.2190549   -.1336299
*        _cons |   .8887791   .1084224     8.20   0.000     .6760163    1.101542
* ------------------------------------------------------------------------------
```

in Stata.  However, you should note that this will only run the regressions and store
the results in `fit1` and `fit2`.  To show the results you can use some of the following
methods and functions.

```{python}
fit1.summary() # Basic summary statisticsmodels
```

```{python}
fit1.tidy() # Estimates, std errors, t-values, etc. in a "tidy" tablemodels
```

```{python}
pf.report.etable([fit1, fit2]) # Customizable table that can include results for multiple models
```

You can also access individual parts of the results with a variety of methods like

```{python}
fit1.coef() # Get the coefficients
```

```{python}
fit1.se() # Get the standard errors
```

```{python}
fit1.pvalue() # Get the p-values
```

## Robust Standard Errors

To get heteroskedasticity robust standard errors you can use

```{python}
fit3 = pf.feols("Y ~ X1 + X2", data=df, vcov="HC1")
fit3.summary()
```

which is equivalent to

```stata
reg Y X1 X2, robust

* Linear regression                               Number of obs     =        998
*                                                 F(2, 995)         =     107.91
*                                                 Prob > F          =     0.0000
*                                                 R-squared         =     0.1770
*                                                 Root MSE          =     2.0936
*
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9929358   .0798259   -12.44   0.000    -1.149582   -.8362893
*           X2 |  -.1763424   .0216936    -8.13   0.000    -.2189129   -.1337719
*        _cons |   .8887791   .1077457     8.25   0.000     .6773442    1.100214
* ------------------------------------------------------------------------------
```

or

```stata
reg Y X1 X2, vce(robust)

* Identical output to above
```

or you can choose a different type of robust standard errors like "HC3" using

```{python}
fit4 = pf.feols("Y ~ X1 + X2", data=df, vcov="HC3")
fit4.summary()
```

Note: This will not exactly match the output of the equivalent Stata command, which is

```stata
reg Y X1 X2, vce(hc3)

* Linear regression                               Number of obs     =        998
*                                                 F(2, 995)         =     107.38
*                                                 Prob > F          =     0.0000
*                                                 R-squared         =     0.1770
*                                                 Root MSE          =     2.0936
*
* ------------------------------------------------------------------------------
*              |             Robust HC3
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9929358   .0799832   -12.41   0.000    -1.149891   -.8359807
*           X2 |  -.1763424   .0217734    -8.10   0.000    -.2190693   -.1336154
*        _cons |   .8887791   .1079372     8.23   0.000     .6769684     1.10059
* ------------------------------------------------------------------------------
```

this is because by default, `pyfixest` uses two small sample size corrections for HC3
robust standard errors, while Stata only uses one of them. You can turn off the
correction that Stata doesn't use with the `ssc` argument.

```{python}
fit5 = pf.feols("Y ~ X1 + X2", data=df, vcov="HC3", ssc=pf.ssc(k_adj = False))
fit5.summary()
```

which matches Stata exactly.  You can read all about the small sample size corrections
implememnted by `pyfixest` at
[On Small Sample Corrections](https://pyfixest.org/ssc.html).

## Clustered Standard Errors

To cluster the standard errors by group you can use

```{python}
fit6 = pf.feols("Y ~ X1 + X2", data=df.dropna(subset=['f1']), vcov={"CRV1": "f1"})
fit6.summary()
```

which is equivalent to

```stata
reg Y X1 X2, vce(cluster f1)

* Linear regression                               Number of obs     =        997
*                                                 F(2, 29)          =     102.51
*                                                 Prob > F          =     0.0000
*                                                 R-squared         =     0.1774
*                                                 Root MSE          =      2.094
*
*                                     (Std. err. adjusted for 30 clusters in f1)
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9951969   .0757246   -13.14   0.000    -1.150071   -.8403227
*           X2 |  -.1766173    .019799    -8.92   0.000    -.2171109   -.1361237
*        _cons |   .8895569   .2622066     3.39   0.002     .3532841     1.42583
* ------------------------------------------------------------------------------
```

Note: clustered standard errors are not supported with missing values in the cluster
variable, which is why we drop the rows with missing values for `f1`.

For two way clustering you would need to use

```{python}
fit7 = pf.feols(
  "Y ~ X1 + X2",
  data=df.dropna(subset=['f1', 'f2']),
  vcov={"CRV1": "f1 + f2"}
)
fit7.summary()
```

Note: This will not exactly match the output of the equivalent Stata command, which is

```stata
reg Y X1 X2, vce(cluster f1 f2)

* Linear regression                                       Number of obs =    997
* Clusters per comb.:                                     Cluster comb. =      3
*   min =  30                                             F(2, 29)      =  88.58
*   avg = 211                                             Prob > F      = 0.0000
*   max = 572                                             R-squared     = 0.1774
*                                                         Adj R-squared = 0.1758
*                                                         Root MSE      = 2.0940
*
*                                   (Std. err. adjusted for multiway clustering)
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9951969    .074771   -13.31   0.000    -1.148121   -.8422731
*           X2 |  -.1766173     .02376    -7.43   0.000     -.225212   -.1280226
*        _cons |   .8895569   .3016464     2.95   0.006     .2726208    1.506493
* ------------------------------------------------------------------------------
* Cluster combinations formed by f1 and f2.
```

this is because by default, `pyfixest` uses a small sample size correction that adjusts
each clustering dimentsion by whichever dimension has the smallest number of clusters,
while in Stata the default is to adjust each dimension based on the number of clusters
in that dimension. You can use the same correction as Stata through the `ssc` argument.

```{python}
fit8 = pf.feols(
  "Y ~ X1 + X2",
  df.dropna(subset=['f1', 'f2']),
  vcov={"CRV1": "f1 + f2"},
  ssc=pf.ssc(G_df="conventional")
)
fit8.summary()
```

As a reminder, for an excellent breakdown on small sample correction in the `pyfixest` package, you can check out
[On Small Sample Correction](https://pyfixest.org/ssc.html).

# Fixed Effect Regressions

To do a fixed effect regression with one fixed effect you could use

```{python}
fit9 = pf.feols("Y ~ X1 + X2 | f1", data=df, vcov="iid")
fit9.summary()
```

which is equivalent to

```stata
xtset f1
xtreg Y X1 X2, fe

* Fixed-effects (within) regression               Number of obs     =        997
* Group variable: f1                              Number of groups  =         30
*
* R-squared:                                      Obs per group:
*      Within  = 0.2388                                         min =         23
*      Between = 0.0770                                         avg =       33.2
*      Overall = 0.1774                                         max =         48
*
*                                                 F(2, 965)         =     151.33
* corr(u_i, Xb) = 0.0268                          Prob > F          =     0.0000
*
* ------------------------------------------------------------------------------
*            Y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9495256   .0663728   -14.31   0.000    -1.079777   -.8192739
*           X2 |  -.1742253   .0175957    -9.90   0.000    -.2087555   -.1396951
*        _cons |    .842222   .0872525     9.65   0.000     .6709955    1.013448
* -------------+----------------------------------------------------------------
*      sigma_u |  1.2570454
*      sigma_e |  1.6751049
*          rho |  .36026283   (fraction of variance due to u_i)
* ------------------------------------------------------------------------------
* F test that all u_i=0: F(29, 965) = 20.29                    Prob > F = 0.0000
```

or

```stata
reghdfe Y X1 X2, absorb(f1)

* HDFE Linear regression                            Number of obs   =        997
* Absorbing 1 HDFE group                            F(   2,    965) =     151.33
*                                                   Prob > F        =     0.0000
*                                                   R-squared       =     0.4890
*                                                   Adj R-squared   =     0.4726
*                                                   Within R-sq.    =     0.2388
*                                                   Root MSE        =     1.6751
*
* ------------------------------------------------------------------------------
*            Y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9495256   .0663728   -14.31   0.000    -1.079777   -.8192739
*           X2 |  -.1742253   .0175957    -9.90   0.000    -.2087555   -.1396951
*        _cons |    .842222   .0872525     9.65   0.000     .6709955    1.013448
* ------------------------------------------------------------------------------
*
* Absorbed degrees of freedom:
* -----------------------------------------------------+
*  Absorbed FE | Categories  - Redundant  = Num. Coefs |
* -------------+---------------------------------------|
*           f1 |        30           0          30     |
* -----------------------------------------------------+
```

To use cluster robust standard errors, specify the `vcov` argument:

```{python}
# Standard iid inference
fit10 = pf.feols("Y ~ X1 + X2 | f1", data=df)
fit10.summary()

# Cluster robust standard errors
fit10_clustered = pf.feols("Y ~ X1 + X2 | f1", data=df, vcov={"CRV1": "f1"})
fit10_clustered.summary()
```

The clustered version `fit10_clustered` is equivalent to

```stata
xtset f1
xtreg Y X1 X2, fe vce(cluster f1)

* Fixed-effects (within) regression               Number of obs     =        997
* Group variable: f1                              Number of groups  =         30
*
* R-squared:                                      Obs per group:
*      Within  = 0.2388                                         min =         23
*      Between = 0.0770                                         avg =       33.2
*      Overall = 0.1774                                         max =         48
*
*                                                 F(2, 29)          =     146.33
* corr(u_i, Xb) = 0.0268                          Prob > F          =     0.0000
*
*                                     (Std. err. adjusted for 30 clusters in f1)
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9495256   .0665572   -14.27   0.000     -1.08565   -.8134009
*           X2 |  -.1742253    .018409    -9.46   0.000    -.2118759   -.1365746
*        _cons |    .842222   .0694639    12.12   0.000     .7001523    .9842916
* -------------+----------------------------------------------------------------
*      sigma_u |  1.2570454
*      sigma_e |  1.6751049
*          rho |  .36026283   (fraction of variance due to u_i)
* ------------------------------------------------------------------------------
```

or

```stata
reghdfe Y X1 X2, absorb(f1) cluster(f1)

* HDFE Linear regression                            Number of obs   =        997
* Absorbing 1 HDFE group                            F(   2,     29) =     146.33
* Statistics robust to heteroskedasticity           Prob > F        =     0.0000
*                                                   R-squared       =     0.4890
*                                                   Adj R-squared   =     0.4726
*                                                   Within R-sq.    =     0.2388
* Number of clusters (f1)      =         30         Root MSE        =     1.6751
*
*                                     (Std. err. adjusted for 30 clusters in f1)
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9495256   .0665572   -14.27   0.000     -1.08565   -.8134009
*           X2 |  -.1742253    .018409    -9.46   0.000    -.2118759   -.1365746
*        _cons |    .842222   .0694639    12.12   0.000     .7001523    .9842916
* ------------------------------------------------------------------------------
*
* Absorbed degrees of freedom:
* -----------------------------------------------------+
*  Absorbed FE | Categories  - Redundant  = Num. Coefs |
* -------------+---------------------------------------|
*           f1 |        30          30           0    *|
* -----------------------------------------------------+
* * = FE nested within cluster; treated as redundant for DoF computation
```

For multiple fixed effects you could do

```{python}
fit11 = pf.feols("Y ~ X1 + X2 | f1 + f2", data=df)
fit11.summary()
```

which is equivalent to

```stata
reghdfe Y X1 X2, absorb(f1 f2) cluster(f1)

* HDFE Linear regression                            Number of obs   =        997
* Absorbing 2 HDFE groups                           F(   2,     29) =     182.76
* Statistics robust to heteroskedasticity           Prob > F        =     0.0000
*                                                   R-squared       =     0.6590
*                                                   Adj R-squared   =     0.6372
*                                                   Within R-sq.    =     0.3026
* Number of clusters (f1)      =         30         Root MSE        =     1.3893
*
*                                     (Std. err. adjusted for 30 clusters in f1)
* ------------------------------------------------------------------------------
*              |               Robust
*            Y | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
* -------------+----------------------------------------------------------------
*           X1 |  -.9240462   .0618743   -14.93   0.000    -1.050593   -.7974991
*           X2 |  -.1741073   .0148338   -11.74   0.000    -.2044458   -.1437689
*        _cons |   .8156588    .064596    12.63   0.000     .6835452    .9477723
* ------------------------------------------------------------------------------
*
* Absorbed degrees of freedom:
* -----------------------------------------------------+
*  Absorbed FE | Categories  - Redundant  = Num. Coefs |
* -------------+---------------------------------------|
*           f1 |        30          30           0    *|
*           f2 |        30           1          29     |
* -----------------------------------------------------+
* * = FE nested within cluster; treated as redundant for DoF computation
```

Note: To cluster standard errors by a specific group, use the `vcov` argument.
See the `fit8` example above for how to specify two way clustering.

---

# table-layout.html

## Table Layout with PyFixest

::: {.callout-note}
## Migration Notice
Starting with pyfixest 0.41.0 (currently in development), the table functionality is powered by [maketables](https://py-econometrics.github.io/maketables/).
The `pf.etable()` API remains unchanged. `pf.dtable()` is deprecated (use `DTable()` directly) and `pf.make_table()` has been removed (use `maketables.MTable()` directly).
:::

Pyfixest comes with functions to generate publication-ready tables. Regression tables are generated with `pf.etable()`, which can output different formats, for instance using the [Great Tables](https://posit-dev.github.io/great-tables/articles/intro.html) package or generating formatted LaTex Tables using [booktabs](https://ctan.org/pkg/booktabs?lang=en). Descriptive statistics tables can be created with `DTable()` and custom tables with `maketables.MTable()`.

To begin, we load some libraries and fit a set of regression models.

```{python}
import numpy as np
import pandas as pd
import pylatex as pl  # for the latex table; note: not a dependency of pyfixest - needs manual installation
from maketables import DTable
from great_tables import loc, style  # great_tables is used by maketables internally
from IPython.display import FileLink, display

import pyfixest as pf

%load_ext autoreload
%autoreload 2

data = pf.get_data()

fit1 = pf.feols("Y ~ X1 + X2 | f1", data=data)
fit2 = pf.feols("Y ~ X1 + X2 | f1 + f2", data=data)
fit3 = pf.feols("Y ~ X1 *X2 | f1 + f2", data=data)
fit4 = pf.feols("Y2 ~ X1 + X2 | f1", data=data)
fit5 = pf.feols("Y2 ~ X1 + X2 | f1 + f2", data=data)
fit6 = pf.feols("Y2 ~ X1 *X2 | f1 + f2", data=data)
```

# Regression Tables via `pf.etable()`

## Basic Usage

We can compare all regression models via the pyfixest-internal `pf.etable()` function:


```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6])
```

You can also estimate and display multiple regressions with one line of code using the (py)fixest stepwise notation:

```{python}
pf.etable(pf.feols("Y+Y2~csw(X1,X2,X1:X2)", data=data))
```

## Keep and drop variables
`etable` allows us to do a few things out of the box. For example, we can only keep the variables that we'd like, which keeps all variables that fit the provided regex match.


```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], keep="X1")
```

We can use the `exact_match` argument to select a specific set of variables:

```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], keep=["X1", "X2"], exact_match=True)
```

We can also easily **drop** variables via the `drop` argument:

```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], drop=["X1"])
```

## Display p-values or confidence intervals
By default, `pf.etable()` reports **standard errors**. But we can also ask to output p-values or confidence intervals via the `coef_fmt` function argument.


```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], coef_fmt="b \n (se) \n [p]")
```

## Significance levels and rounding
Additionally, we can also overwrite the defaults for the reported significance levels and control the rounding of results via the `signif_code` and `digits` function arguments:


```{python}
pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], signif_code=[0.01, 0.05, 0.1], digits=5)
```


## Other output formats
By default, `pf.etable()` returns a GT object (see [the Great Tables package](https://posit-dev.github.io/great-tables/articles/intro.html)), but you can also opt to dataframe, markdown, or latex output via the `type` argument.


```{python}
# Pandas styler output:
pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    signif_code=[0.01, 0.05, 0.1],
    digits=5,
    coef_fmt="b (se)",
    type="df",
)
```

```{python}
# Markdown output:
pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    signif_code=[0.01, 0.05, 0.1],
    digits=5,
    type="md",
)
```

To obtain latex output use `type = "tex"`. If you want to save the table as a tex file, you can use the `file_name=` argument to specify the respective path where it should be saved. Etable will use latex packages `booktabs`, `threeparttable`, `makecell`, and `tabularx` for the table layout, so don't forget to include these packages in your latex document.

```{python}
# LaTex output (include latex packages booktabs, threeparttable, makecell, and tabularx in your document):
tab = pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    signif_code=[0.01, 0.05, 0.1],
    digits=2,
    type="tex",
)
```

The following code generates a pdf including the regression table which you can display clicking on the link below the cell:

```{python}
## Use pylatex to create a tex file with the table


def make_pdf(tab, file):
    "Create a PDF document with tex table."
    doc = pl.Document()
    doc.packages.append(pl.Package("booktabs"))
    doc.packages.append(pl.Package("threeparttable"))
    doc.packages.append(pl.Package("makecell"))
    doc.packages.append(pl.Package("tabularx"))

    with (
        doc.create(pl.Section("A PyFixest LateX Table")),
        doc.create(pl.Table(position="htbp")) as table,
    ):
        table.append(pl.NoEscape(tab))

    doc.generate_pdf(file, clean_tex=False)


# Compile latex to pdf & display a button with the hyperlink to the pdf
# requires tex installation
run = False
if run:
    make_pdf(tab, "latexdocs/SampleTableDoc")
display(FileLink("latexdocs/SampleTableDoc.pdf"))
```


## Rename variables
You can also rename variables if you want to have a more readable output. Just pass a dictionary to the `labels` argument. Note that interaction terms will also be relabeled using the specified labels for the interacted variables (if you want to manually relabel an interaction term differently, add it to the dictionary).


```{python}
labels = {
    "Y": "Wage",
    "Y2": "Wealth",
    "X1": "Age",
    "X2": "Years of Schooling",
    "f1": "Industry",
    "f2": "Year",
}

pf.etable([fit1, fit2, fit3, fit4, fit5, fit6], labels=labels)
```

If you want to label the rows indicating the inclusion of fixed effects not with the variable label but with a custom label, you can pass on a separate dictionary to the `felabels` argument.


```{python}
pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    labels=labels,
    felabels={"f1": "Industry Fixed Effects", "f2": "Year Fixed Effects"},
)
```

## Rename categorical variables

By default, categorical variables are returned using the formulaic "C(variable)[T.value]" notation. Via the `cat_template` argument,
you can rename categorical variables via a specified template *{variable}={value}*. This works when either the variable is categorial in the DataFrame, or the C() or i() operators are used in the regresson formula.
´
```{python}

# Add a categorical variable
data['job'] = np.random.choice(["Managerial", "Admin", "Blue collar"], size=len(data), p=[1/3, 1/3, 1/3])
# Add a label for this variable to the dictionary
labels['job']="Job Family"

fit7 = pf.feols("Y ~ X1 + X2 + job", data = data)

pf.etable([fit7], labels=labels, cat_template = "{variable}::{value}")
```

But you can also remove the variable name and only keep the levels (categories) by specifying *cat_template="{value}"*. Note that the labeling of categories also works in interaction terms:

```{python}

fit7 = pf.feols("Y ~ X1 + X2 + job", data = data)
fit8 = pf.feols("Y ~ X1 + X2 + job*X2", data = data)

pf.etable([fit7, fit8], labels=labels, cat_template="{value}")
```
##  Change reference category
You can also change the reference category of a categorical variable using the `ref` argument in the  interaction `i()` operator. For example, repeating the last estimation but changing the reference category to "Managerial" instead of "Admin":

```{python}

fit9 = pf.feols("Y ~ X1 + X2 + i(job,ref='Managerial') + i(job,X2,ref='Managerial')", data = data)

pf.etable([fit9], labels=labels, cat_template="{value}")
```

Notice that this process will change the `_coefnames`. In this example, the new `_coefnames` are:
```{python}
fit9._coefnames
```

## Custom model headlines
You can also add custom headers for each model by passing a list of strings to the `model_headers` argument.


```{python}
pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    labels=labels,
    model_heads=["US", "China", "EU", "US", "China", "EU"],
)
```

Or change the ordering of headlines having headlines first and then dependent variables using the `head_order` argument. "hd" stands for headlines then dependent variables, "dh" for dependent variables then headlines. Assigning "d" or "h" can be used to only show dependent variables or only headlines. When head_order="" only model numbers are shown.


```{python}
pf.etable(
    [fit1, fit4, fit2, fit5, fit3, fit6],
    labels=labels,
    model_heads=["US", "US", "China", "China", "EU", "EU"],
    head_order="hd",
)
```


Remove the dependent variables from the headers:

```{python}
pf.etable(
    [fit1, fit4, fit2, fit5, fit3, fit6],
    labels=labels,
    model_heads=["US", "US", "China", "China", "EU", "EU"],
    head_order="",
)
```

## Further custom model information
You can add further custom model statistics/information to the bottom of the table by using the `custom_stats` argument to which you pass a dictionary with the name of the row and lists of values. The length of the lists must be equal to the number of models.


```{python}
pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    labels=labels,
    custom_model_stats={
        "Number of Clusters": [42, 42, 42, 37, 37, 37],
        "Additional Info": ["A", "A", "B", "B", "C", "C"],
    },
)
```


## Custom table notes
You can replace the default table notes with your own notes using the `notes` argument.

```{python}
mynotes = "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
pf.etable(
    [fit1, fit4, fit2, fit5, fit3, fit6],
    labels=labels,
    model_heads=["US", "US", "China", "China", "EU", "EU"],
    head_order="hd",
    notes=mynotes,
)
```


## Publication-ready LaTex tables
With few lines of code you thus obtain a publication-ready latex table:


```{python}
tab = pf.etable(
    [fit1, fit4, fit2, fit5, fit3, fit6],
    labels=labels,
    model_heads=["US", "US", "China", "China", "EU", "EU"],
    head_order="hd",
    type="tex",
    notes=mynotes,
    show_fe=True,
    show_se_type=False,
    custom_model_stats={
        "Number of Clusters": [42, 42, 42, 37, 37, 37],
    },
)

# Compile latex to pdf & display a button with the hyperlink to the pdf
run = False
if run:
    make_pdf(tab, "latexdocs/SampleTableDoc2")
display(FileLink("latexdocs/SampleTableDoc2.pdf"))
```


# Rendering Tables in Quarto
When you use quarto you can include latex tables generated by pyfixest when rendering the qmd file as pdf. Just specify `output: asis` in the code block options of the respective chunk and print the LaTex string returned by etable. Don't forget to include the `\usepackage` commands for necessary latex packages in the YAML block. Here you find a sample [qmd file](https://github.com/py-econometrics/pyfixest/blob/master/docs/quarto_example/QuartoExample.qmd).

When you render either a jupyter notebook or qmd file to html it is advisable to turn html-table-processing off in quarto as otherwise quarto adds further formatting which alters how your tables look like. You can do this in a raw cell at the top of your document.

<pre><code>---
format:
  html:
    html-table-processing: none
---</code></pre>

# Descriptive Statistics via `DTable()`

::: {.callout-warning}
## Deprecation Notice
`pf.dtable()` will be deprecated in the future. Please use `DTable` from the `maketables` package.
:::

The function `DTable()` allows to display descriptive statistics for a set of variables in the same layout.

## Basic Usage of DTable
Specify the variables you want to display the descriptive statistics for. You can also use a dictionary to rename the variables and add a caption.



```{python}
DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    labels=labels,
    caption="Descriptive statistics",
    digits=2,
)
```


Choose the set of statistics to be displayed with `stats`. You can use any pandas aggregation functions.


```{python}
DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    stats=["count", "mean", "std", "min", "max"],
    labels=labels,
    caption="Descriptive statistics",
)
```


## Summarize by characteristics in columns and rows
You can summarize by characteristics using the `bycol` argument when groups are to be displayed in columns. When the number of observations is the same for all variables in a group, you can also opt to display the number of observations only once for each group byin a separate line at the bottom of the table with `counts_row_below==True`.


```{python}
# Generate some categorial data
data["country"] = np.random.choice(["US", "EU"], data.shape[0])
data["occupation"] = np.random.choice(["Blue collar", "White collar"], data.shape[0])

# Drop nan values to have balanced data
data.dropna(inplace=True)

DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    labels=labels,
    bycol=["country", "occupation"],
    stats=["count", "mean", "std"],
    caption="Descriptive statistics",
    stats_labels={"count": "Number of observations"},
    counts_row_below=True,
)
```


You can also use custom aggregation functions to compute further statistics or affect how statistics are presented. Pyfixest provides two such functions `mean_std` and `mean_newline_std` which compute the mean and standard deviation and display both the same cell (either with line break between them or not). This allows to have more compact tables when you want to show statistics for many characteristcs in the columns.

You can also hide the display of the statistics labels in the header with `hide_stats_labels=True`. In that case a table note will be added naming the statistics displayed using its label (if you have not provided a custom note).


```{python}
DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    labels=labels,
    bycol=["country", "occupation"],
    stats=["mean_newline_std", "count"],
    caption="Descriptive statistics",
    stats_labels={"count": "Number of observations"},
    counts_row_below=True,
    hide_stats=True,
)
```


You can also split by characteristics in both columns and rows. Note that you can only use one grouping variable in rows, but several in columns (as shown above).


```{python}
DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    labels=labels,
    bycol=["country"],
    byrow="occupation",
    stats=["count", "mean", "std"],
    caption="Descriptive statistics",
)
```


And you can again export descriptive statistics tables also to LaTex:


```{python}
dtab = DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    labels=labels,
    bycol=["country"],
    byrow="occupation",
    stats=["count", "mean", "std"],
    type="tex",
)

run = False
if run:
    make_pdf(dtab, "latexdocs/SampleTableDoc3")
display(FileLink("latexdocs/SampleTableDoc3.pdf"))
```


# Custom Styling with Great Tables
You can use the rich set of methods offered by [Great Tables](https://posit-dev.github.io/great-tables/articles/intro.html) to further customize the table display when the type is "gt".

## Example Styling

```{python}
(
    pf.etable([fit1, fit2, fit3, fit4, fit5, fit6])
    .tab_options(
        column_labels_background_color="cornsilk",
        stub_background_color="whitesmoke",
    )
    .tab_style(
        style=style.fill(color="mistyrose"),
        locations=loc.body(columns="(3)", rows=["X2"]),
    )
)
```

## Defining Table Styles: Some Examples

You can easily define table styles that you can apply to all tables in your project. Just define a dictionary with the respective values for the tab options (see the [Great Tables documentation](https://posit-dev.github.io/great-tables/reference/GT.tab_options.html#great_tables.GT.tab_options)) and use the style with `.tab_options(**style_dict)`.


```{python}
style_print = {
    "table_font_size": "12px",
    "heading_title_font_size": "12px",
    "source_notes_font_size": "8px",
    "data_row_padding": "3px",
    "column_labels_padding": "3px",
    "row_group_border_top_style": "hidden",
    "table_body_border_top_style": "None",
    "table_body_border_bottom_width": "1px",
    "column_labels_border_top_width": "1px",
    "table_width": "14cm",
}


style_presentation = {
    "table_font_size": "16px",
    "table_font_color_light": "white",
    "table_body_border_top_style": "hidden",
    "table_body_border_bottom_style": "hidden",
    "heading_title_font_size": "18px",
    "source_notes_font_size": "12px",
    "data_row_padding": "3px",
    "column_labels_padding": "6px",
    "column_labels_background_color": "midnightblue",
    "stub_background_color": "whitesmoke",
    "row_group_background_color": "whitesmoke",
    "table_background_color": "whitesmoke",
    "heading_background_color": "white",
    "source_notes_background_color": "white",
    "column_labels_border_bottom_color": "white",
    "column_labels_font_weight": "bold",
    "row_group_font_weight": "bold",
    "table_width": "18cm",
}
```


```{python}
t1 = DTable(
    data,
    vars=["Y", "Y2", "X1", "X2"],
    stats=["count", "mean", "std", "min", "max"],
    labels=labels,
    caption="Descriptive statistics",
)

t2 = pf.etable(
    [fit1, fit2, fit3, fit4, fit5, fit6],
    labels=labels,
    show_se=False,
    felabels={"f1": "Industry Fixed Effects", "f2": "Year Fixed Effects"},
    caption="Regression results",
)
```


```{python}
display(t1.make(type="gt", gt_style=style_print))
display(t2.tab_options(**style_print))
```


```{python}
style_printDouble = {
    "table_font_size": "12px",
    "heading_title_font_size": "12px",
    "source_notes_font_size": "8px",
    "data_row_padding": "3px",
    "column_labels_padding": "3px",
    "table_body_border_bottom_style": "double",
    "column_labels_border_top_style": "double",
    "column_labels_border_bottom_width": "0.5px",
    "row_group_border_top_style": "hidden",
    "table_body_border_top_style": "None",
    "table_width": "14cm",
}
display(t1.make(type="gt", gt_style=style_printDouble))
display(t2.tab_options(**style_printDouble))
```
