[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Learning Econometrics with PyFixest",
    "section": "",
    "text": "A few classes and textbooks have adopted PyFixest for teaching. And for core textbooks in causal inference that have not (yet =) ) , we have (or are planning to) write PyFixest translations (there are also multiple current “good first issues” for this if you’d like to help us)."
  },
  {
    "objectID": "resources.html#textbooks",
    "href": "resources.html#textbooks",
    "title": "Learning Econometrics with PyFixest",
    "section": "Textbooks",
    "text": "Textbooks\n\nData Analysis for Business, Economics, and Policy (“Gabor’s Data Analysis”). Data and code and Jupyter Notebooks on github\nCoding for Economists: Regression\nTidy Finance comes with a chapter on fixed effects and clustered standard errors and Differene-in-Differences estimation that uses pyfixest\nThe Effect (first edition): our PyFixest translation\nThe Panel Data Chapter in the Mixtape. You can find a PyFixest translation here\n\nTextbooks / textbook chapters that we still want to cover:\n\nThe Difference-in-Differences chapter in the Mixtape (github issue here)\nAll of the Python translation of Ding’s textbook on causal inference (github issue here)\nThe “Brave and True” chapters on Dummy Regression, Instrumental Variables, Difference-in-Differences and Panel Data and Fixed Effects."
  },
  {
    "objectID": "resources.html#classes",
    "href": "resources.html#classes",
    "title": "Learning Econometrics with PyFixest",
    "section": "Classes",
    "text": "Classes\nIf you are teaching with pyfixest, we’d love to hear from you!\n\nEconometrics II (taught by Vladislav Morozov at UBonn): Great intro to fixed effects estimation theory. Slides on fixed effects here, full class notes here, github repository\nEmpirical Economics (taught at University of Utrecht 2025-2026) - MSc class in empirical economics.\nECON 526 - MA-level course in quantitative economics, data science, and causal inference in economics, taught at the University of Brisith Columbia. Class notes here"
  },
  {
    "objectID": "resources.html#blog-posts-notebooks-videos",
    "href": "resources.html#blog-posts-notebooks-videos",
    "title": "Learning Econometrics with PyFixest",
    "section": "Blog Posts, Notebooks, Videos",
    "text": "Blog Posts, Notebooks, Videos\nIf you’ve written a blog post that illustrates how to use pyfixest, please let us know, we’d love to link to it.\n\nPyData Berlin Presentation (2024) on PyFixest: link"
  },
  {
    "objectID": "regression_decomposition.html",
    "href": "regression_decomposition.html",
    "title": "How much do Covariates Matter?",
    "section": "",
    "text": "In regression analyses, we often wonder about “how much covariates matter?” for explaining the relationship between a target variable \\(D\\) and an outcome variable \\(Y\\).\nFor example, we might start analysing the gender wage gap with a simple regression model as log(wage) on gender. But arguably, men and women differ in many socio-economic characteristics: they might have different (average) levels of education or career experience, and they might work in different industries and select into different higher- or lower-paying industries. So which fraction of the gender wage gap can be explained by these observable characteristics?\nIn this notebook, we will compute and decompose the gender wage gab based on a subset of the PSID data set using a method commonly known as the “Gelbach Decomposition” (Gelbach, JoLE 2016).\nWe start with loading a subset of the PSID data provided by the AER R package.\n\nimport re\n\nimport pandas as pd\n\nimport pyfixest as pf\n\npsid = pd.read_csv(\n    \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/refs/heads/master/csv/AER/PSID7682.csv\"\n)\npsid[\"experience\"] = pd.Categorical(psid[\"experience\"])\npsid[\"year\"] = pd.Categorical(psid[\"year\"])\npsid.head()\n\n\n\n\n\n\n\n\nrownames\nexperience\nweeks\noccupation\nindustry\nsouth\nsmsa\nmarried\ngender\nunion\neducation\nethnicity\nwage\nyear\nid\n\n\n\n\n0\n1\n3\n32\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n260\n1976\n1\n\n\n1\n2\n4\n43\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n305\n1977\n1\n\n\n2\n3\n5\n40\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n402\n1978\n1\n\n\n3\n4\n6\n39\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n402\n1979\n1\n\n\n4\n5\n7\n42\nwhite\nyes\nyes\nno\nyes\nmale\nno\n9\nother\n429\n1980\n1\n\n\n\n\n\n\n\nComputing a first correlation between gender and wage, we find that males earn on average 0.474 log points more than women.\n\nfit_base = pf.feols(\"log(wage) ~ gender\", data=psid, vcov=\"hetero\")\nfit_base.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: log(wage), Fixed effects: 0\nInference:  hetero\nObservations:  4165\n\n| Coefficient    |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:---------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept      |      6.255 |        0.020 |   320.714 |      0.000 |  6.217 |   6.294 |\n| gender[T.male] |      0.474 |        0.021 |    22.818 |      0.000 |  0.434 |   0.515 |\n---\nRMSE: 0.436 R2: 0.106 \n\n\nTo examine the impact of observable on the relationship between wage and gender, a common strategy in applied research is to incrementally add a set of covariates to the baseline regression model of log(wage) on gender. Here, we will incrementally add the following covariates:\n\neducation,\nexperience\noccupation,\nindustry\nyear\nethnicity\n\nWe can do so by using multiple estimation syntax:\n\nfit_stepwise1 = pf.feols(\n    \"log(wage) ~ gender + csw0(education, experience, occupation, industry, year, ethnicity)\",\n    data=psid,\n)\npf.etable(fit_stepwise1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.474***\n(0.019)\n0.425***\n(0.018)\n0.444***\n(0.018)\n0.428***\n(0.018)\n0.432***\n(0.016)\n0.410***\n(0.017)\n\n\neducation\n\n0.065***\n(0.002)\n0.075***\n(0.002)\n0.061***\n(0.003)\n0.063***\n(0.003)\n0.060***\n(0.002)\n0.059***\n(0.002)\n\n\nexperience[T.2]\n\n\n0.147\n(0.156)\n0.156\n(0.155)\n0.158\n(0.154)\n0.124\n(0.137)\n0.128\n(0.136)\n\n\nexperience[T.3]\n\n\n0.273\n(0.139)\n0.284*\n(0.138)\n0.278*\n(0.138)\n0.232\n(0.122)\n0.241*\n(0.122)\n\n\nexperience[T.4]\n\n\n0.377**\n(0.137)\n0.389**\n(0.136)\n0.385**\n(0.135)\n0.287*\n(0.121)\n0.298*\n(0.120)\n\n\nexperience[T.5]\n\n\n0.452***\n(0.135)\n0.461***\n(0.134)\n0.456***\n(0.133)\n0.311**\n(0.119)\n0.323**\n(0.118)\n\n\nexperience[T.6]\n\n\n0.483***\n(0.134)\n0.495***\n(0.133)\n0.491***\n(0.132)\n0.300*\n(0.118)\n0.312**\n(0.118)\n\n\nexperience[T.7]\n\n\n0.591***\n(0.133)\n0.604***\n(0.132)\n0.599***\n(0.132)\n0.374**\n(0.118)\n0.386***\n(0.117)\n\n\nexperience[T.8]\n\n\n0.624***\n(0.133)\n0.641***\n(0.132)\n0.637***\n(0.131)\n0.387***\n(0.117)\n0.400***\n(0.117)\n\n\nexperience[T.9]\n\n\n0.667***\n(0.133)\n0.683***\n(0.132)\n0.679***\n(0.131)\n0.386**\n(0.117)\n0.399***\n(0.117)\n\n\nexperience[T.10]\n\n\n0.632***\n(0.133)\n0.646***\n(0.132)\n0.641***\n(0.131)\n0.380**\n(0.117)\n0.394***\n(0.117)\n\n\nexperience[T.11]\n\n\n0.673***\n(0.133)\n0.688***\n(0.132)\n0.681***\n(0.131)\n0.400***\n(0.117)\n0.415***\n(0.117)\n\n\nexperience[T.12]\n\n\n0.689***\n(0.133)\n0.703***\n(0.132)\n0.697***\n(0.131)\n0.412***\n(0.117)\n0.428***\n(0.117)\n\n\nexperience[T.13]\n\n\n0.750***\n(0.133)\n0.760***\n(0.132)\n0.753***\n(0.131)\n0.453***\n(0.118)\n0.470***\n(0.117)\n\n\nexperience[T.14]\n\n\n0.744***\n(0.133)\n0.753***\n(0.132)\n0.745***\n(0.132)\n0.452***\n(0.118)\n0.467***\n(0.117)\n\n\nexperience[T.15]\n\n\n0.770***\n(0.133)\n0.780***\n(0.132)\n0.771***\n(0.132)\n0.483***\n(0.118)\n0.497***\n(0.117)\n\n\nexperience[T.16]\n\n\n0.808***\n(0.134)\n0.818***\n(0.132)\n0.809***\n(0.132)\n0.498***\n(0.118)\n0.514***\n(0.117)\n\n\nexperience[T.17]\n\n\n0.803***\n(0.134)\n0.807***\n(0.133)\n0.799***\n(0.133)\n0.500***\n(0.119)\n0.517***\n(0.118)\n\n\nexperience[T.18]\n\n\n0.829***\n(0.134)\n0.833***\n(0.133)\n0.825***\n(0.133)\n0.516***\n(0.119)\n0.530***\n(0.118)\n\n\nexperience[T.19]\n\n\n0.866***\n(0.135)\n0.872***\n(0.134)\n0.860***\n(0.133)\n0.550***\n(0.119)\n0.563***\n(0.119)\n\n\nexperience[T.20]\n\n\n0.820***\n(0.135)\n0.826***\n(0.134)\n0.813***\n(0.134)\n0.514***\n(0.119)\n0.530***\n(0.119)\n\n\nexperience[T.21]\n\n\n0.828***\n(0.135)\n0.836***\n(0.134)\n0.828***\n(0.134)\n0.545***\n(0.120)\n0.564***\n(0.119)\n\n\nexperience[T.22]\n\n\n0.834***\n(0.136)\n0.836***\n(0.135)\n0.827***\n(0.134)\n0.556***\n(0.120)\n0.574***\n(0.119)\n\n\nexperience[T.23]\n\n\n0.823***\n(0.136)\n0.825***\n(0.135)\n0.815***\n(0.134)\n0.549***\n(0.120)\n0.567***\n(0.119)\n\n\nexperience[T.24]\n\n\n0.834***\n(0.136)\n0.839***\n(0.135)\n0.828***\n(0.134)\n0.552***\n(0.120)\n0.569***\n(0.119)\n\n\nexperience[T.25]\n\n\n0.876***\n(0.135)\n0.884***\n(0.134)\n0.874***\n(0.134)\n0.616***\n(0.120)\n0.633***\n(0.119)\n\n\nexperience[T.26]\n\n\n0.898***\n(0.135)\n0.911***\n(0.134)\n0.899***\n(0.134)\n0.620***\n(0.120)\n0.636***\n(0.119)\n\n\nexperience[T.27]\n\n\n0.901***\n(0.135)\n0.909***\n(0.134)\n0.897***\n(0.134)\n0.631***\n(0.120)\n0.644***\n(0.119)\n\n\nexperience[T.28]\n\n\n0.867***\n(0.135)\n0.874***\n(0.134)\n0.860***\n(0.134)\n0.601***\n(0.120)\n0.613***\n(0.119)\n\n\nexperience[T.29]\n\n\n0.897***\n(0.135)\n0.902***\n(0.134)\n0.888***\n(0.134)\n0.615***\n(0.120)\n0.629***\n(0.119)\n\n\nexperience[T.30]\n\n\n0.878***\n(0.135)\n0.886***\n(0.134)\n0.871***\n(0.133)\n0.609***\n(0.119)\n0.622***\n(0.119)\n\n\nexperience[T.31]\n\n\n0.928***\n(0.135)\n0.926***\n(0.134)\n0.911***\n(0.133)\n0.644***\n(0.119)\n0.658***\n(0.118)\n\n\nexperience[T.32]\n\n\n0.936***\n(0.135)\n0.935***\n(0.134)\n0.921***\n(0.134)\n0.648***\n(0.120)\n0.660***\n(0.119)\n\n\nexperience[T.33]\n\n\n0.970***\n(0.135)\n0.962***\n(0.134)\n0.947***\n(0.134)\n0.642***\n(0.120)\n0.655***\n(0.119)\n\n\nexperience[T.34]\n\n\n0.942***\n(0.136)\n0.938***\n(0.134)\n0.926***\n(0.134)\n0.629***\n(0.120)\n0.643***\n(0.119)\n\n\nexperience[T.35]\n\n\n0.960***\n(0.136)\n0.953***\n(0.135)\n0.941***\n(0.134)\n0.642***\n(0.120)\n0.657***\n(0.119)\n\n\nexperience[T.36]\n\n\n1.004***\n(0.136)\n1.003***\n(0.135)\n0.991***\n(0.135)\n0.663***\n(0.120)\n0.678***\n(0.120)\n\n\nexperience[T.37]\n\n\n0.951***\n(0.137)\n0.947***\n(0.136)\n0.937***\n(0.136)\n0.623***\n(0.121)\n0.639***\n(0.121)\n\n\nexperience[T.38]\n\n\n0.895***\n(0.139)\n0.900***\n(0.138)\n0.892***\n(0.137)\n0.574***\n(0.123)\n0.591***\n(0.122)\n\n\nexperience[T.39]\n\n\n0.884***\n(0.140)\n0.892***\n(0.139)\n0.880***\n(0.138)\n0.530***\n(0.124)\n0.552***\n(0.123)\n\n\nexperience[T.40]\n\n\n0.815***\n(0.140)\n0.825***\n(0.139)\n0.812***\n(0.139)\n0.461***\n(0.124)\n0.483***\n(0.123)\n\n\nexperience[T.41]\n\n\n0.775***\n(0.144)\n0.783***\n(0.143)\n0.761***\n(0.142)\n0.397**\n(0.127)\n0.421***\n(0.127)\n\n\nexperience[T.42]\n\n\n0.827***\n(0.149)\n0.841***\n(0.148)\n0.817***\n(0.148)\n0.429**\n(0.132)\n0.455***\n(0.131)\n\n\nexperience[T.43]\n\n\n0.844***\n(0.155)\n0.871***\n(0.153)\n0.850***\n(0.153)\n0.424**\n(0.137)\n0.451***\n(0.136)\n\n\nexperience[T.44]\n\n\n0.756***\n(0.165)\n0.781***\n(0.164)\n0.758***\n(0.163)\n0.394**\n(0.146)\n0.427**\n(0.145)\n\n\nexperience[T.45]\n\n\n0.733***\n(0.171)\n0.750***\n(0.169)\n0.739***\n(0.169)\n0.383*\n(0.151)\n0.407**\n(0.150)\n\n\nexperience[T.46]\n\n\n0.751***\n(0.178)\n0.768***\n(0.177)\n0.766***\n(0.176)\n0.358*\n(0.158)\n0.373*\n(0.157)\n\n\nexperience[T.47]\n\n\n0.435\n(0.249)\n0.406\n(0.247)\n0.427\n(0.246)\n0.159\n(0.219)\n0.197\n(0.218)\n\n\nexperience[T.48]\n\n\n0.675**\n(0.249)\n0.647**\n(0.247)\n0.668**\n(0.246)\n0.317\n(0.219)\n0.355\n(0.218)\n\n\nexperience[T.49]\n\n\n1.007***\n(0.249)\n0.979***\n(0.247)\n1.000***\n(0.246)\n0.575**\n(0.219)\n0.614**\n(0.218)\n\n\nexperience[T.50]\n\n\n0.920***\n(0.249)\n0.892***\n(0.247)\n0.913***\n(0.246)\n0.408\n(0.219)\n0.448*\n(0.218)\n\n\nexperience[T.51]\n\n\n0.390\n(0.389)\n0.387\n(0.386)\n0.417\n(0.385)\n-0.121\n(0.342)\n-0.120\n(0.341)\n\n\noccupation[T.white]\n\n\n\n0.125***\n(0.015)\n0.132***\n(0.015)\n0.127***\n(0.013)\n0.123***\n(0.013)\n\n\nindustry[T.yes]\n\n\n\n\n0.064***\n(0.012)\n0.070***\n(0.011)\n0.066***\n(0.011)\n\n\nyear[T.1977]\n\n\n\n\n\n0.073***\n(0.019)\n0.072***\n(0.019)\n\n\nyear[T.1978]\n\n\n\n\n\n0.193***\n(0.019)\n0.192***\n(0.019)\n\n\nyear[T.1979]\n\n\n\n\n\n0.284***\n(0.019)\n0.282***\n(0.019)\n\n\nyear[T.1980]\n\n\n\n\n\n0.363***\n(0.019)\n0.361***\n(0.019)\n\n\nyear[T.1981]\n\n\n\n\n\n0.434***\n(0.019)\n0.432***\n(0.019)\n\n\nyear[T.1982]\n\n\n\n\n\n0.518***\n(0.019)\n0.516***\n(0.019)\n\n\nethnicity[T.other]\n\n\n\n\n\n\n0.133***\n(0.020)\n\n\nIntercept\n6.255***\n(0.020)\n5.419***\n(0.034)\n4.566***\n(0.134)\n4.664***\n(0.133)\n4.636***\n(0.133)\n4.672***\n(0.118)\n4.570***\n(0.118)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.260\n0.376\n0.387\n0.391\n0.520\n0.525\n\n\nAdj. R2\n0.105\n0.260\n0.368\n0.379\n0.383\n0.513\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nBecause the table is so long that it’s hard to see anything, we restrict it to display only a few variables:\n\npf.etable(fit_stepwise1, keep=[\"gender\", \"ethnicity\", \"education\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.474***\n(0.019)\n0.425***\n(0.018)\n0.444***\n(0.018)\n0.428***\n(0.018)\n0.432***\n(0.016)\n0.410***\n(0.017)\n\n\nethnicity[T.other]\n\n\n\n\n\n\n0.133***\n(0.020)\n\n\neducation\n\n0.065***\n(0.002)\n0.075***\n(0.002)\n0.061***\n(0.003)\n0.063***\n(0.003)\n0.060***\n(0.002)\n0.059***\n(0.002)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.260\n0.376\n0.387\n0.391\n0.520\n0.525\n\n\nAdj. R2\n0.105\n0.260\n0.368\n0.379\n0.383\n0.513\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe see that the coefficient on gender is roughly the same in all models. Tentatively, we might already conclude that the observable characteristics in the data do not explain a large part of the gender wage gap.\nBut how much do differences in education matter? We have computed 6 additional models that contain education as a covariate. The obtained point estimates vary between \\(0.059\\) and \\(0.075\\). Which of these numbers should we report?\nAdditionally, note that while we have only computed 6 additional models with covariates, the number of possible models is much larger. If I did the math correctly, simply by additively and incrementally adding covariates, we could have computed \\(57\\) different models (not all of which would have included education as a control).\nAs it turns out, different models will lead to different point estimates. The order of incrementally adding covariates might impact our conclusion. To illustrate this, we keep the same ordering as before, but start with ethnicity as our first variable:\n\nfit_stepwise2 = pf.feols(\n    \"log(wage) ~ gender + csw0(ethnicity, education, experience, occupation, industry, year)\",\n    data=psid,\n)\npf.etable(fit_stepwise2, keep=[\"gender\", \"ethnicity\", \"education\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.436***\n(0.022)\n0.450***\n(0.020)\n0.399***\n(0.019)\n0.418***\n(0.019)\n0.404***\n(0.019)\n0.410***\n(0.017)\n\n\nethnicity[T.other]\n\n0.227***\n(0.026)\n0.141***\n(0.024)\n0.158***\n(0.023)\n0.151***\n(0.022)\n0.146***\n(0.022)\n0.133***\n(0.020)\n\n\neducation\n\n\n0.064***\n(0.002)\n0.074***\n(0.002)\n0.060***\n(0.003)\n0.062***\n(0.003)\n0.059***\n(0.002)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.121\n0.266\n0.383\n0.394\n0.397\n0.525\n\n\nAdj. R2\n0.105\n0.121\n0.266\n0.375\n0.386\n0.389\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe obtain 5 new coefficients on education that vary between 0.074 and 0.059.\nSo, which share of the “raw” gender wage gap can be attributed to differences in education between men and women? Should we report a statisics based on the 0.075 estimate? Or on the 0.059 estimate? Which value should we pick?\nTo help us with this problem, Gelbach (2016, JoLE) develops a decomposition procedure building on the omitted variable bias formula that produces a single value for the contribution of a given covariate, say education, to the gender wage gap."
  },
  {
    "objectID": "regression_decomposition.html#motivation",
    "href": "regression_decomposition.html#motivation",
    "title": "How much do Covariates Matter?",
    "section": "",
    "text": "In regression analyses, we often wonder about “how much covariates matter?” for explaining the relationship between a target variable \\(D\\) and an outcome variable \\(Y\\).\nFor example, we might start analysing the gender wage gap with a simple regression model as log(wage) on gender. But arguably, men and women differ in many socio-economic characteristics: they might have different (average) levels of education or career experience, and they might work in different industries and select into different higher- or lower-paying industries. So which fraction of the gender wage gap can be explained by these observable characteristics?\nIn this notebook, we will compute and decompose the gender wage gab based on a subset of the PSID data set using a method commonly known as the “Gelbach Decomposition” (Gelbach, JoLE 2016).\nWe start with loading a subset of the PSID data provided by the AER R package.\n\nimport re\n\nimport pandas as pd\n\nimport pyfixest as pf\n\npsid = pd.read_csv(\n    \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/refs/heads/master/csv/AER/PSID7682.csv\"\n)\npsid[\"experience\"] = pd.Categorical(psid[\"experience\"])\npsid[\"year\"] = pd.Categorical(psid[\"year\"])\npsid.head()\n\n\n\n\n\n\n\n\nrownames\nexperience\nweeks\noccupation\nindustry\nsouth\nsmsa\nmarried\ngender\nunion\neducation\nethnicity\nwage\nyear\nid\n\n\n\n\n0\n1\n3\n32\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n260\n1976\n1\n\n\n1\n2\n4\n43\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n305\n1977\n1\n\n\n2\n3\n5\n40\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n402\n1978\n1\n\n\n3\n4\n6\n39\nwhite\nno\nyes\nno\nyes\nmale\nno\n9\nother\n402\n1979\n1\n\n\n4\n5\n7\n42\nwhite\nyes\nyes\nno\nyes\nmale\nno\n9\nother\n429\n1980\n1\n\n\n\n\n\n\n\nComputing a first correlation between gender and wage, we find that males earn on average 0.474 log points more than women.\n\nfit_base = pf.feols(\"log(wage) ~ gender\", data=psid, vcov=\"hetero\")\nfit_base.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: log(wage), Fixed effects: 0\nInference:  hetero\nObservations:  4165\n\n| Coefficient    |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:---------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept      |      6.255 |        0.020 |   320.714 |      0.000 |  6.217 |   6.294 |\n| gender[T.male] |      0.474 |        0.021 |    22.818 |      0.000 |  0.434 |   0.515 |\n---\nRMSE: 0.436 R2: 0.106 \n\n\nTo examine the impact of observable on the relationship between wage and gender, a common strategy in applied research is to incrementally add a set of covariates to the baseline regression model of log(wage) on gender. Here, we will incrementally add the following covariates:\n\neducation,\nexperience\noccupation,\nindustry\nyear\nethnicity\n\nWe can do so by using multiple estimation syntax:\n\nfit_stepwise1 = pf.feols(\n    \"log(wage) ~ gender + csw0(education, experience, occupation, industry, year, ethnicity)\",\n    data=psid,\n)\npf.etable(fit_stepwise1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.474***\n(0.019)\n0.425***\n(0.018)\n0.444***\n(0.018)\n0.428***\n(0.018)\n0.432***\n(0.016)\n0.410***\n(0.017)\n\n\neducation\n\n0.065***\n(0.002)\n0.075***\n(0.002)\n0.061***\n(0.003)\n0.063***\n(0.003)\n0.060***\n(0.002)\n0.059***\n(0.002)\n\n\nexperience[T.2]\n\n\n0.147\n(0.156)\n0.156\n(0.155)\n0.158\n(0.154)\n0.124\n(0.137)\n0.128\n(0.136)\n\n\nexperience[T.3]\n\n\n0.273\n(0.139)\n0.284*\n(0.138)\n0.278*\n(0.138)\n0.232\n(0.122)\n0.241*\n(0.122)\n\n\nexperience[T.4]\n\n\n0.377**\n(0.137)\n0.389**\n(0.136)\n0.385**\n(0.135)\n0.287*\n(0.121)\n0.298*\n(0.120)\n\n\nexperience[T.5]\n\n\n0.452***\n(0.135)\n0.461***\n(0.134)\n0.456***\n(0.133)\n0.311**\n(0.119)\n0.323**\n(0.118)\n\n\nexperience[T.6]\n\n\n0.483***\n(0.134)\n0.495***\n(0.133)\n0.491***\n(0.132)\n0.300*\n(0.118)\n0.312**\n(0.118)\n\n\nexperience[T.7]\n\n\n0.591***\n(0.133)\n0.604***\n(0.132)\n0.599***\n(0.132)\n0.374**\n(0.118)\n0.386***\n(0.117)\n\n\nexperience[T.8]\n\n\n0.624***\n(0.133)\n0.641***\n(0.132)\n0.637***\n(0.131)\n0.387***\n(0.117)\n0.400***\n(0.117)\n\n\nexperience[T.9]\n\n\n0.667***\n(0.133)\n0.683***\n(0.132)\n0.679***\n(0.131)\n0.386**\n(0.117)\n0.399***\n(0.117)\n\n\nexperience[T.10]\n\n\n0.632***\n(0.133)\n0.646***\n(0.132)\n0.641***\n(0.131)\n0.380**\n(0.117)\n0.394***\n(0.117)\n\n\nexperience[T.11]\n\n\n0.673***\n(0.133)\n0.688***\n(0.132)\n0.681***\n(0.131)\n0.400***\n(0.117)\n0.415***\n(0.117)\n\n\nexperience[T.12]\n\n\n0.689***\n(0.133)\n0.703***\n(0.132)\n0.697***\n(0.131)\n0.412***\n(0.117)\n0.428***\n(0.117)\n\n\nexperience[T.13]\n\n\n0.750***\n(0.133)\n0.760***\n(0.132)\n0.753***\n(0.131)\n0.453***\n(0.118)\n0.470***\n(0.117)\n\n\nexperience[T.14]\n\n\n0.744***\n(0.133)\n0.753***\n(0.132)\n0.745***\n(0.132)\n0.452***\n(0.118)\n0.467***\n(0.117)\n\n\nexperience[T.15]\n\n\n0.770***\n(0.133)\n0.780***\n(0.132)\n0.771***\n(0.132)\n0.483***\n(0.118)\n0.497***\n(0.117)\n\n\nexperience[T.16]\n\n\n0.808***\n(0.134)\n0.818***\n(0.132)\n0.809***\n(0.132)\n0.498***\n(0.118)\n0.514***\n(0.117)\n\n\nexperience[T.17]\n\n\n0.803***\n(0.134)\n0.807***\n(0.133)\n0.799***\n(0.133)\n0.500***\n(0.119)\n0.517***\n(0.118)\n\n\nexperience[T.18]\n\n\n0.829***\n(0.134)\n0.833***\n(0.133)\n0.825***\n(0.133)\n0.516***\n(0.119)\n0.530***\n(0.118)\n\n\nexperience[T.19]\n\n\n0.866***\n(0.135)\n0.872***\n(0.134)\n0.860***\n(0.133)\n0.550***\n(0.119)\n0.563***\n(0.119)\n\n\nexperience[T.20]\n\n\n0.820***\n(0.135)\n0.826***\n(0.134)\n0.813***\n(0.134)\n0.514***\n(0.119)\n0.530***\n(0.119)\n\n\nexperience[T.21]\n\n\n0.828***\n(0.135)\n0.836***\n(0.134)\n0.828***\n(0.134)\n0.545***\n(0.120)\n0.564***\n(0.119)\n\n\nexperience[T.22]\n\n\n0.834***\n(0.136)\n0.836***\n(0.135)\n0.827***\n(0.134)\n0.556***\n(0.120)\n0.574***\n(0.119)\n\n\nexperience[T.23]\n\n\n0.823***\n(0.136)\n0.825***\n(0.135)\n0.815***\n(0.134)\n0.549***\n(0.120)\n0.567***\n(0.119)\n\n\nexperience[T.24]\n\n\n0.834***\n(0.136)\n0.839***\n(0.135)\n0.828***\n(0.134)\n0.552***\n(0.120)\n0.569***\n(0.119)\n\n\nexperience[T.25]\n\n\n0.876***\n(0.135)\n0.884***\n(0.134)\n0.874***\n(0.134)\n0.616***\n(0.120)\n0.633***\n(0.119)\n\n\nexperience[T.26]\n\n\n0.898***\n(0.135)\n0.911***\n(0.134)\n0.899***\n(0.134)\n0.620***\n(0.120)\n0.636***\n(0.119)\n\n\nexperience[T.27]\n\n\n0.901***\n(0.135)\n0.909***\n(0.134)\n0.897***\n(0.134)\n0.631***\n(0.120)\n0.644***\n(0.119)\n\n\nexperience[T.28]\n\n\n0.867***\n(0.135)\n0.874***\n(0.134)\n0.860***\n(0.134)\n0.601***\n(0.120)\n0.613***\n(0.119)\n\n\nexperience[T.29]\n\n\n0.897***\n(0.135)\n0.902***\n(0.134)\n0.888***\n(0.134)\n0.615***\n(0.120)\n0.629***\n(0.119)\n\n\nexperience[T.30]\n\n\n0.878***\n(0.135)\n0.886***\n(0.134)\n0.871***\n(0.133)\n0.609***\n(0.119)\n0.622***\n(0.119)\n\n\nexperience[T.31]\n\n\n0.928***\n(0.135)\n0.926***\n(0.134)\n0.911***\n(0.133)\n0.644***\n(0.119)\n0.658***\n(0.118)\n\n\nexperience[T.32]\n\n\n0.936***\n(0.135)\n0.935***\n(0.134)\n0.921***\n(0.134)\n0.648***\n(0.120)\n0.660***\n(0.119)\n\n\nexperience[T.33]\n\n\n0.970***\n(0.135)\n0.962***\n(0.134)\n0.947***\n(0.134)\n0.642***\n(0.120)\n0.655***\n(0.119)\n\n\nexperience[T.34]\n\n\n0.942***\n(0.136)\n0.938***\n(0.134)\n0.926***\n(0.134)\n0.629***\n(0.120)\n0.643***\n(0.119)\n\n\nexperience[T.35]\n\n\n0.960***\n(0.136)\n0.953***\n(0.135)\n0.941***\n(0.134)\n0.642***\n(0.120)\n0.657***\n(0.119)\n\n\nexperience[T.36]\n\n\n1.004***\n(0.136)\n1.003***\n(0.135)\n0.991***\n(0.135)\n0.663***\n(0.120)\n0.678***\n(0.120)\n\n\nexperience[T.37]\n\n\n0.951***\n(0.137)\n0.947***\n(0.136)\n0.937***\n(0.136)\n0.623***\n(0.121)\n0.639***\n(0.121)\n\n\nexperience[T.38]\n\n\n0.895***\n(0.139)\n0.900***\n(0.138)\n0.892***\n(0.137)\n0.574***\n(0.123)\n0.591***\n(0.122)\n\n\nexperience[T.39]\n\n\n0.884***\n(0.140)\n0.892***\n(0.139)\n0.880***\n(0.138)\n0.530***\n(0.124)\n0.552***\n(0.123)\n\n\nexperience[T.40]\n\n\n0.815***\n(0.140)\n0.825***\n(0.139)\n0.812***\n(0.139)\n0.461***\n(0.124)\n0.483***\n(0.123)\n\n\nexperience[T.41]\n\n\n0.775***\n(0.144)\n0.783***\n(0.143)\n0.761***\n(0.142)\n0.397**\n(0.127)\n0.421***\n(0.127)\n\n\nexperience[T.42]\n\n\n0.827***\n(0.149)\n0.841***\n(0.148)\n0.817***\n(0.148)\n0.429**\n(0.132)\n0.455***\n(0.131)\n\n\nexperience[T.43]\n\n\n0.844***\n(0.155)\n0.871***\n(0.153)\n0.850***\n(0.153)\n0.424**\n(0.137)\n0.451***\n(0.136)\n\n\nexperience[T.44]\n\n\n0.756***\n(0.165)\n0.781***\n(0.164)\n0.758***\n(0.163)\n0.394**\n(0.146)\n0.427**\n(0.145)\n\n\nexperience[T.45]\n\n\n0.733***\n(0.171)\n0.750***\n(0.169)\n0.739***\n(0.169)\n0.383*\n(0.151)\n0.407**\n(0.150)\n\n\nexperience[T.46]\n\n\n0.751***\n(0.178)\n0.768***\n(0.177)\n0.766***\n(0.176)\n0.358*\n(0.158)\n0.373*\n(0.157)\n\n\nexperience[T.47]\n\n\n0.435\n(0.249)\n0.406\n(0.247)\n0.427\n(0.246)\n0.159\n(0.219)\n0.197\n(0.218)\n\n\nexperience[T.48]\n\n\n0.675**\n(0.249)\n0.647**\n(0.247)\n0.668**\n(0.246)\n0.317\n(0.219)\n0.355\n(0.218)\n\n\nexperience[T.49]\n\n\n1.007***\n(0.249)\n0.979***\n(0.247)\n1.000***\n(0.246)\n0.575**\n(0.219)\n0.614**\n(0.218)\n\n\nexperience[T.50]\n\n\n0.920***\n(0.249)\n0.892***\n(0.247)\n0.913***\n(0.246)\n0.408\n(0.219)\n0.448*\n(0.218)\n\n\nexperience[T.51]\n\n\n0.390\n(0.389)\n0.387\n(0.386)\n0.417\n(0.385)\n-0.121\n(0.342)\n-0.120\n(0.341)\n\n\noccupation[T.white]\n\n\n\n0.125***\n(0.015)\n0.132***\n(0.015)\n0.127***\n(0.013)\n0.123***\n(0.013)\n\n\nindustry[T.yes]\n\n\n\n\n0.064***\n(0.012)\n0.070***\n(0.011)\n0.066***\n(0.011)\n\n\nyear[T.1977]\n\n\n\n\n\n0.073***\n(0.019)\n0.072***\n(0.019)\n\n\nyear[T.1978]\n\n\n\n\n\n0.193***\n(0.019)\n0.192***\n(0.019)\n\n\nyear[T.1979]\n\n\n\n\n\n0.284***\n(0.019)\n0.282***\n(0.019)\n\n\nyear[T.1980]\n\n\n\n\n\n0.363***\n(0.019)\n0.361***\n(0.019)\n\n\nyear[T.1981]\n\n\n\n\n\n0.434***\n(0.019)\n0.432***\n(0.019)\n\n\nyear[T.1982]\n\n\n\n\n\n0.518***\n(0.019)\n0.516***\n(0.019)\n\n\nethnicity[T.other]\n\n\n\n\n\n\n0.133***\n(0.020)\n\n\nIntercept\n6.255***\n(0.020)\n5.419***\n(0.034)\n4.566***\n(0.134)\n4.664***\n(0.133)\n4.636***\n(0.133)\n4.672***\n(0.118)\n4.570***\n(0.118)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.260\n0.376\n0.387\n0.391\n0.520\n0.525\n\n\nAdj. R2\n0.105\n0.260\n0.368\n0.379\n0.383\n0.513\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nBecause the table is so long that it’s hard to see anything, we restrict it to display only a few variables:\n\npf.etable(fit_stepwise1, keep=[\"gender\", \"ethnicity\", \"education\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.474***\n(0.019)\n0.425***\n(0.018)\n0.444***\n(0.018)\n0.428***\n(0.018)\n0.432***\n(0.016)\n0.410***\n(0.017)\n\n\nethnicity[T.other]\n\n\n\n\n\n\n0.133***\n(0.020)\n\n\neducation\n\n0.065***\n(0.002)\n0.075***\n(0.002)\n0.061***\n(0.003)\n0.063***\n(0.003)\n0.060***\n(0.002)\n0.059***\n(0.002)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.260\n0.376\n0.387\n0.391\n0.520\n0.525\n\n\nAdj. R2\n0.105\n0.260\n0.368\n0.379\n0.383\n0.513\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe see that the coefficient on gender is roughly the same in all models. Tentatively, we might already conclude that the observable characteristics in the data do not explain a large part of the gender wage gap.\nBut how much do differences in education matter? We have computed 6 additional models that contain education as a covariate. The obtained point estimates vary between \\(0.059\\) and \\(0.075\\). Which of these numbers should we report?\nAdditionally, note that while we have only computed 6 additional models with covariates, the number of possible models is much larger. If I did the math correctly, simply by additively and incrementally adding covariates, we could have computed \\(57\\) different models (not all of which would have included education as a control).\nAs it turns out, different models will lead to different point estimates. The order of incrementally adding covariates might impact our conclusion. To illustrate this, we keep the same ordering as before, but start with ethnicity as our first variable:\n\nfit_stepwise2 = pf.feols(\n    \"log(wage) ~ gender + csw0(ethnicity, education, experience, occupation, industry, year)\",\n    data=psid,\n)\npf.etable(fit_stepwise2, keep=[\"gender\", \"ethnicity\", \"education\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlog(wage)\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n\n\n\n\ncoef\n\n\ngender[T.male]\n0.474***\n(0.021)\n0.436***\n(0.022)\n0.450***\n(0.020)\n0.399***\n(0.019)\n0.418***\n(0.019)\n0.404***\n(0.019)\n0.410***\n(0.017)\n\n\nethnicity[T.other]\n\n0.227***\n(0.026)\n0.141***\n(0.024)\n0.158***\n(0.023)\n0.151***\n(0.022)\n0.146***\n(0.022)\n0.133***\n(0.020)\n\n\neducation\n\n\n0.064***\n(0.002)\n0.074***\n(0.002)\n0.060***\n(0.003)\n0.062***\n(0.003)\n0.059***\n(0.002)\n\n\nstats\n\n\nObservations\n4165\n4165\n4165\n4165\n4165\n4165\n4165\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.106\n0.121\n0.266\n0.383\n0.394\n0.397\n0.525\n\n\nAdj. R2\n0.105\n0.121\n0.266\n0.375\n0.386\n0.389\n0.518\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe obtain 5 new coefficients on education that vary between 0.074 and 0.059.\nSo, which share of the “raw” gender wage gap can be attributed to differences in education between men and women? Should we report a statisics based on the 0.075 estimate? Or on the 0.059 estimate? Which value should we pick?\nTo help us with this problem, Gelbach (2016, JoLE) develops a decomposition procedure building on the omitted variable bias formula that produces a single value for the contribution of a given covariate, say education, to the gender wage gap."
  },
  {
    "objectID": "regression_decomposition.html#notation-and-gelbachs-algorithm",
    "href": "regression_decomposition.html#notation-and-gelbachs-algorithm",
    "title": "How much do Covariates Matter?",
    "section": "Notation and Gelbach’s Algorithm",
    "text": "Notation and Gelbach’s Algorithm\nBefore we dive into a code example, let us first introduce the notation and Gelbach’s algorithm. We are interested in “decomposing” the effect of a variable \\(X_{1} \\in \\mathbb{R}\\) on an outcome \\(Y \\in \\mathbb{R}\\) into a part explained by covariates \\(X_{2} \\in \\mathbb{R}^{k_{2}}\\) and an unexplained part.\nThus we can specify two regression models:\n\nThe short model \\[\n      Y = X_{1} \\beta_{1} + u_{1}\n  \\]\nthe long (or full) model\n\\[\n      Y = X_{1} \\beta_{1} + X_{2} \\beta_{2} + e\n  \\]\n\nBy fitting the short regression, we obtain an estimate \\(\\hat{\\beta}_{1}\\), which we will denote as the direct effect, and by estimating the long regression, we obtain an estimate of the regression coefficients \\(\\hat{\\beta}_{2} \\in \\mathbb{R}^{k_2}\\). We will denote the estimate on \\(X_1\\) in the long regression as the full effect.\nWe can then compute the contribution of an individual covariate \\(\\hat{\\delta}_{k}\\) via the following algorithm:\n\nStep 1: we compute coefficients from \\(k_{2}\\) auxiliary regression models \\(\\hat{\\Gamma}\\) as \\[\n      \\hat{\\Gamma} = (X_{1}'X_{1})^{-1} X_{1}'X_{2}\n  \\]\nIn words, we regress the target variable \\(X_{1}\\) on each covariate in \\(X_{2}\\). In practice, we can easily do this in one line of code via scipy.linalg.lstsq().\nStep 2: We can compute the total effect explained by the covariates, which we denote by \\(\\delta\\), as\n\\[\n      \\hat{\\delta} = \\sum_{k=1}^{k_2} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n  \\]\nwhere \\(\\hat{\\Gamma}_{k}\\) are the coefficients from an auxiliary regression \\(X_1\\) on covariate \\(X_{2,k}\\) and \\(\\hat{\\beta}_{2,k}\\) is the associated estimate on \\(X_{2,k}\\) from the full model.\nThe individual contribution of covariate \\(k\\) is then defined as\n\\[\n      \\hat{\\delta}_{k} = \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}.\n  \\]\n\nAfter having obtained \\(\\delta_{k}\\) for each auxiliary variable \\(k\\), we can easily aggregate multiple variables into a single groups of interest. For example, if \\(X_{2}\\) contains a set of dummies from industry fixed effects, we could compute the explained part of “industry” by summing over all the dummies:\n\\[\n        \\hat{\\delta}_{\\textit{industry}} = \\sum_{k \\in \\textit{industry dummies}} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n\\]"
  },
  {
    "objectID": "regression_decomposition.html#pyfixest-example",
    "href": "regression_decomposition.html#pyfixest-example",
    "title": "How much do Covariates Matter?",
    "section": "PyFixest Example",
    "text": "PyFixest Example\nTo employ Gelbach’s decomposition in pyfixest, we start with the full regression model that contains all variables of interest:\n\nfit_full = pf.feols(\n    \"log(wage) ~ gender + ethnicity + education + experience + occupation + industry +year\",\n    data=psid,\n)\n\nAfter fitting the full model, we can run the decomposition procedure by calling the decompose() method. The only required argument is to specify the focal variable decomp_var, which in this case is “gender”. Inference is conducted via a non-parametric bootstrap and can optionally be turned off.\n\ngb = fit_full.decompose(decomp_var=\"gender[T.male]\", digits=5)\n\n100%|██████████| 1000/1000 [00:58&lt;00:00, 17.00it/s]\n\n\nAs before, this produces a pretty big output table that reports - the direct effect of the regression of log(wage) ~ gender - the full effect of gender on log wage using the full regression with all control variables - the explained effect as the difference between the full and direct effect - a single scalar value for the individual contributions of a covariate to overall explained effect\nFor our example at hand, the additional covariates only explain a tiny fraction of the differences in log wages between men and women - 0.064 points. Of these, around one third can be attributed to ethnicity, 0.00064 to years of eduaction, etc.\nNote that for now, we ask etable() to only report effects in levels. By switching to panels = \"all\", we would also report normalized coefficient; but we decided not to do so here as otherwise the table would have turned out even longer than it already has.\n\ngb.etable(\n    panels=\"levels\",\n)\n\n\n\n\n\n\n\n\nInitial Difference\nAdjusted Difference\nExplained Difference\n\n\n\n\nLevels (units)\n\n\ngender[T.male]\n0.474\n0.410\n0.064\n\n\n\n[0.469, 0.469]\n[0.409, 0.409]\n[0.060, 0.060]\n\n\nethnicity[T.other]\n-\n-\n0.023\n\n\n\n-\n-\n[0.021, 0.021]\n\n\neducation\n-\n-\n0.001\n\n\n\n-\n-\n[0.007, 0.007]\n\n\nexperience[T.2]\n-\n-\n-0.000\n\n\n\n-\n-\n[0.000, 0.000]\n\n\nexperience[T.3]\n-\n-\n-0.002\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.4]\n-\n-\n-0.004\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.5]\n-\n-\n-0.003\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.6]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.005, -0.005]\n\n\nexperience[T.7]\n-\n-\n0.000\n\n\n\n-\n-\n[0.000, 0.000]\n\n\nexperience[T.8]\n-\n-\n0.002\n\n\n\n-\n-\n[0.004, 0.004]\n\n\nexperience[T.9]\n-\n-\n-0.003\n\n\n\n-\n-\n[-0.008, -0.008]\n\n\nexperience[T.10]\n-\n-\n-0.004\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.11]\n-\n-\n-0.002\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.12]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.13]\n-\n-\n-0.006\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.14]\n-\n-\n-0.010\n\n\n\n-\n-\n[-0.010, -0.010]\n\n\nexperience[T.15]\n-\n-\n-0.009\n\n\n\n-\n-\n[-0.009, -0.009]\n\n\nexperience[T.16]\n-\n-\n-0.006\n\n\n\n-\n-\n[-0.004, -0.004]\n\n\nexperience[T.17]\n-\n-\n-0.000\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.18]\n-\n-\n-0.002\n\n\n\n-\n-\n[0.004, 0.004]\n\n\nexperience[T.19]\n-\n-\n-0.006\n\n\n\n-\n-\n[-0.002, -0.002]\n\n\nexperience[T.20]\n-\n-\n-0.008\n\n\n\n-\n-\n[-0.010, -0.010]\n\n\nexperience[T.21]\n-\n-\n-0.004\n\n\n\n-\n-\n[-0.006, -0.006]\n\n\nexperience[T.22]\n-\n-\n-0.006\n\n\n\n-\n-\n[-0.004, -0.004]\n\n\nexperience[T.23]\n-\n-\n-0.002\n\n\n\n-\n-\n[-0.010, -0.010]\n\n\nexperience[T.24]\n-\n-\n-0.003\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.25]\n-\n-\n-0.001\n\n\n\n-\n-\n[0.002, 0.002]\n\n\nexperience[T.26]\n-\n-\n0.002\n\n\n\n-\n-\n[0.009, 0.009]\n\n\nexperience[T.27]\n-\n-\n0.010\n\n\n\n-\n-\n[0.011, 0.011]\n\n\nexperience[T.28]\n-\n-\n0.009\n\n\n\n-\n-\n[-0.000, -0.000]\n\n\nexperience[T.29]\n-\n-\n0.013\n\n\n\n-\n-\n[0.013, 0.013]\n\n\nexperience[T.30]\n-\n-\n0.012\n\n\n\n-\n-\n[0.011, 0.011]\n\n\nexperience[T.31]\n-\n-\n0.014\n\n\n\n-\n-\n[0.012, 0.012]\n\n\nexperience[T.32]\n-\n-\n0.012\n\n\n\n-\n-\n[0.005, 0.005]\n\n\nexperience[T.33]\n-\n-\n0.012\n\n\n\n-\n-\n[0.011, 0.011]\n\n\nexperience[T.34]\n-\n-\n0.011\n\n\n\n-\n-\n[0.005, 0.005]\n\n\nexperience[T.35]\n-\n-\n0.009\n\n\n\n-\n-\n[0.008, 0.008]\n\n\nexperience[T.36]\n-\n-\n0.007\n\n\n\n-\n-\n[0.011, 0.011]\n\n\nexperience[T.37]\n-\n-\n0.006\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.38]\n-\n-\n0.006\n\n\n\n-\n-\n[0.008, 0.008]\n\n\nexperience[T.39]\n-\n-\n0.003\n\n\n\n-\n-\n[0.009, 0.009]\n\n\nexperience[T.40]\n-\n-\n0.002\n\n\n\n-\n-\n[0.003, 0.003]\n\n\nexperience[T.41]\n-\n-\n-0.000\n\n\n\n-\n-\n[0.001, 0.001]\n\n\nexperience[T.42]\n-\n-\n-0.000\n\n\n\n-\n-\n[0.001, 0.001]\n\n\nexperience[T.43]\n-\n-\n0.000\n\n\n\n-\n-\n[0.001, 0.001]\n\n\nexperience[T.44]\n-\n-\n-0.002\n\n\n\n-\n-\n[-0.003, -0.003]\n\n\nexperience[T.45]\n-\n-\n-0.002\n\n\n\n-\n-\n[-0.000, -0.000]\n\n\nexperience[T.46]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.000, -0.000]\n\n\nexperience[T.47]\n-\n-\n-0.000\n\n\n\n-\n-\n[0.000, 0.000]\n\n\nexperience[T.48]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.002, -0.002]\n\n\nexperience[T.49]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.50]\n-\n-\n-0.001\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nexperience[T.51]\n-\n-\n-0.000\n\n\n\n-\n-\n[-0.000, -0.000]\n\n\noccupation[T.white]\n-\n-\n-0.016\n\n\n\n-\n-\n[-0.020, -0.020]\n\n\nindustry[T.yes]\n-\n-\n0.018\n\n\n\n-\n-\n[0.014, 0.014]\n\n\nyear[T.1977]\n-\n-\n0.000\n\n\n\n-\n-\n[-0.001, -0.001]\n\n\nyear[T.1978]\n-\n-\n0.000\n\n\n\n-\n-\n[-0.002, -0.002]\n\n\nyear[T.1979]\n-\n-\n0.000\n\n\n\n-\n-\n[0.003, 0.003]\n\n\nyear[T.1980]\n-\n-\n0.000\n\n\n\n-\n-\n[-0.005, -0.005]\n\n\nyear[T.1981]\n-\n-\n0.000\n\n\n\n-\n-\n[0.008, 0.008]\n\n\nyear[T.1982]\n-\n-\n0.000\n\n\n\n-\n-\n[0.000, 0.000]\n\n\n\nDecomposition variable: gender[T.male]. CIs are computed using B = 1000 bootstrap replications using iid sampling.Col 1: Raw Difference - Coefficient on gender[T.male] in short regression . Col 2: Adjusted Difference - Coefficient on gender[T.male] in long regression. Col 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression. Panel 1: Levels (units).\n\n\n\n\n\n\n\n\n\nNote: we can also return the decomposition results as a pd.DataFrame via the tidy method.\n\ngb.tidy().head()\n\n\n\n\n\n\n\n\ncoefficients\nci_lower\nci_upper\npanels\n\n\n\n\ndirect_effect\n0.474466\n0.469132\n0.469132\nLevels (units)\n\n\nfull_effect\n0.410338\n0.409346\n0.409346\nLevels (units)\n\n\nexplained_effect\n0.064129\n0.059785\n0.059785\nLevels (units)\n\n\nunexplained_effect\n0.410338\n0.409346\n0.409346\nLevels (units)\n\n\nethnicity[T.other]\n0.022749\n0.021393\n0.021393\nLevels (units)\n\n\n\n\n\n\n\nBecause experience is a categorical variable, the table gets pretty unhandy: we produce one estimate for “each” level. Luckily, Gelbach’s decomposition allows us to group individual contributions into a single number. In the decompose() method, we can combine variables via the combine_covariates argument:\n\ngb2 = fit_full.decompose(\n    decomp_var=\"gender[T.male]\",\n    combine_covariates={\n        \"experience\": re.compile(\"experience\"),\n        \"occupation\": re.compile(\"occupation\"),\n        \"industry\": re.compile(\"industry\"),\n        \"year\": re.compile(\"year\"),\n        \"ethnicity\": re.compile(\"ethnicity\"),\n    },\n    only_coef=True,  # suppress bootstrap for inference\n)\n\nWe now report a single value for “experience”, which explains a good chunk - around half - of the explained part of the gender wage gap.\n\ngb2.etable(\n    panels=\"levels\",\n)\n\n\n\n\n\n\n\n\nInitial Difference\nAdjusted Difference\nExplained Difference\n\n\n\n\nLevels (units)\n\n\ngender[T.male]\n0.474\n0.410\n0.063\n\n\nexperience\n-\n-\n0.039\n\n\noccupation\n-\n-\n-0.016\n\n\nindustry\n-\n-\n0.018\n\n\nyear\n-\n-\n0.000\n\n\nethnicity\n-\n-\n0.023\n\n\n\nDecomposition variable: gender[T.male]. Col 1: Raw Difference - Coefficient on gender[T.male] in short regression . Col 2: Adjusted Difference - Coefficient on gender[T.male] in long regression. Col 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression. Panel 1: Levels (units).\n\n\n\n\n\n\n\n\n\nWe can aggregate even more to “individual level” and “job” level variables:\n\ngb3 = fit_full.decompose(\n    decomp_var=\"gender[T.male]\",\n    combine_covariates={\n        \"job\": re.compile(r\".*(occupation|industry).*\"),\n        \"personal\": re.compile(r\".*(education|experience|ethnicity).*\"),\n        \"year\": re.compile(\"year\"),\n    },\n    only_coef=True,  # suppress inference\n)\n\n\ngb3.etable(panels=\"levels\")\n\n\n\n\n\n\n\n\nInitial Difference\nAdjusted Difference\nExplained Difference\n\n\n\n\nLevels (units)\n\n\ngender[T.male]\n0.474\n0.410\n0.064\n\n\njob\n-\n-\n0.002\n\n\npersonal\n-\n-\n0.062\n\n\nyear\n-\n-\n0.000\n\n\n\nDecomposition variable: gender[T.male]. Col 1: Raw Difference - Coefficient on gender[T.male] in short regression . Col 2: Adjusted Difference - Coefficient on gender[T.male] in long regression. Col 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression. Panel 1: Levels (units).\n\n\n\n\n\n\n\n\n\nWe can easily change multiple aspects of the GT table that etable returns. If we set panels = \"all\", etable() will also report normalized coefficients.\n\ngb3.etable(\n    column_heads=[\"Column A\", \"Column B\", \"Column C\"],\n    panel_heads=[\"Panel A\", \"Panel B\", \"Panel C\"],\n    panels=\"all\",\n    add_notes=\"We add more notes.\",\n    caption=\"Gelbach Decomposition\",\n)\n\n\n\n\n\n\n\nGelbach Decomposition\n\n\n\nColumn A\nColumn B\nColumn C\n\n\n\n\nPanel A\n\n\ngender[T.male]\n0.474\n0.410\n0.064\n\n\njob\n-\n-\n0.002\n\n\npersonal\n-\n-\n0.062\n\n\nyear\n-\n-\n0.000\n\n\nPanel B\n\n\ngender[T.male]\n1.000\n0.865\n0.135\n\n\njob\n-\n-\n0.004\n\n\npersonal\n-\n-\n0.132\n\n\nyear\n-\n-\n0.000\n\n\nPanel C\n\n\ngender[T.male]\n-\n-\n1.000\n\n\njob\n-\n-\n0.026\n\n\npersonal\n-\n-\n0.974\n\n\nyear\n-\n-\n0.000\n\n\n\nDecomposition variable: gender[T.male]. Col 1: Raw Difference - Coefficient on gender[T.male] in short regression . Col 2: Adjusted Difference - Coefficient on gender[T.male] in long regression. Col 3: Explained Difference - Difference in coefficients of gender[T.male] in short and long regression. Panel 1: Levels (units). Panel 2: Share of Full Effect: Levels normalized by coefficient of the short regression. Panel 3: Share of Explained Effect: Levels normalized by coefficient of the long regression. We add more notes.\n\n\n\n\n\n\n\n\n\nWe can visualise the Gelbach decomposition using the coefplot method.\n\ngb3.coefplot()"
  },
  {
    "objectID": "regression_decomposition.html#literature",
    "href": "regression_decomposition.html#literature",
    "title": "How much do Covariates Matter?",
    "section": "Literature",
    "text": "Literature\n\n“When do Covariates Matter? And Which Ones, and How Much?” by Gelbach, Jonah B. (2016), Journal of Labor Economics"
  },
  {
    "objectID": "multiple_testing.html",
    "href": "multiple_testing.html",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "",
    "text": "When conducting online A/B tests or large-scale experiments, we often analyze multiple dependent variables simultaneously. Analyzing multiple KPIs introduces a significant statistical challenge: the multiple testing problem.\nIn classical hypothesis testing, the significance level \\(\\alpha\\) controls the probability of a false positive (Type I error), typically \\(\\alpha=0.05\\). While this ensures a 5% chance of a false positive for a single test, the likelihood of detecting at least one false positive grows rapidly when conducting multiple tests. For example, testing 20 KPIs independently at \\(\\alpha = 0.05\\) results in a 64% chance of at least one false positive—calculated as \\(1 - (1 - 0.05)^{20} \\approx 0.64\\). This issue, known as the multiple testing problem, can lead to false claims of significant effects when none exist.\nTo address this, the concept of controlling the Familywise Error Rate (FWER) has been widely adopted. FWER controls the probability of at least one Type I error across a family of hypotheses. Several correction methods exist to mitigate the multiple testing problem, including:\nThis vignette demonstrates how these methods effectively control the FWER in a variety of scenarios. We will compare their performance and highlight the trade-offs between simplicity and statistical power. Specifically, we show that while Bonferroni provides strong error control, it is conservative in many practical applications. In contrast, Romano-Wolf and Westfall-Young methods are more powerful, offering greater sensitivity to detect true effects while maintaining robust control of the FWER."
  },
  {
    "objectID": "multiple_testing.html#what-is-a-family-wise-error-rate-fwer",
    "href": "multiple_testing.html#what-is-a-family-wise-error-rate-fwer",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "What is a Family-Wise Error Rate (FWER)?",
    "text": "What is a Family-Wise Error Rate (FWER)?\nSuppose that we are running an experiment and want to test if our treatment impacts 20 different dependent variables (KPIs). For any given independent test, the chance of a false positive is given by the (significance) level of the individual test, which is most commonly set to \\(\\alpha = 0.05\\). Formally, we can define the false positive rate for a single hypothesis \\(H\\) as:\n\\[\nP(\\text{reject } H \\mid H \\text{ is true}) = \\alpha\n\\]\nFor a family of tests \\(S = {s \\in \\{1, \\dots, P\\} }\\) hypotheses \\(\\{H_{s}\\}_{s \\in S}\\), we can analogously define the family-wise error rate (FWER) as:\n\\[\nP(\\text{reject at least one } H_{s} \\text{ with } s \\in I) = \\alpha\n\\]\nwhere \\(I\\) is the set of true hypotheses. In other words, the FWER is the probability of making at least one false positive across all tests in the family."
  },
  {
    "objectID": "multiple_testing.html#setup",
    "href": "multiple_testing.html#setup",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "Setup",
    "text": "Setup\nIn a first step, we create a data set with multiple (potentially correlated) dependent variables that share a common set of covariates. All simulations in this notebook are greatly inspired by Clarke, Romano & Wolf (Stata Journal, 2020).\n\n%load_ext autoreload\n%autoreload 2\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom great_tables import loc, style\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\n\nimport pyfixest as pf\n\n\nN = 100\nn_covariates = 5\nn_depvars = 20\n\n\ndef get_data(N, n_covariates, n_depvars, rho, seed):\n    \"Simulate data with true nulls.\"\n    rng = np.random.default_rng(seed)\n    Omega = np.eye(n_depvars)\n    X = rng.standard_normal((N, n_covariates))\n    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)\n    beta = np.zeros((n_covariates, n_depvars))\n    y = X @ beta + u_joint\n\n    data = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(n_covariates)])\n    data = data.assign(**{f\"y{i}\": y[:, i] for i in range(n_depvars)})\n\n    return data\n\n\ndata = get_data(\n    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345\n)\ndata.head()\n\n\n\n\n\n\n\n\nX0\nX1\nX2\nX3\nX4\ny0\ny1\ny2\ny3\ny4\n...\ny10\ny11\ny12\ny13\ny14\ny15\ny16\ny17\ny18\ny19\n\n\n\n\n0\n-1.423825\n1.263728\n-0.870662\n-0.259173\n-0.075343\n1.555991\n0.761234\n1.888281\n-0.614546\n0.179550\n...\n0.142083\n1.689619\n-0.425871\n0.421775\n-0.241513\n-0.770971\n-0.822431\n-1.358922\n0.817455\n0.163253\n\n\n1\n-0.740885\n-1.367793\n0.648893\n0.361058\n-1.952863\n-0.053753\n0.043408\n0.563138\n1.088257\n-0.219516\n...\n-0.882320\n0.641363\n-0.253652\n0.888703\n0.699996\n0.984490\n0.324864\n-1.463119\n-1.755173\n-0.656597\n\n\n2\n2.347410\n0.968497\n-0.759387\n0.902198\n-0.466953\n-0.933649\n0.481619\n0.793995\n1.191231\n-2.087805\n...\n0.962981\n3.198808\n0.901849\n-0.045545\n-0.408414\n-0.127183\n0.624903\n1.565282\n-0.480614\n0.607006\n\n\n3\n-0.060690\n0.788844\n-1.256668\n0.575858\n1.398979\n-0.135890\n0.329208\n0.304811\n-1.102051\n0.800707\n...\n2.080769\n1.058883\n-1.672165\n-1.443892\n-0.250928\n-1.014871\n-2.315505\n2.250437\n1.324270\n0.369040\n\n\n4\n1.322298\n-0.299699\n0.902919\n-1.621583\n-0.158189\n-1.426639\n-0.986879\n-0.140158\n0.986182\n1.498658\n...\n0.109724\n-0.463974\n1.094400\n1.542634\n-0.336492\n-1.932564\n0.669541\n-0.373858\n-0.619859\n-0.233117\n\n\n\n\n5 rows × 25 columns\n\n\n\nNow that we have our data set at hand, we can fit 20 regression models - one for each dependent variable. To do so, we will use pyfixest’s multiple estimation syntax.\n\ndependent_vars = \" + \".join([f\"y{i}\" for i in range(20)])\nindependent_vars = \" + \".join([f\"X{i}\" for i in range(5)])\nfml = f\"{dependent_vars} ~ {independent_vars}\"\n\n\nfit = pf.feols(fml, data=data, vcov=\"hetero\")\n\n\n(pf.etable(fit).tab_style(style=style.fill(color=\"yellow\"), locations=loc.body(rows=1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny0\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\ny11\ny12\ny13\ny14\ny15\ny16\ny17\ny18\ny19\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)\n(11)\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n(20)\n\n\n\n\ncoef\n\n\nX0\n-0.069\n(0.104)\n-0.019\n(0.111)\n0.090\n(0.093)\n0.070\n(0.118)\n-0.011\n(0.120)\n0.061\n(0.102)\n0.008\n(0.107)\n-0.103\n(0.138)\n0.023\n(0.099)\n0.097\n(0.091)\n0.116\n(0.086)\n0.070\n(0.139)\n-0.169\n(0.099)\n0.152\n(0.112)\n0.031\n(0.099)\n0.147\n(0.106)\n-0.085\n(0.109)\n0.266*\n(0.126)\n0.103\n(0.096)\n0.171\n(0.098)\n\n\nX1\n0.119\n(0.092)\n0.182\n(0.118)\n0.052\n(0.094)\n0.096\n(0.119)\n-0.036\n(0.109)\n-0.036\n(0.095)\n0.024\n(0.100)\n0.113\n(0.119)\n0.188\n(0.105)\n-0.027\n(0.096)\n0.085\n(0.087)\n-0.148\n(0.102)\n0.148\n(0.100)\n-0.043\n(0.121)\n-0.225*\n(0.097)\n-0.116\n(0.115)\n-0.088\n(0.088)\n0.098\n(0.110)\n-0.106\n(0.101)\n-0.092\n(0.112)\n\n\nX2\n0.067\n(0.088)\n0.088\n(0.094)\n0.082\n(0.071)\n0.037\n(0.106)\n0.042\n(0.103)\n0.123\n(0.110)\n0.040\n(0.086)\n0.099\n(0.105)\n-0.131\n(0.100)\n0.001\n(0.107)\n-0.171*\n(0.075)\n-0.198\n(0.109)\n0.116\n(0.101)\n0.024\n(0.109)\n-0.118\n(0.092)\n0.013\n(0.126)\n-0.005\n(0.107)\n-0.103\n(0.119)\n-0.018\n(0.109)\n-0.003\n(0.100)\n\n\nX3\n0.113\n(0.111)\n0.100\n(0.119)\n0.012\n(0.082)\n-0.006\n(0.101)\n-0.072\n(0.111)\n-0.127\n(0.112)\n0.132\n(0.114)\n0.206\n(0.147)\n-0.019\n(0.101)\n0.027\n(0.108)\n-0.067\n(0.085)\n0.033\n(0.108)\n0.023\n(0.109)\n0.021\n(0.100)\n0.057\n(0.104)\n-0.011\n(0.134)\n-0.098\n(0.099)\n0.079\n(0.099)\n0.019\n(0.088)\n-0.097\n(0.091)\n\n\nX4\n-0.027\n(0.105)\n0.043\n(0.084)\n-0.005\n(0.074)\n0.090\n(0.114)\n-0.198*\n(0.094)\n0.097\n(0.099)\n-0.102\n(0.100)\n-0.031\n(0.110)\n-0.024\n(0.103)\n0.094\n(0.111)\n0.220*\n(0.093)\n0.164\n(0.117)\n-0.201\n(0.122)\n-0.025\n(0.120)\n-0.069\n(0.095)\n-0.146\n(0.100)\n-0.037\n(0.096)\n-0.030\n(0.109)\n0.076\n(0.108)\n0.001\n(0.090)\n\n\nIntercept\n-0.136\n(0.105)\n-0.091\n(0.090)\n0.041\n(0.086)\n-0.246*\n(0.106)\n0.064\n(0.106)\n0.174\n(0.112)\n-0.053\n(0.101)\n-0.016\n(0.120)\n0.037\n(0.106)\n0.100\n(0.096)\n0.052\n(0.087)\n-0.095\n(0.099)\n-0.036\n(0.106)\n-0.090\n(0.111)\n-0.076\n(0.097)\n0.150\n(0.114)\n0.062\n(0.095)\n-0.082\n(0.107)\n-0.139\n(0.102)\n0.053\n(0.098)\n\n\nstats\n\n\nObservations\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n\n\nS.E. type\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\n\n\nR2\n0.026\n0.050\n0.026\n0.024\n0.047\n0.035\n0.024\n0.046\n0.059\n0.020\n0.122\n0.076\n0.068\n0.023\n0.080\n0.049\n0.028\n0.083\n0.024\n0.045\n\n\nAdj. R2\n-0.025\n-0.001\n-0.025\n-0.028\n-0.004\n-0.016\n-0.028\n-0.005\n0.009\n-0.032\n0.075\n0.027\n0.018\n-0.029\n0.031\n-0.002\n-0.024\n0.035\n-0.028\n-0.005\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe see that our estimation produces multiple false positives for the effect of X1 on multiple dependent variables. Recall that we had simulated X1 to have no effect on any of the dependent variables. Still, the estimation procedure produces a significant effect for X1 on multiple dependent variables.\nPyFixest provides three functions to adjust inference for multiple testing: pf.bonferroni(), pf.rwolf(), and pf.wyoung(). All three share a common API.\n\npval_bonferroni = (\n    pf.bonferroni(fit.to_list(), param=\"X1\").xs(\"Bonferroni Pr(&gt;|t|)\").values\n)\n\npval_rwolf = (\n    pf.rwolf(fit.to_list(), param=\"X1\", reps=1000, seed=22).xs(\"RW Pr(&gt;|t|)\").values\n)\n\npval_wyoung = (\n    pf.wyoung(fit.to_list(), param=\"X1\", reps=1000, seed=22).xs(\"WY Pr(&gt;|t|)\").values\n)\n\n\n(\n    pf.etable(\n        fit,\n        custom_model_stats={\n            \"Bonferroni: pval(X1)\": pval_bonferroni.round(4).tolist(),\n            \"RW: pval(X1)\": pval_rwolf.round(4).tolist(),\n            \"WY: pval(X1)\": pval_wyoung.round(4).tolist(),\n        },\n    ).tab_style(style=style.fill(color=\"yellow\"), locations=loc.body(rows=[6, 7, 8]))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny0\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\ny11\ny12\ny13\ny14\ny15\ny16\ny17\ny18\ny19\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)\n(11)\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n(20)\n\n\n\n\ncoef\n\n\nX0\n-0.069\n(0.104)\n-0.019\n(0.111)\n0.090\n(0.093)\n0.070\n(0.118)\n-0.011\n(0.120)\n0.061\n(0.102)\n0.008\n(0.107)\n-0.103\n(0.138)\n0.023\n(0.099)\n0.097\n(0.091)\n0.116\n(0.086)\n0.070\n(0.139)\n-0.169\n(0.099)\n0.152\n(0.112)\n0.031\n(0.099)\n0.147\n(0.106)\n-0.085\n(0.109)\n0.266*\n(0.126)\n0.103\n(0.096)\n0.171\n(0.098)\n\n\nX1\n0.119\n(0.092)\n0.182\n(0.118)\n0.052\n(0.094)\n0.096\n(0.119)\n-0.036\n(0.109)\n-0.036\n(0.095)\n0.024\n(0.100)\n0.113\n(0.119)\n0.188\n(0.105)\n-0.027\n(0.096)\n0.085\n(0.087)\n-0.148\n(0.102)\n0.148\n(0.100)\n-0.043\n(0.121)\n-0.225*\n(0.097)\n-0.116\n(0.115)\n-0.088\n(0.088)\n0.098\n(0.110)\n-0.106\n(0.101)\n-0.092\n(0.112)\n\n\nX2\n0.067\n(0.088)\n0.088\n(0.094)\n0.082\n(0.071)\n0.037\n(0.106)\n0.042\n(0.103)\n0.123\n(0.110)\n0.040\n(0.086)\n0.099\n(0.105)\n-0.131\n(0.100)\n0.001\n(0.107)\n-0.171*\n(0.075)\n-0.198\n(0.109)\n0.116\n(0.101)\n0.024\n(0.109)\n-0.118\n(0.092)\n0.013\n(0.126)\n-0.005\n(0.107)\n-0.103\n(0.119)\n-0.018\n(0.109)\n-0.003\n(0.100)\n\n\nX3\n0.113\n(0.111)\n0.100\n(0.119)\n0.012\n(0.082)\n-0.006\n(0.101)\n-0.072\n(0.111)\n-0.127\n(0.112)\n0.132\n(0.114)\n0.206\n(0.147)\n-0.019\n(0.101)\n0.027\n(0.108)\n-0.067\n(0.085)\n0.033\n(0.108)\n0.023\n(0.109)\n0.021\n(0.100)\n0.057\n(0.104)\n-0.011\n(0.134)\n-0.098\n(0.099)\n0.079\n(0.099)\n0.019\n(0.088)\n-0.097\n(0.091)\n\n\nX4\n-0.027\n(0.105)\n0.043\n(0.084)\n-0.005\n(0.074)\n0.090\n(0.114)\n-0.198*\n(0.094)\n0.097\n(0.099)\n-0.102\n(0.100)\n-0.031\n(0.110)\n-0.024\n(0.103)\n0.094\n(0.111)\n0.220*\n(0.093)\n0.164\n(0.117)\n-0.201\n(0.122)\n-0.025\n(0.120)\n-0.069\n(0.095)\n-0.146\n(0.100)\n-0.037\n(0.096)\n-0.030\n(0.109)\n0.076\n(0.108)\n0.001\n(0.090)\n\n\nIntercept\n-0.136\n(0.105)\n-0.091\n(0.090)\n0.041\n(0.086)\n-0.246*\n(0.106)\n0.064\n(0.106)\n0.174\n(0.112)\n-0.053\n(0.101)\n-0.016\n(0.120)\n0.037\n(0.106)\n0.100\n(0.096)\n0.052\n(0.087)\n-0.095\n(0.099)\n-0.036\n(0.106)\n-0.090\n(0.111)\n-0.076\n(0.097)\n0.150\n(0.114)\n0.062\n(0.095)\n-0.082\n(0.107)\n-0.139\n(0.102)\n0.053\n(0.098)\n\n\nstats\n\n\nBonferroni: pval(X1)\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.4402\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nRW: pval(X1)\n0.968\n0.9341\n0.995\n0.995\n1.0\n1.0\n1.0\n0.995\n0.8182\n1.0\n0.995\n0.9491\n0.9491\n1.0\n0.4006\n0.995\n0.995\n0.995\n0.995\n0.995\n\n\nWY: pval(X1)\n0.968\n0.934\n0.995\n0.995\n1.0\n1.0\n1.0\n0.995\n0.818\n1.0\n0.995\n0.949\n0.949\n1.0\n0.4\n0.995\n0.995\n0.995\n0.995\n0.995\n\n\nObservations\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n\n\nS.E. type\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\nhetero\n\n\nR2\n0.026\n0.050\n0.026\n0.024\n0.047\n0.035\n0.024\n0.046\n0.059\n0.020\n0.122\n0.076\n0.068\n0.023\n0.080\n0.049\n0.028\n0.083\n0.024\n0.045\n\n\nAdj. R2\n-0.025\n-0.001\n-0.025\n-0.028\n-0.004\n-0.016\n-0.028\n-0.005\n0.009\n-0.032\n0.075\n0.027\n0.018\n-0.029\n0.031\n-0.002\n-0.024\n0.035\n-0.028\n-0.005\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe quickly see that the corrected p-values do not flag any false positives."
  },
  {
    "objectID": "multiple_testing.html#controlling-for-the-familiy-wise-error-rate-fwer",
    "href": "multiple_testing.html#controlling-for-the-familiy-wise-error-rate-fwer",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "Controlling for the Familiy-Wise Error Rate (FWER)",
    "text": "Controlling for the Familiy-Wise Error Rate (FWER)\nWe now show by means of simulation that the three methods control the family-wise error rate (FWER). To do so, we simulate 1000 data sets imposing true nulls for the effect of X1 on all of 20 created dependent variables. For each simulation, we then count if the methods flag more than one false positive and report our results.\n\ndef compute_family_rejection(seed, rho):\n    \"Simulate data, estimate models, and compute family rejection rates.\"\n    data = get_data(N, n_covariates, n_depvars, rho, seed=seed)\n    fit = pf.feols(fml=fml, data=data, vcov=\"hetero\")\n    df = fit.tidy().reset_index().set_index(\"Coefficient\").xs(\"X1\")\n\n    df[\"Pr(&gt;|t|) reject\"] = df[\"Pr(&gt;|t|)\"] &lt; 0.05\n    df[\"Bonferroni reject\"] = (\n        pf.bonferroni(fit, param=\"X1\").xs(\"Bonferroni Pr(&gt;|t|)\").values &lt; 0.05\n    )\n    df[\"rwolf reject\"] = (\n        pf.rwolf(fit, param=\"X1\", reps=1000, seed=seed * 11).xs(\"RW Pr(&gt;|t|)\").values\n        &lt; 0.05\n    )\n    df[\"wyoung reject\"] = (\n        pf.wyoung(fit, param=\"X1\", reps=1000, seed=seed * 11).xs(\"WY Pr(&gt;|t|)\").values\n        &lt; 0.05\n    )\n\n    # Compute family rejection means\n    family_rejection = {\n        \"Pr(&gt;|t|) reject family\": df[\"Pr(&gt;|t|) reject\"].sum() &gt; 0,\n        \"Bonferroni reject family\": df[\"Bonferroni reject\"].sum() &gt; 0,\n        \"rwolf reject family\": df[\"rwolf reject\"].sum() &gt; 0,\n        \"wyoung reject family\": df[\"wyoung reject\"].sum() &gt; 0,\n    }\n\n    return pd.Series(family_rejection)\n\n\ndef run_fwer_simulation(n_iter, rho):\n    \"Run simulation for family-wise error rate.\"\n    results = Parallel(n_jobs=-1)(\n        delayed(compute_family_rejection)(seed, rho=rho) for seed in tqdm(range(n_iter))\n    )\n    return pd.concat(results).reset_index().groupby(\"index\").mean()\n\n\nrun_fwer_simulation(n_iter=1000, rho=0.5)\n\n  8%|▊         | 80/1000 [03:34&lt;38:48,  2.53s/it]  \n\n\nWe see that all three correction methods get close to the desired 5% level. In contrast, the uncorrected method produces the expected much higher family-wise error rate."
  },
  {
    "objectID": "multiple_testing.html#power",
    "href": "multiple_testing.html#power",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "Power",
    "text": "Power\nNow that we’ve seen that all three methods effectively handle false positives, let’s see how well they avoid false negatives. In other words, we will study how powerful all three methods are in detecting true effects.\nTo do so, we slightly have to adjust the data generating process. Instead of simulating the impact of X1 on all dependent variables to be zero (a true null effect), we will now simulate the impact of X1 to be \\(0.5\\) for all dependent variables. Hence we simulate true positives and count how often we correctly detect the true effect, or, equivalently stated, how often we correctly reject the null of no treatment effect.\n\ndef get_data_true_effect(N, n_covariates, n_depvars, rho=0.5, seed=12345, effect=0.1):\n    \"Generate data with true positives.\"\n    rng = np.random.default_rng(seed)\n    Omega = np.eye(n_depvars)\n    Omega[Omega != 1] = rho\n    X = rng.standard_normal((N, n_covariates))\n    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)\n    beta = np.zeros((n_covariates, n_depvars))\n    beta[1, :] = effect\n    y = X @ beta + u_joint\n\n    data = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(n_covariates)])\n    data = data.assign(**{f\"y{i}\": y[:, i] for i in range(n_depvars)})\n\n    return data\n\n\ndata_true = get_data_true_effect(\n    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345, effect=0.5\n)\nfit = pf.feols(fml, data=data_true)\n\n\n(\n    pf.etable(fit).tab_style(\n        style=style.fill(color=\"yellow\"), locations=loc.body(rows=[1])\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny0\ny1\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\ny11\ny12\ny13\ny14\ny15\ny16\ny17\ny18\ny19\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)\n(11)\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n(20)\n\n\n\n\ncoef\n\n\nX0\n-0.108\n(0.103)\n-0.152\n(0.116)\n-0.243*\n(0.113)\n-0.026\n(0.112)\n-0.158\n(0.109)\n-0.132\n(0.108)\n-0.155\n(0.104)\n-0.180\n(0.114)\n-0.254*\n(0.111)\n-0.157\n(0.106)\n-0.066\n(0.109)\n-0.005\n(0.115)\n-0.214*\n(0.101)\n-0.183\n(0.115)\n-0.007\n(0.099)\n-0.138\n(0.105)\n-0.104\n(0.107)\n-0.138\n(0.111)\n-0.102\n(0.110)\n-0.154\n(0.098)\n\n\nX1\n0.538***\n(0.099)\n0.493***\n(0.112)\n0.589***\n(0.109)\n0.503***\n(0.108)\n0.340**\n(0.106)\n0.493***\n(0.104)\n0.568***\n(0.100)\n0.590***\n(0.110)\n0.502***\n(0.107)\n0.505***\n(0.102)\n0.375***\n(0.105)\n0.411***\n(0.111)\n0.452***\n(0.098)\n0.586***\n(0.111)\n0.438***\n(0.096)\n0.558***\n(0.101)\n0.408***\n(0.103)\n0.442***\n(0.107)\n0.498***\n(0.106)\n0.572***\n(0.095)\n\n\nX2\n0.081\n(0.101)\n0.026\n(0.114)\n0.094\n(0.111)\n0.094\n(0.110)\n0.008\n(0.108)\n-0.036\n(0.106)\n-0.054\n(0.102)\n-0.014\n(0.112)\n-0.043\n(0.109)\n-0.042\n(0.104)\n0.128\n(0.107)\n-0.043\n(0.113)\n0.039\n(0.099)\n0.053\n(0.113)\n-0.048\n(0.097)\n0.027\n(0.103)\n-0.021\n(0.105)\n0.019\n(0.109)\n0.015\n(0.108)\n-0.106\n(0.097)\n\n\nX3\n-0.068\n(0.104)\n-0.110\n(0.117)\n-0.033\n(0.114)\n-0.098\n(0.113)\n-0.127\n(0.110)\n-0.086\n(0.109)\n-0.056\n(0.104)\n-0.094\n(0.115)\n-0.092\n(0.112)\n-0.064\n(0.107)\n-0.099\n(0.110)\n-0.003\n(0.116)\n0.032\n(0.102)\n-0.157\n(0.116)\n-0.085\n(0.100)\n-0.017\n(0.106)\n-0.147\n(0.108)\n-0.027\n(0.111)\n-0.067\n(0.110)\n-0.088\n(0.099)\n\n\nX4\n-0.063\n(0.101)\n0.013\n(0.114)\n-0.142\n(0.111)\n0.002\n(0.110)\n0.086\n(0.107)\n-0.092\n(0.106)\n-0.059\n(0.102)\n0.030\n(0.112)\n-0.013\n(0.109)\n-0.067\n(0.104)\n0.068\n(0.107)\n-0.134\n(0.113)\n-0.012\n(0.099)\n-0.021\n(0.112)\n0.046\n(0.097)\n0.076\n(0.103)\n-0.033\n(0.105)\n-0.019\n(0.108)\n-0.073\n(0.107)\n0.061\n(0.096)\n\n\nIntercept\n-0.007\n(0.100)\n-0.061\n(0.113)\n-0.010\n(0.110)\n0.060\n(0.109)\n-0.035\n(0.107)\n0.003\n(0.105)\n-0.013\n(0.101)\n-0.014\n(0.111)\n-0.029\n(0.108)\n0.063\n(0.103)\n-0.003\n(0.106)\n0.071\n(0.112)\n0.117\n(0.098)\n0.022\n(0.112)\n-0.013\n(0.097)\n-0.034\n(0.102)\n0.054\n(0.104)\n-0.013\n(0.108)\n0.027\n(0.107)\n0.053\n(0.096)\n\n\nstats\n\n\nObservations\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n\n\nS.E. type\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\niid\n\n\nR2\n0.259\n0.202\n0.264\n0.214\n0.146\n0.226\n0.288\n0.269\n0.239\n0.238\n0.151\n0.149\n0.207\n0.273\n0.218\n0.267\n0.186\n0.170\n0.213\n0.333\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe will now study power more systematically via a simulation. More concretely, we will compute how often we detect the true effect of X1 on Y1, Y2, …, etc given a fixed sample size \\(N\\) using “uncorrected” p-values, the Bonferroni, Romano-Wolf and Westfall-Young methods.\n\ndef compute_power(seed, rho, effect):\n    \"Simulate data, estimate models, and compute power.\"\n    data = get_data_true_effect(\n        N, n_covariates, n_depvars, rho, seed=seed, effect=effect\n    )\n    fit = pf.feols(\n        fml=fml, data=data, vcov=\"hetero\"\n    )  # model '1' regresses on Y1 - we're only interested in the power of this specific test\n    df = fit.tidy().reset_index().set_index(\"Coefficient\").xs(\"X1\")\n\n    df[\"Pr(&gt;|t|) detect\"] = df[\"Pr(&gt;|t|)\"] &lt; 0.05\n    df[\"Bonferroni detect\"] = (\n        pf.bonferroni(fit, param=\"X1\").xs(\"Bonferroni Pr(&gt;|t|)\").values &lt; 0.05\n    )\n    df[\"rwolf detect\"] = (\n        pf.rwolf(fit, param=\"X1\", reps=200, seed=seed * 11).xs(\"RW Pr(&gt;|t|)\").values\n        &lt; 0.05\n    )\n    df[\"wyoung detect\"] = (\n        pf.wyoung(fit, param=\"X1\", reps=200, seed=seed * 11).xs(\"WY Pr(&gt;|t|)\").values\n        &lt; 0.05\n    )\n\n    # Compute family rejection means\n    detect_effect = {\n        \"Pr(&gt;|t|) detect effect\": df[\"Pr(&gt;|t|) detect\"].mean(),\n        \"Bonferroni detect effect\": df[\"Bonferroni detect\"].mean(),\n        \"rwolf detect effect\": df[\"rwolf detect\"].mean(),\n        \"wyoung detect effect\": df[\"wyoung detect\"].mean(),\n    }\n\n    detect_effect_df = pd.DataFrame(detect_effect, index=[effect])\n    return detect_effect_df\n\n\ndef run_power_simulation(n_iter, rho, effect, nthreads=-1):\n    \"Run simulation for power.\"\n    seeds = list(range(n_iter))\n    results = Parallel(n_jobs=nthreads)(\n        delayed(compute_power)(seed, rho=rho, effect=effect) for seed in tqdm(seeds)\n    )\n\n    return pd.concat(results).mean()\n\n\nrun_power_simulation(n_iter=1000, rho=0.5, effect=0.4)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]c:\\Users\\alexa\\Documents\\pyfixest\\.pixi\\envs\\dev\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n100%|██████████| 1000/1000 [12:00&lt;00:00,  1.39it/s]\n\n\nPr(&gt;|t|) detect effect      0.97045\nBonferroni detect effect    0.78485\nrwolf detect effect         0.86985\nwyoung detect effect        0.86985\ndtype: float64\n\n\nWe see that the “unadjusted” method detects the “true effects” at the highest frequency with on average 97% correctly detected effects. Does this mean that we should use uncorrected tests then? Well, maybe, but these do not control the family-wise error rate. While we have a better chance to detect a true effect, we also have a higher risk of flagging false positives.\nAdditionally, it looks as if the rwolf and wyoung methods detect the true positives at a slightly higher rate than the Bonferroni method.\nDo these findings generalize to other effect sizes? We can check this by simply imposing different effects on the data generating process and repeating the previous exercise multiple times.\n\ndef run_power_simulation_vary_effect():\n    \"Run power simulations with varying effect sizes.\"\n    n_points = 10\n    max_val = 0.7\n\n    effects = (\n        np.sign(np.linspace(-1, 1, n_points))\n        * max_val\n        * (np.linspace(-1, 1, n_points) ** 2)\n    )\n\n    res_list = []\n    for effect_size in tqdm(effects):\n        res = run_power_simulation(n_iter=1000, rho=0.5, effect=effect_size)\n        res[\"effect\"] = effect_size\n        res_list.append(res)\n    return pd.concat(res_list, axis=1).T.set_index(\"effect\")\n\n\nres = run_power_simulation_vary_effect()\n\n100%|██████████| 1000/1000 [10:54&lt;00:00,  1.53it/s]\n100%|██████████| 1000/1000 [10:17&lt;00:00,  1.62it/s]\n100%|██████████| 1000/1000 [10:05&lt;00:00,  1.65it/s]\n100%|██████████| 1000/1000 [10:09&lt;00:00,  1.64it/s]\n100%|██████████| 1000/1000 [10:25&lt;00:00,  1.60it/s]\n100%|██████████| 1000/1000 [09:54&lt;00:00,  1.68it/s]\n100%|██████████| 1000/1000 [09:44&lt;00:00,  1.71it/s]\n100%|██████████| 1000/1000 [10:53&lt;00:00,  1.53it/s]\n100%|██████████| 1000/1000 [14:32&lt;00:00,  1.15it/s]\n100%|██████████| 1000/1000 [14:38&lt;00:00,  1.14it/s]\n100%|██████████| 10/10 [1:53:04&lt;00:00, 678.48s/it]\n\n\n\ncolumn_to_label_dict = {\n    \"Pr(&gt;|t|) detect effect\": \"Unadjusted\",\n    \"Bonferroni detect effect\": \"Bonferroni\",\n    \"rwolf detect effect\": \"RW\",\n    \"wyoung detect effect\": \"WY\",\n}\n\nplt.figure(figsize=(10, 6))\nline_styles = [\"-\", \"--\", \"-.\", \":\"]\nfor column, line_style in zip(res.columns, line_styles):\n    plt.plot(\n        res.index, res[column], linestyle=line_style, label=column_to_label_dict[column]\n    )\n\nplt.title(\"Power of Multiple Testing Correction Procedures\")\nplt.xlabel(\"Effect\")\nplt.ylabel(\"Proportion of correctly detected effects\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe see that for any simulated effect size, the Romano-Wolf and Westfall-Young methods detect a higher share of true positives than the Bonferroni method: they have higher power."
  },
  {
    "objectID": "multiple_testing.html#literature",
    "href": "multiple_testing.html#literature",
    "title": "Multiple Hypothesis Testing Corrections",
    "section": "Literature",
    "text": "Literature\n\nClarke, Damian, Joseph P. Romano, and Michael Wolf. “The Romano–Wolf multiple-hypothesis correction in Stata.” The Stata Journal 20.4 (2020): 812-843.\nRomano, Joseph P., and Michael Wolf. “Stepwise multiple testing as formalized data snooping.” Econometrica 73.4 (2005): 1237-1282.\nWestfall, Peter H., and S. Stanley Young. Resampling-based multiple testing: Examples and methods for p-value adjustment. Vol. 279. John Wiley & Sons, 1993."
  },
  {
    "objectID": "pyfixest.html#support-pyfixest",
    "href": "pyfixest.html#support-pyfixest",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Support PyFixest",
    "text": "Support PyFixest\nIf you enjoy using PyFixest, please consider donating to GiveDirectly and dedicating your donation to pyfixest.dev@gmail.com. You can also leave a message through the donation form - your support and encouragement mean a lot to the developers!"
  },
  {
    "objectID": "pyfixest.html#features",
    "href": "pyfixest.html#features",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Features",
    "text": "Features\n\nOLS, WLS and IV Regression with Fixed-Effects Demeaning via Frisch-Waugh-Lovell\nPoisson Regression following the pplmhdfe algorithm\nProbit, Logit and Gaussian Family GLMs (currently without fixed effects demeaning, this is WIP)\nQuantile Regression using an Interior Point Solver\nMultiple Estimation Syntax\nSeveral Robust, Cluster Robust and HAC Variance-Covariance Estimators\nWild Cluster Bootstrap Inference (via wildboottest)\nDifference-in-Differences Estimators:\n\nThe canonical Two-Way Fixed Effects Estimator\nGardner’s two-stage (“Did2s”) estimator\nBasic Versions of the Local Projections estimator following Dube et al (2023)\nThe fully saturated Event-Study estimator following Sun & Abraham (2021)\n\nMultiple Hypothesis Corrections following the Procedure by Romano and Wolf and Simultaneous Confidence Intervals using a Multiplier Bootstrap\nThe Causal Cluster Variance Estimator (CCV) following Abadie et al.\nRegression Decomposition following Gelbach (2016)\nPublication-ready tables with Great Tables or LaTex booktabs"
  },
  {
    "objectID": "pyfixest.html#installation",
    "href": "pyfixest.html#installation",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Installation",
    "text": "Installation\nYou can install the release version from PyPi by running\npip install -U pyfixest\nor the development version from github by running\npip install git+https://github.com/py-econometrics/pyfixest.git\n\nGPU Acceleration (Optional)\nPyFixest supports GPU-accelerated fixed effects demeaning via CuPy. To enable GPU acceleration, install CuPy matching your CUDA version:\n# For CUDA 11.x, 12.x, 13.x\npip install cupy-cuda11x\npip install cupy-cuda12x\npip install cupy-cuda13x\nOnce installed, you can use GPU-accelerated demeaning by setting the demean_backend parameter:\n# Use GPU with float32 and float64 precision\npf.feols(\"Y ~ X1 | f1 + f2\", data=data, demean_backend=\"cupy32\")\npf.feols(\"Y ~ X1 | f1 + f2\", data=data, demean_backend=\"cupy64\")"
  },
  {
    "objectID": "pyfixest.html#benchmarks",
    "href": "pyfixest.html#benchmarks",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Benchmarks",
    "text": "Benchmarks\nAll benchmarks follow the fixest benchmarks. All non-pyfixest timings are taken from the fixest benchmarks."
  },
  {
    "objectID": "pyfixest.html#quickstart",
    "href": "pyfixest.html#quickstart",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Quickstart",
    "text": "Quickstart\nimport pyfixest as pf\n\ndata = pf.get_data()\npf.feols(\"Y ~ X1 | f1 + f2\", data=data).summary()\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.919 |        0.065 |   -14.057 |      0.000 | -1.053 |  -0.786 |\n---\nRMSE: 1.441   R2: 0.609   R2 Within: 0.2\n\nMultiple Estimation\nYou can estimate multiple models at once by using multiple estimation syntax:\n# OLS Estimation: estimate multiple models at once\nfit = pf.feols(\"Y + Y2 ~X1 | csw0(f1, f2)\", data = data, vcov = {'CRV1':'group_id'})\n# Print the results\nfit.etable()\n                           est1               est2               est3               est4               est5               est6\n------------  -----------------  -----------------  -----------------  -----------------  -----------------  -----------------\ndepvar                        Y                 Y2                  Y                 Y2                  Y                 Y2\n------------------------------------------------------------------------------------------------------------------------------\nIntercept      0.919*** (0.121)   1.064*** (0.232)\nX1            -1.000*** (0.117)  -1.322*** (0.211)  -0.949*** (0.087)  -1.266*** (0.212)  -0.919*** (0.069)  -1.228*** (0.194)\n------------------------------------------------------------------------------------------------------------------------------\nf2                            -                  -                  -                  -                  x                  x\nf1                            -                  -                  x                  x                  x                  x\n------------------------------------------------------------------------------------------------------------------------------\nR2                        0.123              0.037              0.437              0.115              0.609              0.168\nS.E. type          by: group_id       by: group_id       by: group_id       by: group_id       by: group_id       by: group_id\nObservations                998                999                997                998                997                998\n------------------------------------------------------------------------------------------------------------------------------\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nFormat of coefficient cell:\nCoefficient (Std. Error)\n\n\nAdjust Standard Errors “on-the-fly”\nStandard Errors can be adjusted after estimation, “on-the-fly”:\nfit1 = fit.fetch_model(0)\nfit1.vcov(\"hetero\").summary()\nModel:  Y~X1\n###\n\nEstimation:  OLS\nDep. var.: Y\nInference:  hetero\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.919 |        0.112 |     8.223 |      0.000 |  0.699 |   1.138 |\n| X1            |     -1.000 |        0.082 |   -12.134 |      0.000 | -1.162 |  -0.838 |\n---\nRMSE: 2.158   R2: 0.123\n\n\nPoisson Regression via fepois()\nYou can estimate Poisson Regressions via the fepois() function:\npoisson_data = pf.get_data(model = \"Fepois\")\npf.fepois(\"Y ~ X1 + X2 | f1 + f2\", data = poisson_data).summary()\n###\n\nEstimation:  Poisson\nDep. var.: Y, Fixed effects: f1+f2\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.007 |        0.035 |    -0.190 |      0.850 | -0.075 |   0.062 |\n| X2            |     -0.015 |        0.010 |    -1.449 |      0.147 | -0.035 |   0.005 |\n---\nDeviance: 1068.169\n\n\nIV Estimation via three-part formulas\nLast, PyFixest also supports IV estimation via three part formula syntax:\nfit_iv = pf.feols(\"Y ~ 1 | f1 | X1 ~ Z1\", data = data)\nfit_iv.summary()\n###\n\nEstimation:  IV\nDep. var.: Y, Fixed effects: f1\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.025 |        0.115 |    -8.930 |      0.000 | -1.259 |  -0.790 |\n---"
  },
  {
    "objectID": "pyfixest.html#quantile-regression-via-pf.quantreg",
    "href": "pyfixest.html#quantile-regression-via-pf.quantreg",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Quantile Regression via pf.quantreg",
    "text": "Quantile Regression via pf.quantreg\nfit_qr = pf.quantreg(\"Y ~ X1 + X2\", data = data, quantile = 0.5)"
  },
  {
    "objectID": "pyfixest.html#call-for-contributions",
    "href": "pyfixest.html#call-for-contributions",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Call for Contributions",
    "text": "Call for Contributions\nThanks for showing interest in contributing to pyfixest! We appreciate all contributions and constructive feedback, whether that be reporting bugs, requesting new features, or suggesting improvements to documentation.\nUpcoming: We’re hosting a PyFixest Sprint in Heilbronn with AppliedAI in late February/early March 2026. Interested in joining? Learn more and get in touch.\nIf you’d like to get involved, but are not yet sure how, please feel free to send us an email. Some familiarity with either Python or econometrics will help, but you really don’t need to be a numpy core developer or have published in Econometrica =) We’d be more than happy to invest time to help you get started!"
  },
  {
    "objectID": "pyfixest.html#contributors",
    "href": "pyfixest.html#contributors",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Contributors ✨",
    "text": "Contributors ✨\nThanks goes to these wonderful people:\n\n\n\n\n\n\n\nstyfenschaer💻\n\n\nNiall Keleher🚇 💻\n\n\nWenzhi Ding💻\n\n\nApoorva Lal💻 🐛\n\n\nJuan Orduz🚇 💻\n\n\nAlexander Fischer💻 🚇\n\n\naeturrell✅ 📖 📣\n\n\n\n\n\n\n\nThis project follows the all-contributors specification. Contributions of any kind welcome!"
  },
  {
    "objectID": "pyfixest.html#acknowledgements",
    "href": "pyfixest.html#acknowledgements",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nFirst and foremost, we want to acknowledge Laurent Bergé’s formidable fixest, which is so good we decided to stick to its API and conventions as closely as Python allows.\nFor a full list of software packages and papers that have influenced PyFixest, please take a look at the Acknowledgements page.\nWe also want to thank all institutions that have funded or supported work on PyFixest!"
  },
  {
    "objectID": "pyfixest.html#how-to-cite",
    "href": "pyfixest.html#how-to-cite",
    "title": "PyFixest: Fast High-Dimensional Fixed Effects Regression in Python",
    "section": "How to Cite",
    "text": "How to Cite\nIf you want to cite PyFixest, you can use the following BibTeX entry:\n@software{pyfixest,\n  author  = {{The PyFixest Authors}},\n  title   = {{pyfixest: Fast high-dimensional fixed effect estimation in Python}},\n  year    = {2025},\n  url     = {https://github.com/py-econometrics/pyfixest}\n}"
  },
  {
    "objectID": "difference-in-differences.html",
    "href": "difference-in-differences.html",
    "title": "Difference-in-Differences Estimation",
    "section": "",
    "text": "PyFixest supports event study designs via the canonical two-way fixed effects design, the 2-Step imputation estimator, and local projections.\nSee also NBER SI methods lectures on Linear Panel Event Studies."
  },
  {
    "objectID": "difference-in-differences.html#setup",
    "href": "difference-in-differences.html#setup",
    "title": "Difference-in-Differences Estimation",
    "section": "Setup",
    "text": "Setup\n\nfrom importlib import resources\n\nimport pandas as pd\n\nimport pyfixest as pf\nfrom pyfixest.report.utils import rename_event_study_coefs\nfrom pyfixest.utils.dgps import get_sharkfin\n\n%load_ext watermark\n%watermark --iversions\n%load_ext autoreload\n%autoreload 2\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\npyfixest: 0.40.1\npandas  : 2.3.3\n\n\n\n\n# one-shot adoption data - parallel trends is true\ndf_one_cohort = get_sharkfin()\ndf_one_cohort.head()\n\n\n\n\n  \n    \n      \n      unit\n      year\n      treat\n      Y\n      ever_treated\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      1.629307\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0.825902\n      0\n    \n    \n      2\n      0\n      2\n      0\n      0.208988\n      0\n    \n    \n      3\n      0\n      3\n      0\n      -0.244739\n      0\n    \n    \n      4\n      0\n      4\n      0\n      0.804665\n      0\n    \n  \n\n\n\n\n\n# multi-cohort adoption data\ndf_multi_cohort = pd.read_csv(\n    resources.files(\"pyfixest.did.data\").joinpath(\"df_het.csv\")\n)\ndf_multi_cohort.head()\n\n\n\n\n  \n    \n      \n      unit\n      state\n      group\n      unit_fe\n      g\n      year\n      year_fe\n      treat\n      rel_year\n      rel_year_binned\n      error\n      te\n      te_dynamic\n      dep_var\n    \n  \n  \n    \n      0\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1990\n      0.066159\n      False\n      -20.0\n      -6\n      -0.086466\n      0\n      0.0\n      7.022709\n    \n    \n      1\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1991\n      -0.030980\n      False\n      -19.0\n      -6\n      0.766593\n      0\n      0.0\n      7.778628\n    \n    \n      2\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1992\n      -0.119607\n      False\n      -18.0\n      -6\n      1.512968\n      0\n      0.0\n      8.436377\n    \n    \n      3\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1993\n      0.126321\n      False\n      -17.0\n      -6\n      0.021870\n      0\n      0.0\n      7.191207\n    \n    \n      4\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1994\n      -0.106921\n      False\n      -16.0\n      -6\n      -0.017603\n      0\n      0.0\n      6.918492"
  },
  {
    "objectID": "difference-in-differences.html#examining-treatment-timing",
    "href": "difference-in-differences.html#examining-treatment-timing",
    "title": "Difference-in-Differences Estimation",
    "section": "Examining Treatment Timing",
    "text": "Examining Treatment Timing\nBefore any DiD estimation, we need to examine the treatment timing, since it is crucial to our choice of estimator.\n\npf.panelview(\n    df_one_cohort,\n    unit=\"unit\",\n    time=\"year\",\n    treat=\"treat\",\n    collapse_to_cohort=True,\n    sort_by_timing=True,\n    ylab=\"Cohort\",\n    xlab=\"Year\",\n    title=\"Treatment Assignment Cohorts\",\n    figsize=(6, 5),\n)\n\n\n\n\n\n\n\n\n\npf.panelview(\n    df_multi_cohort,\n    unit=\"unit\",\n    time=\"year\",\n    treat=\"treat\",\n    collapse_to_cohort=True,\n    sort_by_timing=True,\n    ylab=\"Cohort\",\n    xlab=\"Year\",\n    title=\"Treatment Assignment Cohorts\",\n    figsize=(6, 5),\n)\n\n\n\n\n\n\n\n\nWe immediately see that we have staggered adoption of treatment in the second case, which implies that a naive application of 2WFE might yield biased estimates under substantial effect heterogeneity.\nWe can also plot treatment assignment in a disaggregated fashion, which gives us a sense of cohort sizes.\n\npf.panelview(\n    df_multi_cohort,\n    unit=\"unit\",\n    time=\"year\",\n    treat=\"treat\",\n    sort_by_timing=True,\n    ylab=\"Unit\",\n    xlab=\"Year\",\n    title=\"Treatment Assignment (all units)\",\n    figsize=(6, 5),\n)"
  },
  {
    "objectID": "difference-in-differences.html#inspecting-the-outcome-variable",
    "href": "difference-in-differences.html#inspecting-the-outcome-variable",
    "title": "Difference-in-Differences Estimation",
    "section": "Inspecting the Outcome Variable",
    "text": "Inspecting the Outcome Variable\npf.panelview() further allows us to inspect the “outcome” variable over time:\n\npf.panelview(\n    df_multi_cohort,\n    outcome=\"dep_var\",\n    unit=\"unit\",\n    time=\"year\",\n    treat=\"treat\",\n    collapse_to_cohort=True,\n    title=\"Outcome Plot\",\n    legend=True,\n    figsize=(7, 2.5),\n)\n\n/Users/afischer/Documents/pyfixest/pyfixest/did/visualize.py:197: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(get_treatment_start)\n\n\n\n\n\n\n\n\n\nWe immediately see that the first cohort is switched into treatment in 2000, while the second cohort is switched into treatment by 2010. Before each cohort is switched into treatment, the trends are parallel.\nWe can additionally inspect individual units by dropping the collapse_to_cohort argument. Because we have a large sample, we might want to inspect only a subset of units.\n\npf.panelview(\n    df_multi_cohort,\n    outcome=\"dep_var\",\n    unit=\"unit\",\n    time=\"year\",\n    treat=\"treat\",\n    subsamp=100,\n    title = \"Outcome Plot\",\n    legend=True,\n    figsize=(7, 2.5),\n)"
  },
  {
    "objectID": "difference-in-differences.html#one-shot-adoption-static-and-dynamic-specifications",
    "href": "difference-in-differences.html#one-shot-adoption-static-and-dynamic-specifications",
    "title": "Difference-in-Differences Estimation",
    "section": "One-shot adoption: Static and Dynamic Specifications",
    "text": "One-shot adoption: Static and Dynamic Specifications\nAfter taking a first look at the data, let’s turn to estimation. We return to the df_one_cohort data set (without staggered treatment rollout).\n\nfit_static_twfe = pf.feols(\n    \"Y ~ treat | unit + year\",\n    df_one_cohort,\n    vcov={\"CRV1\": \"unit\"},\n)\nfit_static_twfe.summary()\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: unit+year\nInference:  CRV1\nObservations:  30000\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| treat         |      0.206 |        0.052 |     3.927 |      0.000 |  0.103 |   0.308 |\n---\nRMSE: 0.701 R2: 0.905 R2 Within: 0.003 \n\n\nSince this is a single-cohort dataset, this estimate is consistent for the ATT under parallel trends. We can estimate heterogeneous effects by time by interacting time with the treated group:\n\nfit_dynamic_twfe = pf.feols(\n    \"Y ~ i(year, ever_treated,  ref = 14) | unit + year\",\n    df_one_cohort,\n    vcov={\"CRV1\": \"unit\"},\n)\n\n\nfit_dynamic_twfe.iplot(\n    coord_flip=False,\n    title=\"Event Study\",\n    figsize=[1200, 400],\n    yintercept=0,\n    xintercept=13.5,\n    labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),\n)\n\n/var/folders/98/c353q4p95v5_fz62gr5c9td80000gn/T/ipykernel_93968/3569604189.py:7: DeprecationWarning: The function `_relabel_expvar` is deprecated as we have adjusted the naming of variables interacted via the i() operator with pyfixest 0.50. For regression tables, please rely on the `labels` and `cat_template` arguments of `pf.etable()` instead. \n  labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),\n\n\n   \n   \n\n\nEvent study plots like this are very informative, as they allow us to visually inspect the parallel trends assumption and also the dynamic effects of the treatment.\nBased on a cursory glance, one would conclude that parallel trends does not hold because one of the pre-treatment coefficient has a confidence interval that does not include zero. However, we know that parallel trends is true because the treatment is randomly assigned in the underlying DGP."
  },
  {
    "objectID": "difference-in-differences.html#pointwise-vs-simultaneous-inference-in-event-studies",
    "href": "difference-in-differences.html#pointwise-vs-simultaneous-inference-in-event-studies",
    "title": "Difference-in-Differences Estimation",
    "section": "Pointwise vs Simultaneous Inference in Event Studies",
    "text": "Pointwise vs Simultaneous Inference in Event Studies\nThis is an example of a false positive in testing for pre-trends produced by pointwise inference (where each element of the coefficient vector is tested separately).\nAs an alternative, we can use simultaneous confidence bands of the form \\([a, b] = ([a_k, b_k])_{k=1}^K\\) such that\n\\[\nP(\\beta \\in [a, b]) = P(\\beta_k \\in [a_k, b_k] \\forall k) \\rightarrow 1 - \\alpha\n\\]\nThese bands can be constructed by using a carefully chosen critical value \\(c\\) that accounts for the covariance between coefficients using the multiplier bootstrap. In pointwise inference, the critical value is \\(c = z_{1 - \\alpha/2} = 1.96\\) for \\(\\alpha = 0.05\\); the corresponding critical value for simultaneous inference is typically larger. These are also known as sup-t bands in the literature (see lec 3 of the NBER SI methods lectures linked above).\nThis is implemented in the confint(joint=True) method in the feols class. If we pass the joint='both' argument to iplot, we get the simultaneous confidence bands (for all event study coefficients) in addition to the pointwise confidence intervals. Note that simultaneous inference for all event study coefficients may be overly conservative, especially when the number of coefficients is large; one may instead choose to perform joint inference for all pre-treatment coefficients and all post-treatment coefficients separately.\n\nfit_dynamic_twfe.iplot(\n    coord_flip=False,\n    title=\"Event Study\",\n    figsize=[1200, 400],\n    yintercept=0,\n    xintercept=13.5,\n    joint=\"both\",\n    labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),\n)\n\n/var/folders/98/c353q4p95v5_fz62gr5c9td80000gn/T/ipykernel_93968/3057495514.py:8: DeprecationWarning: The function `_relabel_expvar` is deprecated as we have adjusted the naming of variables interacted via the i() operator with pyfixest 0.50. For regression tables, please rely on the `labels` and `cat_template` arguments of `pf.etable()` instead. \n  labels=rename_event_study_coefs(fit_dynamic_twfe._coefnames),\n\n\n   \n   \n\n\nThe joint confidence bands are wider than the pointwise confidence intervals, and they include zero for all pre-treatment coefficients. This is consistent with the parallel trends assumption."
  },
  {
    "objectID": "difference-in-differences.html#event-study-under-staggered-adoption-via-feols-event_study-did2s-lpdid",
    "href": "difference-in-differences.html#event-study-under-staggered-adoption-via-feols-event_study-did2s-lpdid",
    "title": "Difference-in-Differences Estimation",
    "section": "Event Study under Staggered Adoption via feols(), event_study(), did2s(), lpdid()",
    "text": "Event Study under Staggered Adoption via feols(), event_study(), did2s(), lpdid()\nWe now return to the data set with staggered treatment rollout, df_multi_cohort.\n\nTwo-Way Fixed Effects\nAs a baseline model, we can estimate a simple two-way fixed effects DiD regression via feols():\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(rel_year, ref=-1.0) | state + year\",\n    df_multi_cohort,\n    vcov={\"CRV1\": \"state\"},\n)\n\nYou can also estimate a TWFE model via the event_study() function, which aims to provide a common interface to multiple difference-in-differences implementations:\n\nfit_twfe_event = pf.event_study(\n    data=df_multi_cohort,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"twfe\",\n)\n\n\n\nFully-Interacted / Saturated Event Study (Sun-Abraham)\nIn a similar spirit, you can fit a fully-interacted difference-in-differences model by selecting the estimator = \"saturated\":\n\nfit_saturated = pf.event_study(\n    data=df_multi_cohort,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"saturated\",\n)\n\nfit_saturated.iplot()\n\n/Users/afischer/Documents/pyfixest/pyfixest/did/saturated_twfe.py:68: UserWarning: The SaturatedEventStudyClass is currently in beta. Please report any issues you may encounter.\n  warnings.warn(\n/Users/afischer/Documents/pyfixest/pyfixest/estimation/feols_.py:2756: UserWarning: \n            22 variables dropped due to multicollinearity.\n            The following variables are dropped: \n    C(rel_time, contr.treatment(base=-1.0))[-20.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-19.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-18.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-17.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-16.0]:cohort_dummy_2000\n    ....\n            \n  warnings.warn(\n\n\n\n\n\n\n\n\n\nWe can obtain treatment effects by period via the aggregate() method\n\nfit_saturated.aggregate(weighting = \"shares\")\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      period\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      -20.0\n      -0.073055\n      0.083868\n      -0.871071\n      0.383715\n      -0.237434\n      0.091324\n    \n    \n      -19.0\n      -0.005748\n      0.087254\n      -0.065871\n      0.94748\n      -0.176763\n      0.165268\n    \n    \n      -18.0\n      0.019863\n      0.084817\n      0.234181\n      0.814844\n      -0.146376\n      0.186101\n    \n    \n      -17.0\n      0.075561\n      0.08836\n      0.85515\n      0.392468\n      -0.097622\n      0.248744\n    \n    \n      -16.0\n      -0.051278\n      0.088165\n      -0.581611\n      0.560829\n      -0.224077\n      0.121522\n    \n    \n      -15.0\n      0.047891\n      0.085371\n      0.560977\n      0.574813\n      -0.119433\n      0.215215\n    \n    \n      -14.0\n      0.030061\n      0.085232\n      0.352697\n      0.724316\n      -0.13699\n      0.197112\n    \n    \n      -13.0\n      0.03987\n      0.082118\n      0.485517\n      0.62731\n      -0.121079\n      0.200818\n    \n    \n      -12.0\n      0.048458\n      0.087172\n      0.555888\n      0.578287\n      -0.122396\n      0.219312\n    \n    \n      -11.0\n      -0.01282\n      0.091618\n      -0.13993\n      0.888715\n      -0.192388\n      0.166748\n    \n    \n      -10.0\n      0.001529\n      0.06211\n      0.02461\n      0.980366\n      -0.120205\n      0.123262\n    \n    \n      -9.0\n      0.017902\n      0.062642\n      0.285777\n      0.775049\n      -0.104874\n      0.140677\n    \n    \n      -8.0\n      0.02942\n      0.062148\n      0.473384\n      0.635939\n      -0.092389\n      0.151229\n    \n    \n      -7.0\n      0.06927\n      0.06336\n      1.093268\n      0.274276\n      -0.054914\n      0.193453\n    \n    \n      -6.0\n      -0.000168\n      0.06252\n      -0.002687\n      0.997856\n      -0.122705\n      0.122369\n    \n    \n      -5.0\n      -0.042447\n      0.060507\n      -0.701529\n      0.482973\n      -0.161039\n      0.076144\n    \n    \n      -4.0\n      0.025822\n      0.061739\n      0.418243\n      0.67577\n      -0.095185\n      0.146828\n    \n    \n      -3.0\n      0.062146\n      0.061508\n      1.010385\n      0.312311\n      -0.058406\n      0.182699\n    \n    \n      -2.0\n      0.031255\n      0.06339\n      0.493059\n      0.621971\n      -0.092988\n      0.155498\n    \n    \n      0.0\n      1.38732\n      0.061787\n      22.453107\n      0.0\n      1.266219\n      1.508421\n    \n    \n      1.0\n      1.628963\n      0.063322\n      25.724928\n      0.0\n      1.504854\n      1.753073\n    \n    \n      2.0\n      1.686253\n      0.062372\n      27.035432\n      0.0\n      1.564007\n      1.8085\n    \n    \n      3.0\n      1.796582\n      0.063137\n      28.455425\n      0.0\n      1.672837\n      1.920328\n    \n    \n      4.0\n      1.934649\n      0.063037\n      30.690833\n      0.0\n      1.8111\n      2.058199\n    \n    \n      5.0\n      2.065588\n      0.061803\n      33.422231\n      0.0\n      1.944456\n      2.186719\n    \n    \n      6.0\n      2.166252\n      0.064074\n      33.808421\n      0.0\n      2.040668\n      2.291835\n    \n    \n      7.0\n      2.263413\n      0.06352\n      35.632879\n      0.0\n      2.138916\n      2.387911\n    \n    \n      8.0\n      2.358293\n      0.06232\n      37.841747\n      0.0\n      2.236149\n      2.480438\n    \n    \n      9.0\n      2.354538\n      0.056481\n      41.687038\n      0.0\n      2.243837\n      2.465239\n    \n    \n      10.0\n      2.515993\n      0.060297\n      41.72651\n      0.0\n      2.397813\n      2.634174\n    \n    \n      11.0\n      2.552978\n      0.092825\n      27.503048\n      0.0\n      2.371044\n      2.734912\n    \n    \n      12.0\n      2.603939\n      0.090658\n      28.722574\n      0.0\n      2.426252\n      2.781625\n    \n    \n      13.0\n      2.578044\n      0.093739\n      27.502403\n      0.0\n      2.394319\n      2.761769\n    \n    \n      14.0\n      2.722969\n      0.092043\n      29.583642\n      0.0\n      2.542568\n      2.90337\n    \n    \n      15.0\n      2.909275\n      0.092438\n      31.472726\n      0.0\n      2.7281\n      3.09045\n    \n    \n      16.0\n      2.859325\n      0.092643\n      30.864025\n      0.0\n      2.677749\n      3.040901\n    \n    \n      17.0\n      2.907943\n      0.093763\n      31.013703\n      0.0\n      2.724171\n      3.091716\n    \n    \n      18.0\n      2.873678\n      0.093009\n      30.896721\n      0.0\n      2.691384\n      3.055973\n    \n    \n      19.0\n      2.882807\n      0.09253\n      31.155532\n      0.0\n      2.701453\n      3.064162\n    \n    \n      20.0\n      2.911501\n      0.091093\n      31.961938\n      0.0\n      2.732962\n      3.09004\n    \n  \n\n\n\n\nand plot the effects\n\nfit_saturated.iplot_aggregate(weighting = \"shares\")\n\n/Users/afischer/Documents/pyfixest/pyfixest/did/saturated_twfe.py:271: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\n\n\n\n\n\n\nWhen can we get away with using the two-way fixed effects regression?\nWe will motivate this section by lazily quoting the abstract of Lal (2025):\n\nThe use of the two-way fixed effects regression in empirical social science was historically motivated by folk wisdom that it uncovers the Average Treatment effect on the Treated (ATT) as in the canonical two-period two-group case. This belief has come under scrutiny recently due to recent results in applied econometrics showing that it fails to uncover meaningful averages of heterogeneous treatment effects in the presence of effect heterogeneity over time and across adoption cohorts, and several heterogeneity-robust alternatives have been proposed. However, these estimators often have higher variance and are therefore under-powered for many applications, which poses a bias-variance tradeoff that is challenging for researchers to navigate. In this paper, we propose simple tests of linear restrictions that can be used to test for differences in dynamic treatment effects over cohorts, which allows us to test for when the two-way fixed effects regression is likely to yield biased estimates of the ATT.\n\nYou can employ the proposed test after running a saturated event study by calling the test_treatment_heterogeneity() method:\n\nfit_saturated.test_treatment_heterogeneity()\n\nstatistic    3256.497337\npvalue          0.000000\ndtype: float64\n\n\nIn this case, we might be willing to rely on the simple TWFE model to produce unbiased estimates. If we’re not, two “new” difference-in-differences estimators are implemented (beyond the already-presented saturated Sun-Abraham approach) that produce unbiased estimates under staggered assignment and heterogeneous treatment effects: Gardner’s 2-Step Estimator and the Local Projections estimator from Dube et al.\n\n\nGardner’s 2-Step Estimator\nTo do the same via Gardners 2-stage estimator, we employ the the pf.did2s() function:\n\nfit_did2s = pf.did2s(\n    df_multi_cohort,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | unit + year\",\n    second_stage=\"~i(rel_year,ref=-1.0)\",\n    treatment=\"treat\",\n    cluster=\"state\",\n)\n\n\n\nLocal Projections (Dube et al)\nLast, we can estimate the ATT for each time period via local projections by using the lpdid() function:\n\nfit_lpdid = pf.lpdid(\n    data=df_multi_cohort,\n    yname=\"dep_var\",\n    gname=\"g\",\n    tname=\"year\",\n    idname=\"unit\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-20,\n    post_window=20,\n    att=False,\n)\n\nLet’s look at some results:\n\nfigsize = [1200, 400]\n\n\nfit_twfe.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=figsize,\n    xintercept=18.5,\n    yintercept=0,\n    labels=rename_event_study_coefs(fit_twfe._coefnames),  # rename coefficients\n).show()\n\n/var/folders/98/c353q4p95v5_fz62gr5c9td80000gn/T/ipykernel_93968/383819466.py:7: DeprecationWarning: The function `_relabel_expvar` is deprecated as we have adjusted the naming of variables interacted via the i() operator with pyfixest 0.50. For regression tables, please rely on the `labels` and `cat_template` arguments of `pf.etable()` instead. \n  labels=rename_event_study_coefs(fit_twfe._coefnames),  # rename coefficients\n\n\n   \n   \n\n\n\nfit_lpdid.iplot(\n    coord_flip=False,\n    title=\"Local-Projections-Estimator\",\n    figsize=figsize,\n    yintercept=0,\n    xintercept=18.5,\n).show()\n\n   \n   \n\n\nWhat if we are not interested in the ATT per treatment period, but in a pooled effects?\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(treat) | unit + year\",\n    df_multi_cohort,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfit_did2s = pf.did2s(\n    df_multi_cohort,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | unit + year\",\n    second_stage=\"~i(treat)\",\n    treatment=\"treat\",\n    cluster=\"state\",\n)\n\nfit_lpdid = pf.lpdid(\n    data=df_multi_cohort,\n    yname=\"dep_var\",\n    gname=\"g\",\n    tname=\"year\",\n    idname=\"unit\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-20,\n    post_window=20,\n    att=True,\n)\npd.concat(\n    [\n        fit_twfe.tidy().assign(estimator=\"TWFE\"),\n        fit_did2s.tidy().assign(estimator=\"DID2s\"),\n        fit_lpdid.tidy().assign(estimator=\"LPDID\").drop(\"N\", axis=1),\n    ],\n    axis=0,\n)\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n      estimator\n    \n  \n  \n    \n      C(treat)[T.True]\n      1.982540\n      0.019338\n      102.521988\n      0.0\n      1.943426\n      2.021655\n      TWFE\n    \n    \n      C(treat)[T.True]\n      2.230482\n      0.024709\n      90.271446\n      0.0\n      2.182052\n      2.278911\n      DID2s\n    \n    \n      treat_diff\n      2.506746\n      0.071420\n      35.098900\n      0.0\n      2.362287\n      2.651206\n      LPDID"
  },
  {
    "objectID": "replicating-the-effect.html",
    "href": "replicating-the-effect.html",
    "title": "Replicating Examples from “The Effect”",
    "section": "",
    "text": "This notebook replicates code examples from Nick Huntington-Klein’s book on causal inference, The Effect.\nfrom causaldata import Mroz, gapminder, organ_donations, restaurant_inspections\n\nimport pyfixest as pf\n\n%load_ext watermark\n%watermark --iversions\n\npyfixest  : 0.40.1\ncausaldata: 0.1.5"
  },
  {
    "objectID": "replicating-the-effect.html#chapter-4-describing-relationships",
    "href": "replicating-the-effect.html#chapter-4-describing-relationships",
    "title": "Replicating Examples from “The Effect”",
    "section": "Chapter 4: Describing Relationships",
    "text": "Chapter 4: Describing Relationships\n\n# Read in data\ndt = Mroz.load_pandas().data\n# Keep just working women\ndt = dt.query(\"lfp\")\n# Create unlogged earnings\ndt.loc[:, \"earn\"] = dt[\"lwg\"].apply(\"exp\")\n\n# 5. Run multiple linear regression models by succesively adding controls\nfit = pf.feols(fml=\"lwg ~ csw(inc, wc, k5)\", data=dt, vcov=\"iid\")\npf.etable(fit)\n\n/var/folders/98/c353q4p95v5_fz62gr5c9td80000gn/T/ipykernel_94009/786816010.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt.loc[:, \"earn\"] = dt[\"lwg\"].apply(\"exp\")\n\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n  \n  \n    lwg\n  \n\n\n  (1)\n  (2)\n  (3)\n\n\n\n  \n    coef\n  \n  \n    inc\n    0.01**  (0.003)\n    0.005  (0.003)\n    0.005  (0.003)\n  \n  \n    wc\n    \n    0.342***  (0.075)\n    0.349***  (0.075)\n  \n  \n    k5\n    \n    \n    -0.072  (0.087)\n  \n  \n    Intercept\n    1.007***  (0.071)\n    0.972***  (0.07)\n    0.982***  (0.071)\n  \n  \n    stats\n  \n  \n    Observations\n    428\n    428\n    428\n  \n  \n    R2\n    0.02\n    0.066\n    0.068\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "replicating-the-effect.html#chapter-13-regression",
    "href": "replicating-the-effect.html#chapter-13-regression",
    "title": "Replicating Examples from “The Effect”",
    "section": "Chapter 13: Regression",
    "text": "Chapter 13: Regression\n\nExample 1\n\nres = restaurant_inspections.load_pandas().data\nres.inspection_score = res.inspection_score.astype(float)\nres.NumberofLocations = res.NumberofLocations.astype(float)\nres.dtypes\n\nfit = pf.feols(fml=\"inspection_score ~ NumberofLocations\", data=res)\npf.etable([fit])\n\n\n\n\n\n\n\n  \n  \n    inspection_score\n  \n\n\n  (1)\n\n\n\n  \n    coef\n  \n  \n    NumberofLocations\n    -0.019***  (0.000436)\n  \n  \n    Intercept\n    94.866***  (0.046)\n  \n  \n    stats\n  \n  \n    Observations\n    27,178\n  \n  \n    R2\n    0.065\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\n\n\nExample 2\n\ndf = restaurant_inspections.load_pandas().data\n\nfit1 = pf.feols(\n    fml=\"inspection_score ~ NumberofLocations + I(NumberofLocations^2) + Year\", data=df\n)\nfit2 = pf.feols(fml=\"inspection_score ~ NumberofLocations*Weekend + Year\", data=df)\n\npf.etable([fit1, fit2])\n\n\n\n\n\n\n\n  \n  \n    inspection_score\n  \n\n\n  (1)\n  (2)\n\n\n\n  \n    coef\n  \n  \n    NumberofLocations\n    -0.075***  (0.019)\n    -0.019***  (0.000437)\n  \n  \n    I(NumberofLocations ^ 2)\n    0.056**  (0.019)\n    \n  \n  \n    Year\n    -0.065***  (0.006)\n    -0.065***  (0.006)\n  \n  \n    Weekend\n    \n    1.759***  (0.488)\n  \n  \n    NumberofLocations × Weekend\n    \n    -0.01  (0.008)\n  \n  \n    Intercept\n    225.504***  (12.409)\n    225.126***  (12.415)\n  \n  \n    stats\n  \n  \n    Observations\n    27,178\n    27,178\n  \n  \n    R2\n    0.069\n    0.069\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\n\n\nExample 3: HC Standard Errors\n\npf.feols(fml=\"inspection_score ~ Year + Weekend\", data=df, vcov=\"HC3\").summary()\n\n###\n\nEstimation:  OLS\nDep. var.: inspection_score, Fixed effects: 0\nInference:  HC3\nObservations:  27178\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |    2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|--------:|--------:|\n| Intercept     |    185.380 |       12.150 |    15.257 |      0.000 | 161.564 | 209.196 |\n| Year          |     -0.046 |        0.006 |    -7.551 |      0.000 |  -0.057 |  -0.034 |\n| Weekend       |      2.057 |        0.353 |     5.829 |      0.000 |   1.365 |   2.749 |\n---\nRMSE: 6.248 R2: 0.003 \n\n\n\n\nExample 4: Clustered Standard Errors\n\npf.feols(\n    fml=\"inspection_score ~ Year + Weekend\", data=df, vcov={\"CRV1\": \"Weekend\"}\n).tidy()\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Intercept\n      185.380033\n      3.264345\n      56.789344\n      0.011209\n      143.902592\n      226.857474\n    \n    \n      Year\n      -0.045640\n      0.001624\n      -28.107556\n      0.022640\n      -0.066272\n      -0.025008\n    \n    \n      Weekend\n      2.057166\n      0.001401\n      1468.256802\n      0.000434\n      2.039364\n      2.074969\n    \n  \n\n\n\n\n\n\nExample 5: Bootstrap Inference\n\nfit = pf.feols(fml=\"inspection_score ~ Year + Weekend\", data=df)\nfit.wildboottest(reps=999, param=\"Year\")\n\nparam                 Year\nt value           -7.55233\nPr(&gt;|t|)               0.0\nbootstrap_type          11\ninference               HC\nimpose_null           True\nssc               1.000074\ndtype: object"
  },
  {
    "objectID": "replicating-the-effect.html#chapter-16-fixed-effects",
    "href": "replicating-the-effect.html#chapter-16-fixed-effects",
    "title": "Replicating Examples from “The Effect”",
    "section": "Chapter 16: Fixed Effects",
    "text": "Chapter 16: Fixed Effects\n\nExample 1\ntba\n\n\nExample 2\n\ngm = gapminder.load_pandas().data\ngm[\"logGDPpercap\"] = gm[\"gdpPercap\"].apply(\"log\")\n\nfit = pf.feols(fml=\"lifeExp ~ C(country) + np.log(gdpPercap)\", data=gm)\nfit.tidy().head()\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Intercept\n      -27.773459\n      2.500533\n      -11.107015\n      0.000000e+00\n      -32.678217\n      -22.868701\n    \n    \n      C(country)[T.Albania]\n      17.782625\n      2.195160\n      8.100835\n      1.110223e-15\n      13.476853\n      22.088397\n    \n    \n      C(country)[T.Algeria]\n      5.241055\n      2.214496\n      2.366704\n      1.806875e-02\n      0.897356\n      9.584755\n    \n    \n      C(country)[T.Angola]\n      -13.907122\n      2.201727\n      -6.316460\n      3.481857e-10\n      -18.225777\n      -9.588468\n    \n    \n      C(country)[T.Argentina]\n      8.132158\n      2.272781\n      3.578065\n      3.567229e-04\n      3.674133\n      12.590183\n    \n  \n\n\n\n\n\n\nExample 3: TWFE\n\n# Set our individual and time (index) for our data\nfit = pf.feols(fml=\"lifeExp ~ np.log(gdpPercap) | country + year\", data=gm)\nfit.summary()\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n###\n\nEstimation:  OLS\nDep. var.: lifeExp, Fixed effects: country+year\nInference:  iid\nObservations:  1704\n\n| Coefficient       |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| np.log(gdpPercap) |      1.450 |        0.268 |     5.419 |      0.000 |  0.925 |   1.975 |\n---\nRMSE: 3.267 R2: 0.936 R2 Within: 0.019"
  },
  {
    "objectID": "replicating-the-effect.html#chapter-18-difference-in-differences",
    "href": "replicating-the-effect.html#chapter-18-difference-in-differences",
    "title": "Replicating Examples from “The Effect”",
    "section": "Chapter 18: Difference-in-Differences",
    "text": "Chapter 18: Difference-in-Differences\n\nExample 1\n\nod = organ_donations.load_pandas().data\n\n# Create Treatment Variable\nod[\"California\"] = od[\"State\"] == \"California\"\nod[\"After\"] = od[\"Quarter_Num\"] &gt; 3\nod[\"Treated\"] = 1 * (od[\"California\"] & od[\"After\"])\n\ndid = pf.feols(fml=\"Rate ~ Treated | State + Quarter\", data=od)\ndid.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Rate, Fixed effects: State+Quarter\nInference:  iid\nObservations:  162\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Treated       |     -0.022 |        0.020 |    -1.096 |      0.275 | -0.063 |   0.018 |\n---\nRMSE: 0.022 R2: 0.979 R2 Within: 0.009 \n\n\n\n\nExample 3: Dynamic Treatment Effect\n\nod = organ_donations.load_pandas().data\n\n# Create Treatment Variable\nod[\"California\"] = od[\"State\"] == \"California\"\n# od[\"Quarter_Num\"] = pd.Categorical(od.Quarter_Num)\nod[\"California\"] = od.California.astype(float)\n\ndid2 = pf.feols(\n    fml=\"Rate ~ i(Quarter_Num, California,ref=3) | State + Quarter_Num\", data=od\n)\n\ndid2.tidy()\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C(Quarter_Num, contr.treatment(base=3))[1]:California\n      -0.002942\n      0.036055\n      -0.081606\n      0.935090\n      -0.074299\n      0.068415\n    \n    \n      C(Quarter_Num, contr.treatment(base=3))[2]:California\n      0.006296\n      0.036055\n      0.174627\n      0.861655\n      -0.065061\n      0.077653\n    \n    \n      C(Quarter_Num, contr.treatment(base=3))[4]:California\n      -0.021565\n      0.036055\n      -0.598127\n      0.550837\n      -0.092922\n      0.049792\n    \n    \n      C(Quarter_Num, contr.treatment(base=3))[5]:California\n      -0.020292\n      0.036055\n      -0.562817\n      0.574567\n      -0.091649\n      0.051065\n    \n    \n      C(Quarter_Num, contr.treatment(base=3))[6]:California\n      -0.022165\n      0.036055\n      -0.614768\n      0.539825\n      -0.093522\n      0.049192"
  },
  {
    "objectID": "quantile-regression.html",
    "href": "quantile-regression.html",
    "title": "Quantile Regression",
    "section": "",
    "text": "PyFixest now experimentally supports quantile regression!\n%load_ext autoreload\n\nimport pyfixest as pf\ndata = pf.get_data()"
  },
  {
    "objectID": "quantile-regression.html#basic-example",
    "href": "quantile-regression.html#basic-example",
    "title": "Quantile Regression",
    "section": "Basic Example",
    "text": "Basic Example\nJust as in statsmodels, the function that runs a quantile regression is quantreg().\nBelow, we loop over 10 different quantiles.\n\n%%capture\nfits = pf.quantreg(\n  fml = \"Y~X1 + X2\",\n  data = data,\n  quantile=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n)\n\nWe can inspect the quantile regression results using the dedicated qplot() function.\n\npf.qplot(fits, nrow = 2)\n\n(&lt;Figure size 960x576 with 4 Axes&gt;,\n array([&lt;Axes: title={'center': 'Intercept'}, xlabel='Quantile', ylabel='Coefficient (95 % CI)'&gt;,\n        &lt;Axes: title={'center': 'X1'}, xlabel='Quantile', ylabel='Coefficient (95 % CI)'&gt;,\n        &lt;Axes: title={'center': 'X2'}, xlabel='Quantile', ylabel='Coefficient (95 % CI)'&gt;,\n        &lt;Axes: &gt;], dtype=object))\n\n\n\n\n\n\n\n\n\nWe observe some heterogeneity in the intercept, but all other variants are homogeneous across users."
  },
  {
    "objectID": "quantile-regression.html#solvers",
    "href": "quantile-regression.html#solvers",
    "title": "Quantile Regression",
    "section": "Solvers",
    "text": "Solvers\nBy default, pf.quantreg uses an interior-point solver as in Koenker and Ng (2004) (methd = \"fn\"). This is different to e.g. statsmodels, which implements an iterated weighted least squares solver.\nFor big data sets with many observations, it is often sensible to use an interior-point solver with pre-processing (as in Portnoy and Koenker (1997), see Chernozhukov et al (2019) for details), which can speed up the estimation time significantly. Because the pre-processing step requires taking a random sample, the method assumes that observations are independent. Additionally, for the purpose of reproducibility, it is advisable to set a seed.\nYou can access the “preprocessing frisch-newton” algorithm by setting the method argument to \"pfn\":\n\n%%capture\nfit_fn = pf.quantreg(\n  fml = \"Y ~ X1\",\n  method = \"fn\",     # standard frisch newton interior point solver\n  data = data,\n)\nfit_pfn = pf.quantreg(\n  fml = \"Y ~ X1\",\n  method = \"pfn\",   # standard frisch newton interior point solver with pre-processing\n  seed = 92,         # set a seed for reproducibility\n  data = data,\n)\n\npf.etable([fit_fn, fit_pfn])"
  },
  {
    "objectID": "quantile-regression.html#quantile-regression-process",
    "href": "quantile-regression.html#quantile-regression-process",
    "title": "Quantile Regression",
    "section": "Quantile Regression Process",
    "text": "Quantile Regression Process\nInstead of running multiple independent quantile regression via a for-loop, the literature on quantile regression has developed multiple algorithms to speed up the “quantile regression process”. Two such algorithms are described in detail in Chernozhukov, Fernandez-Val and Melly (2019) and are implemented in PyFixest. They can be accessed via the multi_method argument, and both can significantly speed up estimation time of the full quantile regression process.\n\nfml = \"Y~X1\"\nmethod = \"pfn\"\nseed = 929\nquantiles = [0.1, 0.5, 0.9]\n\nfit_multi1 = pf.quantreg(\n  fml = fml,\n  data = data,\n  method = method,\n  multi_method = \"cfm1\",  # this is algorithm 2 in CFM, the 1rst algorithm for the full qr process\n  seed = seed,\n  quantile = quantiles,\n)\n\nfit_multi2 = pf.quantreg(\n  fml = fml,\n  data = data,\n  method = method,\n  multi_method = \"cfm2\",  # this is algorithm 3 in CFM, the 2nd algorithm for the full qr process\n  seed = seed,\n  quantile = quantiles\n)\n\npf.etable(fit_multi1.to_list() +  fit_multi2.to_list())\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.96***  (0.161)\n    -1.091***  (0.107)\n    -0.796***  (0.121)\n    -1.091***  (0.161)\n    -1.091***  (0.107)\n    -1.091***  (0.121)\n  \n  \n    Intercept\n    -1.935***  (0.231)\n    1.024***  (0.145)\n    3.505***  (0.157)\n    -1.354***  (0.231)\n    1.024***  (0.145)\n    3.403***  (0.157)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n    998\n    998\n    998\n    998\n  \n  \n    R2\n    -\n    -\n    -\n    -\n    -\n    -\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nNote that the first method cfm1 is exactly identical to running separate regressions per quantile, while the second method cfm2 is only asymptotically identical.\nYou can combine different estimation method’s with different multi_methods:\n\nfit_multi2a = pf.quantreg(\n  fml = \"Y~X1\",\n  data = data,\n  method = \"fn\",\n  multi_method = \"cfm1\",\n  seed = 233,\n  quantile = [0.25, 0.75]\n)\n\nfit_multi2b = pf.quantreg(\n  fml = \"Y~X1\",\n  data = data,\n  method = \"pfn\",\n  multi_method = \"cfm1\",\n  seed = 233,\n  quantile = [0.25, 0.75]\n)\n\npf.etable(fit_multi2a.to_list() +  fit_multi2b.to_list())\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.851***  (0.109)\n    -1.017***  (0.126)\n    -0.851***  (0.109)\n    -1.017***  (0.126)\n  \n  \n    Intercept\n    -0.782***  (0.132)\n    2.495***  (0.169)\n    -0.782***  (0.132)\n    2.495***  (0.169)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n    998\n    998\n  \n  \n    R2\n    -\n    -\n    -\n    -\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "quantile-regression.html#inference",
    "href": "quantile-regression.html#inference",
    "title": "Quantile Regression",
    "section": "Inference",
    "text": "Inference\nBy default, the \"iid\", \"hetero\" and cluster robust variance estimators implement (sandwich) estimators as in Powell (1991), using a uniform kernel to estimate the “sparsity”.\nThe cluster robust estimator follows Parente & Santos Silva. See this slide set or the Journal of Econometrics paper for details.\nAdditionally, pf.quantreg supports the \"nid\" (“non-iid”) estimator from Hendricks and Koenker (1991), which uses a linear approximation of the conditional quantile function.\n\nfit_nid = pf.quantreg(\"Y ~ X1 + X2 + f1\", data = data, quantile = 0.5, vcov = \"nid\")\nfit_crv = pf.quantreg(\"Y ~ X1 + X2 + f1\", data = data, quantile = 0.5, vcov = {\"CRV1\": \"f1\"})\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn("
  },
  {
    "objectID": "quantile-regression.html#performance",
    "href": "quantile-regression.html#performance",
    "title": "Quantile Regression",
    "section": "Performance",
    "text": "Performance\nHere we benchmark the performance of the solvers accessible via the method and multi_method arguments.\n\nDifferent Solvers\nTba.\n\n\nQuantile Regression Process\nWe fit a quantile regression process on \\(q = 0.1, 0.2, ..., 0.9\\) quantiles and vary sample size and number of covariates. We test pyfixest’s implementation of the quantile regression process against a “naive” for loop implementation via statsmodels. We can see that both multi_method = \"cmf1\" and multi_method = \"cmf2\" outperform the for-loop strategy for large problems. Note that the plot is in log-scale!"
  },
  {
    "objectID": "stata-2-pyfixest.html",
    "href": "stata-2-pyfixest.html",
    "title": "Translating Stata to PyFixest",
    "section": "",
    "text": "This guide will focus on how to replicate the regression results you would get in Stata with the Python package pyfixest and assumes you know how to do things like install Python packages and load data into Pandas. For a broader introduction to doing econmetrics in Python you might check out Arthur Turrell’s Coding for Economist, which includes a section on Coming from Stata, or Tidy Finance with Python by Christopher Scheuch, Stefan Voigt, Patrick Weiss, and Christoph Frey. You can also check out the fixest section of stata2r as there is a lot of overlap between fixest and PyFixest syntax."
  },
  {
    "objectID": "stata-2-pyfixest.html#robust-standard-errors",
    "href": "stata-2-pyfixest.html#robust-standard-errors",
    "title": "Translating Stata to PyFixest",
    "section": "Robust Standard Errors",
    "text": "Robust Standard Errors\nTo get heteroskedasticity robust standard errors you can use\n\nfit3 = pf.feols(\"Y ~ X1 + X2\", data=df, vcov=\"HC1\")\nfit3.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.889 |        0.108 |     8.249 |      0.000 |  0.677 |   1.100 |\n| X1            |     -0.993 |        0.080 |   -12.439 |      0.000 | -1.150 |  -0.836 |\n| X2            |     -0.176 |        0.022 |    -8.129 |      0.000 | -0.219 |  -0.134 |\n---\nRMSE: 2.09 R2: 0.177 \n\n\nwhich is equivalent to\nreg Y X1 X2, robust\n\n* Linear regression                               Number of obs     =        998\n*                                                 F(2, 995)         =     107.91\n*                                                 Prob &gt; F          =     0.0000\n*                                                 R-squared         =     0.1770\n*                                                 Root MSE          =     2.0936\n*\n* ------------------------------------------------------------------------------\n*              |               Robust\n*            Y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n* -------------+----------------------------------------------------------------\n*           X1 |  -.9929358   .0798259   -12.44   0.000    -1.149582   -.8362893\n*           X2 |  -.1763424   .0216936    -8.13   0.000    -.2189129   -.1337719\n*        _cons |   .8887791   .1077457     8.25   0.000     .6773442    1.100214\n* ------------------------------------------------------------------------------\nor\nreg Y X1 X2, vce(robust)\n\n* Identical output to above\nor you can choose a different type of robust standard errors like “HC3” using\n\nfit4 = pf.feols(\"Y ~ X1 + X2\", data=df, vcov=\"HC3\")\nfit4.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC3\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.889 |        0.108 |     8.222 |      0.000 |  0.677 |   1.101 |\n| X1            |     -0.993 |        0.080 |   -12.396 |      0.000 | -1.150 |  -0.836 |\n| X2            |     -0.176 |        0.022 |    -8.087 |      0.000 | -0.219 |  -0.134 |\n---\nRMSE: 2.09 R2: 0.177 \n\n\nNote: This will not exactly match the output of the equivalent Stata command, which is\nreg Y X1 X2, vce(hc3)\n\n* Linear regression                               Number of obs     =        998\n*                                                 F(2, 995)         =     107.38\n*                                                 Prob &gt; F          =     0.0000\n*                                                 R-squared         =     0.1770\n*                                                 Root MSE          =     2.0936\n*\n* ------------------------------------------------------------------------------\n*              |             Robust HC3\n*            Y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n* -------------+----------------------------------------------------------------\n*           X1 |  -.9929358   .0799832   -12.41   0.000    -1.149891   -.8359807\n*           X2 |  -.1763424   .0217734    -8.10   0.000    -.2190693   -.1336154\n*        _cons |   .8887791   .1079372     8.23   0.000     .6769684     1.10059\n* ------------------------------------------------------------------------------\nthis is because by default, pyfixest uses two small sample size corrections for HC3 robust standard errors, while Stata only uses one of them. You can turn off the correction that Stata doesn’t use with the ssc argument.\n\nfit5 = pf.feols(\"Y ~ X1 + X2\", data=df, vcov=\"HC3\", ssc=pf.ssc(k_adj = False))\nfit5.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC3\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.889 |        0.108 |     8.234 |      0.000 |  0.677 |   1.101 |\n| X1            |     -0.993 |        0.080 |   -12.414 |      0.000 | -1.150 |  -0.836 |\n| X2            |     -0.176 |        0.022 |    -8.099 |      0.000 | -0.219 |  -0.134 |\n---\nRMSE: 2.09 R2: 0.177 \n\n\nwhich matches Stata exactly. You can read all about the small sample size corrections implememnted by pyfixest at On Small Sample Corrections."
  },
  {
    "objectID": "stata-2-pyfixest.html#clustered-standard-errors",
    "href": "stata-2-pyfixest.html#clustered-standard-errors",
    "title": "Translating Stata to PyFixest",
    "section": "Clustered Standard Errors",
    "text": "Clustered Standard Errors\nTo cluster the standard errors by group you can use\n\nfit6 = pf.feols(\"Y ~ X1 + X2\", data=df.dropna(subset=['f1']), vcov={\"CRV1\": \"f1\"})\nfit6.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.890 |        0.262 |     3.393 |      0.002 |  0.353 |   1.426 |\n| X1            |     -0.995 |        0.076 |   -13.142 |      0.000 | -1.150 |  -0.840 |\n| X2            |     -0.177 |        0.020 |    -8.920 |      0.000 | -0.217 |  -0.136 |\n---\nRMSE: 2.091 R2: 0.177 \n\n\nwhich is equivalent to\nreg Y X1 X2, vce(cluster f1)\n\n* Linear regression                               Number of obs     =        997\n*                                                 F(2, 29)          =     102.51\n*                                                 Prob &gt; F          =     0.0000\n*                                                 R-squared         =     0.1774\n*                                                 Root MSE          =      2.094\n*\n*                                     (Std. err. adjusted for 30 clusters in f1)\n* ------------------------------------------------------------------------------\n*              |               Robust\n*            Y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n* -------------+----------------------------------------------------------------\n*           X1 |  -.9951969   .0757246   -13.14   0.000    -1.150071   -.8403227\n*           X2 |  -.1766173    .019799    -8.92   0.000    -.2171109   -.1361237\n*        _cons |   .8895569   .2622066     3.39   0.002     .3532841     1.42583\n* ------------------------------------------------------------------------------\nNote: clustered standard errors are not supported with missing values in the cluster variable, which is why we drop the rows with missing values for f1.\nFor two way clustering you would need to use\n\nfit7 = pf.feols(\n  \"Y ~ X1 + X2\",\n  data=df.dropna(subset=['f1', 'f2']),\n  vcov={\"CRV1\": \"f1 + f2\"}\n)\nfit7.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.890 |        0.301 |     2.958 |      0.006 |  0.274 |   1.505 |\n| X1            |     -0.995 |        0.073 |   -13.570 |      0.000 | -1.145 |  -0.845 |\n| X2            |     -0.177 |        0.023 |    -7.535 |      0.000 | -0.225 |  -0.129 |\n---\nRMSE: 2.091 R2: 0.177 \n\n\nNote: This will not exactly match the output of the equivalent Stata command, which is\nreg Y X1 X2, vce(cluster f1 f2)\n\n* Linear regression                                       Number of obs =    997\n* Clusters per comb.:                                     Cluster comb. =      3\n*   min =  30                                             F(2, 29)      =  88.58\n*   avg = 211                                             Prob &gt; F      = 0.0000\n*   max = 572                                             R-squared     = 0.1774\n*                                                         Adj R-squared = 0.1758\n*                                                         Root MSE      = 2.0940\n*\n*                                   (Std. err. adjusted for multiway clustering)\n* ------------------------------------------------------------------------------\n*              |               Robust\n*            Y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n* -------------+----------------------------------------------------------------\n*           X1 |  -.9951969    .074771   -13.31   0.000    -1.148121   -.8422731\n*           X2 |  -.1766173     .02376    -7.43   0.000     -.225212   -.1280226\n*        _cons |   .8895569   .3016464     2.95   0.006     .2726208    1.506493\n* ------------------------------------------------------------------------------\n* Cluster combinations formed by f1 and f2.\nthis is because by default, pyfixest uses a small sample size correction that adjusts each clustering dimentsion by whichever dimension has the smallest number of clusters, while in Stata the default is to adjust each dimension based on the number of clusters in that dimension. You can use the same correction as Stata through the ssc argument.\n\nfit8 = pf.feols(\n  \"Y ~ X1 + X2\",\n  df.dropna(subset=['f1', 'f2']),\n  vcov={\"CRV1\": \"f1 + f2\"},\n  ssc=pf.ssc(G_df=\"conventional\")\n)\nfit8.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  CRV1\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.890 |        0.302 |     2.949 |      0.006 |  0.273 |   1.506 |\n| X1            |     -0.995 |        0.075 |   -13.310 |      0.000 | -1.148 |  -0.842 |\n| X2            |     -0.177 |        0.024 |    -7.433 |      0.000 | -0.225 |  -0.128 |\n---\nRMSE: 2.091 R2: 0.177 \n\n\nAs a reminder, for an excellent breakdown on small sample correction in the pyfixest package, you can check out On Small Sample Correction."
  },
  {
    "objectID": "reference/estimation.demean.html",
    "href": "reference/estimation.demean.html",
    "title": "estimation.demean",
    "section": "",
    "text": "estimation.demean(x, flist, weights, tol=1e-08, maxiter=100000)\nDemean an array.\nWorkhorse for demeaning an input array x based on the specified fixed effects and weights via the alternating projections algorithm.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.demean"
    ]
  },
  {
    "objectID": "reference/estimation.demean.html#parameters",
    "href": "reference/estimation.demean.html#parameters",
    "title": "estimation.demean",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nnumpy.ndarray\nInput array of shape (n_samples, n_features). Needs to be of type float.\nrequired\n\n\nflist\nnumpy.ndarray\nArray of shape (n_samples, n_factors) specifying the fixed effects. Needs to already be converted to integers.\nrequired\n\n\nweights\nnumpy.ndarray\nArray of shape (n_samples,) specifying the weights.\nrequired\n\n\ntol\nfloat\nTolerance criterion for convergence. Defaults to 1e-08.\n1e-08\n\n\nmaxiter\nint\nMaximum number of iterations. Defaults to 100_000.\n100000",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.demean"
    ]
  },
  {
    "objectID": "reference/estimation.demean.html#returns",
    "href": "reference/estimation.demean.html#returns",
    "title": "estimation.demean",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[numpy.ndarray, bool]\nA tuple containing the demeaned array of shape (n_samples, n_features) and a boolean indicating whether the algorithm converged successfully.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.demean"
    ]
  },
  {
    "objectID": "reference/estimation.demean.html#examples",
    "href": "reference/estimation.demean.html#examples",
    "title": "estimation.demean",
    "section": "Examples",
    "text": "Examples\n\nimport numpy as np\nimport pyfixest as pf\nfrom pyfixest.utils.dgps import get_blw\nfrom pyfixest.estimation.demean_ import demean\nfrom formulaic import model_matrix\n\nfml = \"y ~ treat | state + year\"\n\ndata = get_blw()\ndata.head()\n\nY, rhs = model_matrix(fml, data)\nX = rhs[0].drop(columns=\"Intercept\")\nfe = rhs[1].drop(columns=\"Intercept\")\nYX = np.concatenate([Y, X], axis=1)\n\n# to numpy\nY = Y.to_numpy()\nX = X.to_numpy()\nYX = np.concatenate([Y, X], axis=1)\nfe = fe.to_numpy().astype(int)  # demean requires fixed effects as ints!\n\nYX_demeaned, success = demean(YX, fe, weights = np.ones(YX.shape[0]))\nY_demeaned = YX_demeaned[:, 0]\nX_demeaned = YX_demeaned[:, 1:]\n\nprint(np.linalg.lstsq(X_demeaned, Y_demeaned, rcond=None)[0])\nprint(pf.feols(fml, data).coef())\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n[-6.70829589]\nCoefficient\ntreat   -6.708296\nName: Estimate, dtype: float64",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.demean"
    ]
  },
  {
    "objectID": "reference/estimation.api.feglm.feglm.html",
    "href": "reference/estimation.api.feglm.feglm.html",
    "title": "estimation.api.feglm.feglm",
    "section": "",
    "text": "estimation.api.feglm.feglm(\n    fml,\n    data,\n    family,\n    vcov=None,\n    vcov_kwargs=None,\n    ssc=None,\n    fixef_rm='singleton',\n    fixef_tol=1e-06,\n    fixef_maxiter=100000,\n    iwls_tol=1e-08,\n    iwls_maxiter=25,\n    collin_tol=1e-09,\n    separation_check=None,\n    solver='scipy.linalg.solve',\n    drop_intercept=False,\n    copy_data=True,\n    store_data=True,\n    lean=False,\n    context=None,\n    split=None,\n    fsplit=None,\n    accelerate=True,\n)\nEstimate GLM regression models with fixed effects.\nSupported families: logit, probit, gaussian.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feglm.feglm"
    ]
  },
  {
    "objectID": "reference/estimation.api.feglm.feglm.html#references",
    "href": "reference/estimation.api.feglm.feglm.html#references",
    "title": "estimation.api.feglm.feglm",
    "section": "References",
    "text": "References\n\nBergé, L. (2018). Efficient estimation of maximum likelihood models with multiple fixed-effects: the R package FENmlm. CREA Discussion Paper.\nCorreia, S., Guimaraes, P., & Zylkin, T. (2019). ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects. The Stata Journal.\nStammann, A. (2018). Fast and Feasible Estimation of Generalized Linear Models with High-Dimensional k-way Fixed Effects. arXiv:1707.01815.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feglm.feglm"
    ]
  },
  {
    "objectID": "reference/estimation.api.feglm.feglm.html#parameters",
    "href": "reference/estimation.api.feglm.feglm.html#parameters",
    "title": "estimation.api.feglm.feglm",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfml\nstr\nA two-sided formula string using fixest formula syntax. Syntax: “Y ~ X1 + X2 | FE1 + FE2”. “|” separates left-hand side and fixed effects. Special syntax includes: - Stepwise regressions (sw, sw0) - Cumulative stepwise regression (csw, csw0) - Multiple dependent variables (Y1 + Y2 ~ X) - Interaction of variables (i(X1,X2)) - Interacted fixed effects (fe1^fe2) Compatible with formula parsing via the formulaic module.\nrequired\n\n\ndata\nDataFrameType\nA pandas or polars dataframe containing the variables in the formula.\nrequired\n\n\nfamily\nstr\nThe family of the GLM model. Options include “gaussian”, “logit” and “probit”.\nrequired\n\n\nvcov\nUnion[VcovTypeOptions, dict[str, str]]\nType of variance-covariance matrix for inference. Options include “iid”, “hetero”, “HC1”, “HC2”, “HC3”, “NW” for Newey-West HAC standard errors, “DK” for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the vcov_kwargs argument. For time-series HAC, you need to pass the ‘time_id’ column. For panel-HAC, you need to add pass both ‘time_id’ and ‘panel_id’. See vcov_kwargs for details.\nNone\n\n\nvcov_kwargs\nOptional[dict[str, any]]\nAdditional keyword arguments to pass to the vcov function. These keywoards include “lag” for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. “time_id” for the time ID used for NW and DK standard errors, and “panel_id” for the panel identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time periods is always treated as 1. More flexible time-step selection is work in progress.\nNone\n\n\nssc\nstr\nA ssc object specifying the small sample correction for inference.\nNone\n\n\nfixef_rm\nFixedRmOptions\nSpecifies whether to drop singleton fixed effects. Can be equal to “singleton” (default), or “none”. “singletons” will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.\n'singleton'\n\n\nfixef_tol\nfloat\nTolerance for the fixed effects demeaning algorithm. Defaults to 1e-06. Currently does not do anything, as fixed effects are not supported for GLMs.\n1e-06\n\n\nfixef_maxiter\nint\nMaximum iterations for the demeaning algorithm. Currently does not do anything, as fixed effects are not supported for GLMs.\n100000\n\n\niwls_tol\nOptional[float]\nTolerance for IWLS convergence, by default 1e-08.\n1e-08\n\n\niwls_maxiter\nOptional[float]\nMaximum number of iterations for IWLS convergence, by default 25.\n25\n\n\ncollin_tol\nfloat\nTolerance for collinearity check, by default 1e-10.\n1e-09\n\n\nseparation_check\nOptional[list[str]]\nMethods to identify and drop separated observations. Either “fe” or “ir”. Executes “fe” by default (when None).\nNone\n\n\nsolver\nSolverOptions, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n'scipy.linalg.solve'\n\n\ndrop_intercept\nbool\nWhether to drop the intercept from the model, by default False.\nFalse\n\n\ncopy_data\nbool\nWhether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of fepois. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you’ll find a column with interacted fixed effects in the data set.\nTrue\n\n\nstore_data\nbool\nWhether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the data attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. predict() or vcov().\nTrue\n\n\nlean\nbool\nFalse by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won’t be possible afterwards.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\nNone\n\n\nsplit\nOptional[str]\nA character string, i.e. ‘split = var’. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.\nNone\n\n\nfsplit\nOptional[str]\nThis argument is the same as split but also includes the full sample as the first estimation.\nNone\n\n\naccelerate\nbool\nWhether to use acceleration tricks developed in the ppmlhdfe paper (warm start and adaptive fixed effects tolerance) for models with fixed effects. Produces numerically identical results faster, so we recommend to always set it to True.\nTrue",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feglm.feglm"
    ]
  },
  {
    "objectID": "reference/estimation.api.feglm.feglm.html#returns",
    "href": "reference/estimation.api.feglm.feglm.html#returns",
    "title": "estimation.api.feglm.feglm",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nAn instance of the Fepois class or an instance of class FixestMulti for multiple models specified via fml.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feglm.feglm"
    ]
  },
  {
    "objectID": "reference/estimation.api.feglm.feglm.html#examples",
    "href": "reference/estimation.api.feglm.feglm.html#examples",
    "title": "estimation.api.feglm.feglm",
    "section": "Examples",
    "text": "Examples\nThe following example regresses Y on X1 and X2 with fixed effects for f1 and f2: fixed effects are specified after the | symbol.\n\nimport pyfixest as pf\nimport numpy as np\n\ndata = pf.get_data()\ndata[\"Y\"] = np.where(data[\"Y\"] &gt; 0, 1, 0)\ndata[\"f1\"] = np.where(data[\"f1\"] &gt; data[\"f1\"].median(), \"group1\", \"group2\")\n\nfit_probit = pf.feglm(\"Y ~ X1*f1\", data, family = \"probit\")\nfit_logit = pf.feglm(\"Y ~ X1*f1\", data, family = \"logit\")\nfit_gaussian = pf.feglm(\"Y ~ X1*f1\", data, family = \"gaussian\")\n\npf.etable([fit_probit, fit_logit, fit_gaussian])\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n(3)\n\n\n\n\ncoef\n\n\nX1\n-0.466***\n(0.073)\n-0.752***\n(0.119)\n-0.179**\n(0.056)\n\n\nf1=group2\n-0.118\n(0.133)\n-0.19\n(0.216)\n-0.044\n(0.104)\n\n\nX1 × f1=group2\n0.035\n(0.102)\n0.057\n(0.168)\n0.014\n(0.079)\n\n\nIntercept\n0.449***\n(0.096)\n0.724***\n(0.156)\n0.673***\n(0.074)\n\n\nstats\n\n\nObservations\n999\n999\n999\n\n\nR2\n-\n-\n-\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nPyFixest integrates with the marginaleffects package. For example, to compute average marginal effects for the probit model above, you can use the following code:\n\n# we load polars as marginaleffects outputs pl.DataFrame's\nimport polars as pl\nfrom marginaleffects import avg_slopes\nresults = [avg_slopes(model, variables  = \"X1\") for model in [fit_probit, fit_logit, fit_gaussian]]\npl.concat([r.to_polars() for r in results])\n\n\nshape: (3, 9)\n\n\n\nterm\ncontrast\nestimate\nstd_error\nstatistic\np_value\ns_value\nconf_low\nconf_high\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"X1\"\n\"dY/dX\"\n-0.448614\n0.051084\n-8.781909\n0.0\ninf\n-0.548736\n-0.348491\n\n\n\"X1\"\n\"dY/dX\"\n-0.722455\n0.083797\n-8.62145\n0.0\ninf\n-0.886695\n-0.558215\n\n\n\"X1\"\n\"dY/dX\"\n-0.172566\n0.039244\n-4.397222\n0.000011\n16.476798\n-0.249484\n-0.095649\n\n\n\n\n\n\nWe can also compute marginal effects by group (group average marginal effects):\n\navg_slopes(fit_probit, variables  = \"X1\", by = \"f1\")\n\n\nshape: (2, 10)\n\n\n\nf1\nterm\ncontrast\nestimate\nstd_error\nstatistic\np_value\ns_value\nconf_low\nconf_high\n\n\nenum\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"group1\"\n\"X1\"\n\"dY/dX\"\n-0.466327\n0.072613\n-6.422116\n1.3439e-10\n32.792824\n-0.608646\n-0.324009\n\n\n\"group2\"\n\"X1\"\n\"dY/dX\"\n-0.431424\n0.071948\n-5.996292\n2.0187e-9\n28.883895\n-0.57244\n-0.290407\n\n\n\n\n\n\nWe find homogeneous effects by “f1” in the probit model.\nFor more examples of other function arguments, please take a look at the documentation of the feols() function.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feglm.feglm"
    ]
  },
  {
    "objectID": "reference/report.summary.html",
    "href": "reference/report.summary.html",
    "title": "report.summary",
    "section": "",
    "text": "report.summary(models, digits=3)\nPrint a summary of estimation results for each estimated model.\nFor each model, this method prints a header indicating the fixed-effects and the dependent variable, followed by a table of coefficient estimates with standard errors, t-values, and p-values.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.summary"
    ]
  },
  {
    "objectID": "reference/report.summary.html#parameters",
    "href": "reference/report.summary.html#parameters",
    "title": "report.summary",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nA supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of\nFeols, Fepois & Feiv models.\nrequired\n\n\ndigits\nint\nThe number of decimal places to round the summary statistics to. Default is 3.\n3",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.summary"
    ]
  },
  {
    "objectID": "reference/report.summary.html#returns",
    "href": "reference/report.summary.html#returns",
    "title": "report.summary",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.summary"
    ]
  },
  {
    "objectID": "reference/report.summary.html#examples",
    "href": "reference/report.summary.html#examples",
    "title": "report.summary",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\n\n# load data\ndf = pf.get_data()\nfit1 = pf.feols(\"Y~X1 + X2 | f1\", df)\nfit2 = pf.feols(\"Y~X1 + X2 | f1 + f2\", df)\nfit3 = pf.feols(\"Y~X1 + X2 | f1 + f2 + f3\", df)\n\npf.summary([fit1, fit2, fit3])\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.950 |        0.066 |   -14.306 |      0.000 | -1.080 |  -0.819 |\n| X2            |     -0.174 |        0.018 |    -9.902 |      0.000 | -0.209 |  -0.140 |\n---\nRMSE: 1.648 R2: 0.489 R2 Within: 0.239 \n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.924 |        0.056 |   -16.483 |      0.000 | -1.034 |  -0.814 |\n| X2            |     -0.174 |        0.015 |   -11.717 |      0.000 | -0.203 |  -0.145 |\n---\nRMSE: 1.346 R2: 0.659 R2 Within: 0.303 \n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2+f3\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.957 |        0.041 |   -23.321 |      0.000 | -1.038 |  -0.877 |\n| X2            |     -0.194 |        0.011 |   -17.895 |      0.000 | -0.215 |  -0.173 |\n---\nRMSE: 0.97 R2: 0.823 R2 Within: 0.481",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.summary"
    ]
  },
  {
    "objectID": "reference/did.visualize.panelview.html",
    "href": "reference/did.visualize.panelview.html",
    "title": "did.visualize.panelview",
    "section": "",
    "text": "did.visualize.panelview(\n    data,\n    unit,\n    time,\n    treat,\n    outcome=None,\n    collapse_to_cohort=False,\n    subsamp=None,\n    units_to_plot=None,\n    sort_by_timing=False,\n    xlab=None,\n    ylab=None,\n    figsize=(11, 3),\n    noticks=False,\n    title=None,\n    legend=False,\n    ax=None,\n    xlim=None,\n    ylim=None,\n)\nGenerate a panel view of the treatment variable over time for each unit.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "did.visualize.panelview"
    ]
  },
  {
    "objectID": "reference/did.visualize.panelview.html#parameters",
    "href": "reference/did.visualize.panelview.html#parameters",
    "title": "did.visualize.panelview",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npandas.DataFrame\nThe input dataframe containing the data.\nrequired\n\n\nunit\nstr\nThe column name representing the unit identifier.\nrequired\n\n\ntime\nstr\nThe column name representing the time identifier.\nrequired\n\n\ntreat\nstr\nThe column name representing the treatment variable.\nrequired\n\n\noutcome\nstr\nThe column name representing the outcome variable. If not None, an outcome plot is generated.\nNone\n\n\ncollapse_to_cohort\nbool\nWhether to collapse units into treatment cohorts.\nFalse\n\n\nsubsamp\nint\nThe number of samples to draw from data set for display (default is None).\nNone\n\n\nsort_by_timing\nbool\nWhether to sort the treatment cohorts by the number of treated periods.\nFalse\n\n\nxlab\nstr\nThe label for the x-axis. Default is None, in which case default labels are used.\nNone\n\n\nylab\nstr\nThe label for the y-axis. Default is None, in which case default labels are used.\nNone\n\n\nfigsize\ntuple\nThe figure size for the outcome plot. Default is (11, 3).\n(11, 3)\n\n\nnoticks\nbool\nWhether to display ticks on the plot. Default is False.\nFalse\n\n\ntitle\nstr\nThe title for the plot. Default is None, in which case no title is displayed.\nNone\n\n\nlegend\nbool\nWhether to display a legend. Default is False (since binary treatments are self-explanatory).\nFalse\n\n\nax\nmatplotlib.pyplot.Axes\nThe axes on which to draw the plot. Default is None, in which case a new figure is created.\nNone\n\n\nxlim\ntuple\nThe limits for the x-axis of the plot. Default is None.\nNone\n\n\nylim\ntuple\nThe limits for the y-axis of the plot. Default is None.\nNone\n\n\nunits_to_plot\nlist\nA list of unit to include in the plot. If None, all units in the dataset are plotted.\nNone",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "did.visualize.panelview"
    ]
  },
  {
    "objectID": "reference/did.visualize.panelview.html#returns",
    "href": "reference/did.visualize.panelview.html#returns",
    "title": "did.visualize.panelview",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\nax\nmatplotlib.pyplot.Axes",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "did.visualize.panelview"
    ]
  },
  {
    "objectID": "reference/did.visualize.panelview.html#examples",
    "href": "reference/did.visualize.panelview.html#examples",
    "title": "did.visualize.panelview",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport numpy as np\nimport pyfixest as pf\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\n\n# Inspect treatment assignment\npf.panelview(\n    data = df_het,\n    unit = \"unit\",\n    time = \"year\",\n    treat = \"treat\",\n    subsamp = 50,\n    title = \"Treatment Assignment\"\n)\n\n# Outcome plot\npf.panelview(\n    data = df_het,\n    unit = \"unit\",\n    time = \"year\",\n    outcome = \"dep_var\",\n    treat = \"treat\",\n    subsamp = 50,\n    title = \"Outcome Plot\"\n)",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "did.visualize.panelview"
    ]
  },
  {
    "objectID": "reference/report.coefplot.html",
    "href": "reference/report.coefplot.html",
    "title": "report.coefplot",
    "section": "",
    "text": "report.coefplot(\n    models,\n    alpha=0.05,\n    figsize=None,\n    yintercept=0,\n    xintercept=None,\n    rotate_xticks=0,\n    title=None,\n    coord_flip=True,\n    keep=None,\n    drop=None,\n    exact_match=False,\n    plot_backend='lets_plot' if _HAS_LETS_PLOT else 'matplotlib',\n    labels=None,\n    joint=None,\n    seed=None,\n    ax=None,\n    rename_models=None,\n)\nPlot model coefficients with confidence intervals.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.coefplot"
    ]
  },
  {
    "objectID": "reference/report.coefplot.html#parameters",
    "href": "reference/report.coefplot.html#parameters",
    "title": "report.coefplot",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nA supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of\nFeols, Fepois & Feiv models.\nrequired\n\n\nfigsize\ntuple or None\nThe size of the figure. If None, the default size is used.\nNone\n\n\nalpha\nfloat\nThe significance level for the confidence intervals.\n0.05\n\n\nyintercept\nfloat or None\nThe value at which to draw a horizontal line on the plot. Default is 0.\n0\n\n\nxintercept\nfloat or None\nThe value at which to draw a vertical line on the plot. Default is None.\nNone\n\n\nrotate_xticks\nfloat\nThe angle in degrees to rotate the xticks labels. Default is 0 (no rotation).\n0\n\n\ntitle\nstr\nThe title of the plot.\nNone\n\n\ncoord_flip\nbool\nWhether to flip the coordinates of the plot. Default is True.\nTrue\n\n\nkeep\nOptional[Union[list, str]]\nThe pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients. “age”, # would keep all coefficients containing age r”^tr”, # would keep all coefficients starting with tr r”\\d$“, # would keep all coefficients ending with number Output will be in the order of the patterns.\nNone\n\n\ndrop\nOptional[Union[list, str]]\nThe pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for keep. Default is keeping all coefficients. Parameter keep and drop can be used simultaneously.\nNone\n\n\nexact_match\nbool\nWhether to use exact match for keep and drop. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.\nFalse\n\n\nplot_backend\nstr\nThe plotting backend to use. Options are “lets_plot” (default if installed) and “matplotlib”. If “lets_plot” is specified but not installed, an ImportError will be raised with instructions to install it or use “matplotlib” instead.\n'lets_plot' if _HAS_LETS_PLOT else 'matplotlib'\n\n\nrename_models\ndict\nA dictionary to rename the models. The keys are the original model names and the values the new names.\nNone\n\n\nlabels\nOptional[dict]\nA dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the _coefnames attribute of the model. The values in the dictionary are the new names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The renaming is applied after the selection of the coefficients via keep and drop.\nNone\n\n\njoint\nOptional[Union[str, bool]]\nWhether to plot simultaneous confidence bands for the coefficients. If True, simultaneous confidence bands are plotted. If False, “standard” confidence intervals are plotted. If “both”, both are plotted in one figure. Default is None, which returns the standard confidence intervals. Note that this option is not available for objects of type FixestMulti, i.e. multiple estimation.\nNone\n\n\nseed\nOptional[int]\nThe seed for the random number generator. Default is None. Only required / used when joint is True.\nNone",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.coefplot"
    ]
  },
  {
    "objectID": "reference/report.coefplot.html#returns",
    "href": "reference/report.coefplot.html#returns",
    "title": "report.coefplot",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nA plot figure from the specified backend.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.coefplot"
    ]
  },
  {
    "objectID": "reference/report.coefplot.html#examples",
    "href": "reference/report.coefplot.html#examples",
    "title": "report.coefplot",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\n\ndf = pf.get_data()\nfit1 = pf.feols(\"Y ~ i(f1)\", data = df)\nfit2 = pf.feols(\"Y ~ i(f1) + X2\", data = df)\nfit3 = pf.feols(\"Y ~ i(f1) + X2 | f1\", data = df)\n\npf.iplot([fit1, fit2, fit3])\npf.iplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        fit1._model_name_plot: \"Model 1\",\n        fit2._model_name_plot: \"Model 2\",\n        fit3._model_name_plot: \"Model 3\"\n    },\n)\npf.iplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        \"Y~i(f1)\": \"Model 1\",\n        \"Y~i(f1)+X2\": \"Model 2\",\n        \"Y~i(f1)+X2|f1\": \"Model 3\"\n    },\n)\npf.iplot([fit1], joint = \"both\")\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:2761: UserWarning: \n            29 variables dropped due to multicollinearity.\n            The following variables are dropped: \n    C(f1)[T.1.0]\n    C(f1)[T.2.0]\n    C(f1)[T.3.0]\n    C(f1)[T.4.0]\n    C(f1)[T.5.0]\n    ....\n            \n  warnings.warn(",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.coefplot"
    ]
  },
  {
    "objectID": "reference/estimation.feols_compressed_.FeolsCompressed.html",
    "href": "reference/estimation.feols_compressed_.FeolsCompressed.html",
    "title": "estimation.feols_compressed_.FeolsCompressed",
    "section": "",
    "text": "estimation.feols_compressed_.FeolsCompressed(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    solver='np.linalg.solve',\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    context=0,\n    sample_split_var=None,\n    sample_split_value=None,\n    reps=None,\n    seed=None,\n)\nNon-user-facing class for compressed regression with fixed effects.\nSee the paper “You only compress once” by Wong et al (https://arxiv.org/abs/2102.11297) for details on regression compression.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nFixestFormula\nFixestFormula\nThe formula object.\nrequired\n\n\ndata\npd.DataFrame\nThe data.\nrequired\n\n\nssc_dict\ndict[str, Union[str, bool]]\nThe ssc dictionary.\nrequired\n\n\ndrop_singletons\nbool\nWhether to drop columns with singleton fixed effects.\nrequired\n\n\ndrop_intercept\nbool\nWhether to include an intercept.\nrequired\n\n\nweights\nOptional[str]\nThe column name of the weights. None if no weights are used. For this method, weights needs to be None.\nrequired\n\n\nweights_type\nOptional[str]\nThe type of weights. For this method, weights_type needs to be ‘fweights’.\nrequired\n\n\ncollin_tol\nfloat\nThe tolerance level for collinearity.\nrequired\n\n\nfixef_tol\nfloat\nThe tolerance level for the fixed effects.\nrequired\n\n\nfixef_maxiter\nint\nThe maximum iterations for the demeaning algorithm.\nrequired\n\n\nlookup_demeaned_data\ndict[str, pd.DataFrame]\nThe lookup table for demeaned data.\nrequired\n\n\nsolver\nSolverOptions\nThe solver to use.\n'np.linalg.solve'\n\n\nstore_data\nbool\nWhether to store the data.\nTrue\n\n\ncopy_data\nbool\nWhether to copy the data.\nTrue\n\n\nlean\nbool\nWhether to keep memory-heavy objects as attributes or not.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n0\n\n\nreps\nint\nThe number of bootstrap repetitions. Default is 100. Only used for CRV1 inference, where a wild cluster bootstrap is used.\nNone\n\n\nseed\nOptional[int]\nThe seed for the random number generator. Only relevant for CRV1 inference, where a wild cluster bootstrap is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndemean\nCompression ‘handles demeaning’ via Mundlak transform.\n\n\npredict\nCompute predicted values.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nvcov\nCompute the variance-covariance matrix for the compressed regression.\n\n\n\n\n\nestimation.feols_compressed_.FeolsCompressed.demean()\nCompression ‘handles demeaning’ via Mundlak transform.\n\n\n\nestimation.feols_compressed_.FeolsCompressed.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nCompute predicted values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nOptional[DataFrameType]\nThe new data. If None, makes a prediction based on the uncompressed data set.\nNone\n\n\natol\nfloat\nThe absolute tolerance.\n1e-06\n\n\nbtol\nfloat\nThe relative tolerance.\n1e-06\n\n\ntype\nstr\nThe type of prediction.\n'link'\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.feols_compressed_.FeolsCompressed.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.feols_compressed_.FeolsCompressed.vcov(\n    vcov,\n    vcov_kwargs=None,\n    data=None,\n)\nCompute the variance-covariance matrix for the compressed regression.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_compressed_.FeolsCompressed"
    ]
  },
  {
    "objectID": "reference/estimation.feols_compressed_.FeolsCompressed.html#parameters",
    "href": "reference/estimation.feols_compressed_.FeolsCompressed.html#parameters",
    "title": "estimation.feols_compressed_.FeolsCompressed",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nFixestFormula\nFixestFormula\nThe formula object.\nrequired\n\n\ndata\npd.DataFrame\nThe data.\nrequired\n\n\nssc_dict\ndict[str, Union[str, bool]]\nThe ssc dictionary.\nrequired\n\n\ndrop_singletons\nbool\nWhether to drop columns with singleton fixed effects.\nrequired\n\n\ndrop_intercept\nbool\nWhether to include an intercept.\nrequired\n\n\nweights\nOptional[str]\nThe column name of the weights. None if no weights are used. For this method, weights needs to be None.\nrequired\n\n\nweights_type\nOptional[str]\nThe type of weights. For this method, weights_type needs to be ‘fweights’.\nrequired\n\n\ncollin_tol\nfloat\nThe tolerance level for collinearity.\nrequired\n\n\nfixef_tol\nfloat\nThe tolerance level for the fixed effects.\nrequired\n\n\nfixef_maxiter\nint\nThe maximum iterations for the demeaning algorithm.\nrequired\n\n\nlookup_demeaned_data\ndict[str, pd.DataFrame]\nThe lookup table for demeaned data.\nrequired\n\n\nsolver\nSolverOptions\nThe solver to use.\n'np.linalg.solve'\n\n\nstore_data\nbool\nWhether to store the data.\nTrue\n\n\ncopy_data\nbool\nWhether to copy the data.\nTrue\n\n\nlean\nbool\nWhether to keep memory-heavy objects as attributes or not.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n0\n\n\nreps\nint\nThe number of bootstrap repetitions. Default is 100. Only used for CRV1 inference, where a wild cluster bootstrap is used.\nNone\n\n\nseed\nOptional[int]\nThe seed for the random number generator. Only relevant for CRV1 inference, where a wild cluster bootstrap is used.\nNone",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_compressed_.FeolsCompressed"
    ]
  },
  {
    "objectID": "reference/estimation.feols_compressed_.FeolsCompressed.html#methods",
    "href": "reference/estimation.feols_compressed_.FeolsCompressed.html#methods",
    "title": "estimation.feols_compressed_.FeolsCompressed",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndemean\nCompression ‘handles demeaning’ via Mundlak transform.\n\n\npredict\nCompute predicted values.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nvcov\nCompute the variance-covariance matrix for the compressed regression.\n\n\n\n\n\nestimation.feols_compressed_.FeolsCompressed.demean()\nCompression ‘handles demeaning’ via Mundlak transform.\n\n\n\nestimation.feols_compressed_.FeolsCompressed.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nCompute predicted values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nOptional[DataFrameType]\nThe new data. If None, makes a prediction based on the uncompressed data set.\nNone\n\n\natol\nfloat\nThe absolute tolerance.\n1e-06\n\n\nbtol\nfloat\nThe relative tolerance.\n1e-06\n\n\ntype\nstr\nThe type of prediction.\n'link'\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.feols_compressed_.FeolsCompressed.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.feols_compressed_.FeolsCompressed.vcov(\n    vcov,\n    vcov_kwargs=None,\n    data=None,\n)\nCompute the variance-covariance matrix for the compressed regression.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_compressed_.FeolsCompressed"
    ]
  },
  {
    "objectID": "reference/estimation.api.quantreg.quantreg.html",
    "href": "reference/estimation.api.quantreg.quantreg.html",
    "title": "estimation.api.quantreg.quantreg",
    "section": "",
    "text": "estimation.api.quantreg.quantreg(\n    fml,\n    data,\n    vcov='nid',\n    quantile=0.5,\n    method='fn',\n    multi_method='cfm1',\n    tol=1e-06,\n    maxiter=None,\n    ssc=None,\n    collin_tol=1e-09,\n    separation_check=None,\n    drop_intercept=False,\n    copy_data=True,\n    store_data=True,\n    lean=False,\n    context=None,\n    split=None,\n    fsplit=None,\n    seed=None,\n)\nFit a quantile regression model using the interior point algorithm from Portnoy and Koenker (1997). Note that the interior point algorithm assumes independent observations.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.quantreg.quantreg"
    ]
  },
  {
    "objectID": "reference/estimation.api.quantreg.quantreg.html#parameters",
    "href": "reference/estimation.api.quantreg.quantreg.html#parameters",
    "title": "estimation.api.quantreg.quantreg",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfml\nstr\nA two-sided formula string using fixest formula syntax. In contrast to feols() and feglm(), no fixed effects formula syntax is supported.\nrequired\n\n\ndata\nDataFrameType\nA pandas or polars dataframe containing the variables in the formula.\nrequired\n\n\nquantile\nfloat\nThe quantile to estimate. Must be between 0 and 1.\n0.5\n\n\nmethod\nQuantregMethodOptions\nThe method to use for the quantile regression. Currently, only “fn” is supported. In the future, will be either “fn” or “pfn”. “fn” implements the Frisch-Newton interior point algorithm described in Portnoy and Koenker (1997). The “pfn” method implements a variant of the algorithm proposed by Portnoy and Koenker (1997) including preprocessing steps, which a) can speed up the algorithm if N is very large but b) assumes independent observations. For details, you can either take a look at the Portnoy and Koenker paper, or “Fast Algorithms for the Quantile Regression Process” by Chernozhukov, Fernández-Val, and Melly (2019).\n'fn'\n\n\nmulti_method\nQuantregMultiOptions\nControls the algorithm for running the quantile regression process. Only relevant if more than one quantile regression is fit in one quantreg call. Options are ‘cmf1’, which is the default and implements algorithm 2 from Chernozhukov et al, ‘cmf2’, which implements their algorithm 3, and ‘none’, which just loops over separate model calls.\n'cfm1'\n\n\ntol\nfloat\nThe tolerance for the algorithm. Defaults to 1e-06. As in R’s quantreg package, the algorithm stops when the relative change in the duality gap is less than tol.\n1e-06\n\n\nmaxiter\nint\nThe maximum number of iterations. If None, maxiter = the number of observations in the model (as in R’s quantreg package via nit(3) = n).\nNone\n\n\nvcov\nUnion[VcovTypeOptions, dict[str, str]]\nType of variance-covariance matrix for inference. Currently supported are “iid”, “nid”, and cluster robust errors, “iid” by default. All of “iid”, “hetero”and “cluster” robust error are based on a kernel-based estimator as in Powell (1991). The “nid” method implements the robust sandwich estimator proposed in Hendricks and Koenker (1993). Any of “HC1 / HC2 / HC3 also works and is equivalent to”hetero”. Cluster robust inference following Parente and Santos Silva (2016) can be specified via a dictionary with the keys “type” and “cluster”. Only one-way clustering is supported.\n'nid'\n\n\nssc\ndict[str, Union[str, bool]]\nA dictionary specifying the small sample correction for inference. If None, uses default settings from ssc_func(). Note that by default, R’s quantreg and Stata’s qreg2 do not use small sample corrections. To match their behavior, set ssc = pf.ssc(adj = False, cluster_adj = False).\nNone\n\n\ncollin_tol\nfloat\nTolerance for collinearity check, by default 1e-10.\n1e-09\n\n\nseparation_check\nlist[str]\nMethods to identify and drop separated observations. Not used in quantile regression.\nNone\n\n\ndrop_intercept\nbool\nWhether to drop the intercept from the model, by default False.\nFalse\n\n\ncopy_data\nbool\nWhether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of quantreg.\nTrue\n\n\nstore_data\nbool\nWhether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the data attribute of the model object.\nTrue\n\n\nlean\nbool\nFalse by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won’t be possible afterwards.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\nNone\n\n\nsplit\nstr\nA character string, i.e. ‘split = var’. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.\nNone\n\n\nfsplit\nstr\nThis argument is the same as split but also includes the full sample as the first estimation.\nNone\n\n\nseed\nOptional[int]\nA random seed for reproducibility. If None, no seed is set. Only relevant for the “pfn” method. The “fn” method is deterministic and does not require a seed.\nNone",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.quantreg.quantreg"
    ]
  },
  {
    "objectID": "reference/estimation.api.quantreg.quantreg.html#returns",
    "href": "reference/estimation.api.quantreg.quantreg.html#returns",
    "title": "estimation.api.quantreg.quantreg",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nAn instance of the Quantreg class or FixestMulti class for multiple models specified via fml.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.quantreg.quantreg"
    ]
  },
  {
    "objectID": "reference/estimation.api.quantreg.quantreg.html#examples",
    "href": "reference/estimation.api.quantreg.quantreg.html#examples",
    "title": "estimation.api.quantreg.quantreg",
    "section": "Examples",
    "text": "Examples\nThe following example regresses Y on X1 and X2 at the median (0.5 quantile):\n\nimport pyfixest as pf\nimport pandas as pd\nimport numpy as np\n\ndata = pf.get_data()\n\nfit = pf.quantreg(\"Y ~ X1 + X2\", data, quantile=0.5)\nfit.summary()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/quantreg/quantreg_.py:77: FutureWarning: \n           The Quantile Regression implementation is experimental and may change in future releases.\n           But mostly, we expect the API to remain unchanged.\n           \n  warnings.warn(\n\n\n###\n\nEstimation:  quantreg: q = 0.5\nDep. var.: Y, Fixed effects: 0\nInference:  nid\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.998 |        0.172 |     5.800 |      0.000 |  0.660 |   1.335 |\n| X1            |     -1.071 |        0.122 |    -8.776 |      0.000 | -1.310 |  -0.831 |\n| X2            |     -0.182 |        0.032 |    -5.713 |      0.000 | -0.245 |  -0.120 |\n---\n\n\n\nFor details around inference, estimation techniques, (fast) fitting and visualizing the full quantile regression process, please take a look at the dedicated vignette.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.quantreg.quantreg"
    ]
  },
  {
    "objectID": "reference/report.etable.html",
    "href": "reference/report.etable.html",
    "title": "report.etable",
    "section": "",
    "text": "report.etable(\n    models,\n    type='gt',\n    signif_code=None,\n    coef_fmt='b \\n (se)',\n    custom_stats=None,\n    custom_model_stats=None,\n    keep=None,\n    drop=None,\n    exact_match=False,\n    labels=None,\n    cat_template=None,\n    show_fe=True,\n    felabels=None,\n    fe_present='x',\n    fe_absent='-',\n    notes='',\n    model_heads=None,\n    head_order='dh',\n    file_name=None,\n    **kwargs,\n)\nGenerate a table summarizing the results of multiple regression models.\nThis function uses the maketables package internally to create publication-ready regression tables. It supports various output formats including HTML (via Great Tables), markdown, and LaTeX.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.etable"
    ]
  },
  {
    "objectID": "reference/report.etable.html#parameters",
    "href": "reference/report.etable.html#parameters",
    "title": "report.etable",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nA supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of\nFeols, Fepois & Feiv models. The models to be summarized in the table.\nrequired\n\n\ntype\nstr\nType of output. Either “df” for pandas DataFrame, “md” for markdown, “gt” for great_tables, or “tex” for LaTeX table. Default is “gt”.\n'gt'\n\n\nsignif_code\nlist\nSignificance levels for the stars. Default is None, which sets [0.001, 0.01, 0.05]. If None, no stars are printed.\nNone\n\n\ncoef_fmt\nstr\nThe format of the coefficient (b), standard error (se), t-stats (t), and p-value (p). Default is \"b \\n (se)\". Spaces , parentheses (), brackets [], newlines \\n are supported.\n'b \\n (se)'\n\n\ncustom_stats\nOptional[dict]\nA dictionary of custom statistics that can be used in the coef_fmt string to be displayed in the coefficuent cells analogously to “b”, “se” etc. The keys are the names of the custom statistics, and the values are lists of lists, where each inner list contains the custom statistic values for all coefficients each model. Note that “b”, “se”, “t”, or “p” are reserved and cannot be used as keys.\nNone\n\n\ncustom_model_stats\nOptional[dict]\nA dictionary of custom model statistics or model information displayed in a new line in the bottom panel of the table. The keys are the names of the statistics (i.e. entry in the first column) and the values are a lists of the same length as the number of models. Default is None.\nNone\n\n\nkeep\nOptional[Union[list, str]]\nThe pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients. “age”, # would keep all coefficients containing age r”^tr”, # would keep all coefficients starting with tr r”\\d$“, # would keep all coefficients ending with number Output will be in the order of the patterns.\nNone\n\n\ndrop\nOptional[Union[list, str]]\nThe pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for keep. Default is keeping all coefficients. Parameter keep and drop can be used simultaneously.\nNone\n\n\nexact_match\nOptional[bool]\nWhether to use exact match for keep and drop. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.\nFalse\n\n\nlabels\nOptional[dict]\nA dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the _coefnames attribute of the model. The values in the dictionary are the new names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The command is applied after the keep and drop commands.\nNone\n\n\ncat_template\nOptional[str]\nTemplate to relabel categorical variables. None by default, which applies no relabeling. Other options include combinations of “{variable}” and “{value}”, e.g. “{variable}::{value}” to mimic fixest encoding. But “{variable}–{value}” or “{variable}{value}” or just “{value}” are also possible.\nNone\n\n\nshow_fe\nOptional[bool]\nWhether to show the rows with fixed effects markers. Default is True.\nTrue\n\n\nfelabels\nOptional[dict]\nA dictionary to relabel the fixed effects. Only needed if you want to relabel the FE lines with a different label than the one specied for the respective variable in the labels dictionary. The command is applied after the keep and drop commands.\nNone\n\n\nfe_present\nstr\nSymbol to use when a fixed effect is present in a model. Default is “x”. Common alternatives include “Y”, “YES”, “✓”, “✅”, or any custom string.\n'x'\n\n\nfe_absent\nstr\nSymbol to use when a fixed effect is absent from a model. Default is “-”. Common alternatives include “N”, “NO”, “✗”, ““, or any custom string.\n'-'\n\n\ndigits\n\nThe number of digits to round to.\nrequired\n\n\nthousands_sep\n\nThe thousands separator. Default is False.\nrequired\n\n\nscientific_notation\n\nWhether to use scientific notation. Default is True.\nrequired\n\n\nscientific_notation_threshold\n\nThe threshold for using scientific notation. Default is 10_000.\nrequired\n\n\nnotes\nstr\nCustom table notes. Default shows the significance levels and the format of the coefficient cell.\n''\n\n\nmodel_heads\nOptional[list]\nAdd custom headlines to models when output as df or latex. Length of list must correspond to number of models. Default is None.\nNone\n\n\nhead_order\nOptional[str]\nString to determine the display of the table header when output as df or latex. Allowed values are “dh”, “hd”, “d”, “h”, or ““. When head_order is”dh”, the dependent variable is displayed first, followed by the custom model_heads (provided the user has specified them). With “hd” it is the other way around. When head_order is “d”, only the dependent variable and model numbers are displayed and with “” only the model numbers. Default is “dh”.\n'dh'\n\n\nfile_name\nOptional[str]\nThe name/path of the file to save the LaTeX table to. Default is None.\nNone",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.etable"
    ]
  },
  {
    "objectID": "reference/report.etable.html#returns",
    "href": "reference/report.etable.html#returns",
    "title": "report.etable",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA styled DataFrame with the coefficients and standard errors of the models. When output is “tex”, the LaTeX code is returned as a string.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.etable"
    ]
  },
  {
    "objectID": "reference/report.etable.html#examples",
    "href": "reference/report.etable.html#examples",
    "title": "report.etable",
    "section": "Examples",
    "text": "Examples\nFor more examples, take a look at the regression tables and summary statistics vignette.\n\nimport pyfixest as pf\n\n# load data\ndf = pf.get_data()\nfit1 = pf.feols(\"Y~X1 + X2 | f1\", df)\nfit2 = pf.feols(\"Y~X1 + X2 | f1 + f2\", df)\n\npf.etable([fit1, fit2])\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n-0.95***\n(0.066)\n-0.924***\n(0.056)\n\n\nX2\n-0.174***\n(0.018)\n-0.174***\n(0.015)\n\n\nfe\n\n\nf1\nx\nx\n\n\nf2\n-\nx\n\n\nstats\n\n\nObservations\n997\n997\n\n\nR2\n0.489\n0.659\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.etable"
    ]
  },
  {
    "objectID": "reference/estimation.bonferroni.html",
    "href": "reference/estimation.bonferroni.html",
    "title": "estimation.bonferroni",
    "section": "",
    "text": "estimation.bonferroni(models, param)\nCompute Bonferroni adjusted p-values for multiple hypothesis testing.\nFor each model, it is assumed that tests to adjust are of the form “param = 0”.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.bonferroni"
    ]
  },
  {
    "objectID": "reference/estimation.bonferroni.html#parameters",
    "href": "reference/estimation.bonferroni.html#parameters",
    "title": "estimation.bonferroni",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nA supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of\nFeols, Fepois & Feiv models.\nrequired\n\n\nparam\nstr\nThe parameter for which the p-values should be adjusted.\nrequired",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.bonferroni"
    ]
  },
  {
    "objectID": "reference/estimation.bonferroni.html#returns",
    "href": "reference/estimation.bonferroni.html#returns",
    "title": "estimation.bonferroni",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA DataFrame containing estimation statistics, including the Bonferroni adjusted p-values.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.bonferroni"
    ]
  },
  {
    "objectID": "reference/estimation.bonferroni.html#examples",
    "href": "reference/estimation.bonferroni.html#examples",
    "title": "estimation.bonferroni",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\nfrom pyfixest.utils import get_data\n\ndata = get_data().dropna()\nfit1 = pf.feols(\"Y ~ X1\", data=data)\nfit2 = pf.feols(\"Y ~ X1 + X2\", data=data)\nbonf_df = pf.bonferroni([fit1, fit2], param=\"X1\")\nbonf_df\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\nest0\nest1\n\n\n\n\nEstimate\n-1.001930\n-0.995197\n\n\nStd. Error\n0.084823\n0.082194\n\n\nt value\n-11.811964\n-12.107957\n\n\nPr(&gt;|t|)\n0.000000\n0.000000\n\n\n2.5%\n-1.168383\n-1.156490\n\n\n97.5%\n-0.835476\n-0.833904\n\n\nBonferroni Pr(&gt;|t|)\n0.000000\n0.000000",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.bonferroni"
    ]
  },
  {
    "objectID": "reference/report.dtable.html",
    "href": "reference/report.dtable.html",
    "title": "report.dtable",
    "section": "",
    "text": "report.dtable(\n    df,\n    vars,\n    stats=None,\n    bycol=None,\n    byrow=None,\n    type='gt',\n    labels=None,\n    stats_labels=None,\n    digits=2,\n    notes='',\n    counts_row_below=False,\n    hide_stats=False,\n    **kwargs,\n)\nGenerate descriptive statistics tables and create a booktab style table in the desired format (gt or tex).\n.. deprecated:: 0.41.0 This function is deprecated and will be removed in a future version. Please use maketables.DTable() directly instead. See https://py-econometrics.github.io/maketables/ for documentation.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.dtable"
    ]
  },
  {
    "objectID": "reference/report.dtable.html#parameters",
    "href": "reference/report.dtable.html#parameters",
    "title": "report.dtable",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the table to be displayed.\nrequired\n\n\nvars\nlist\nList of variables to be included in the table.\nrequired\n\n\nstats\nlist\nList of statistics to be calculated. The default is None, that sets [‘count’,‘mean’, ‘std’]. All pandas aggregation functions are supported.\nNone\n\n\nbycol\nlist\nList of variables to be used to group the data by columns. The default is None.\nNone\n\n\nbyrow\nstr\nVariable to be used to group the data by rows. The default is None.\nNone\n\n\ntype\nstr\nType of table to be created. The default is ‘gt’. Type can be ‘gt’ for great_tables, ‘tex’ for LaTeX or ‘df’ for dataframe.\n'gt'\n\n\nlabels\ndict\nDictionary containing the labels for the variables. The default is None.\nNone\n\n\nstats_labels\ndict\nDictionary containing the labels for the statistics. The default is None. The function uses a default labeling which will be replaced by the labels in the dictionary.\nNone\n\n\ndigits\nint\nNumber of decimal places to round the statistics to. The default is 2.\n2\n\n\nnotes\nstr\nTable notes to be displayed at the bottom of the table.\n''\n\n\ncounts_row_below\nbool\nWhether to display the number of observations at the bottom of the table. Will only be carried out when each var has the same number of obs and when byrow is None. The default is False\nFalse\n\n\nhide_stats\nbool\nWhether to hide the names of the statistics in the table header. When stats are hidden and the user provides no notes string the labels of the stats are listed in the table notes. The default is False.\nFalse\n\n\nkwargs\ndict\nAdditional arguments to be passed to maketables.DTable.\n{}",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.dtable"
    ]
  },
  {
    "objectID": "reference/report.dtable.html#returns",
    "href": "reference/report.dtable.html#returns",
    "title": "report.dtable",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA table in the specified format.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.dtable"
    ]
  },
  {
    "objectID": "reference/report.dtable.html#examples",
    "href": "reference/report.dtable.html#examples",
    "title": "report.dtable",
    "section": "Examples",
    "text": "Examples\nFor more examples, take a look at the regression tables and summary statistics vignette.\n\nimport pyfixest as pf\n\n# load data\ndf = pf.get_data()\npf.dtable(df, vars = [\"Y\", \"X1\", \"X2\", \"f1\"])\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/tmp/ipykernel_6359/2291538525.py:5: FutureWarning: pf.dtable() is deprecated and will be removed in a future version. Please use maketables.DTable() directly. See https://py-econometrics.github.io/maketables/ for documentation.\n  pf.dtable(df, vars = [\"Y\", \"X1\", \"X2\", \"f1\"])\n\n\n\n\n\n\n\n\n\nN\nMean\nStd. Dev.\n\n\n\n\nY\n999.00\n-0.13\n2.30\n\n\nX1\n999.00\n1.04\n0.81\n\n\nX2\n1,000\n-0.13\n3.05\n\n\nf1\n999.00\n14.21\n8.70",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.dtable"
    ]
  },
  {
    "objectID": "reference/estimation.feglm_.Feglm.html",
    "href": "reference/estimation.feglm_.Feglm.html",
    "title": "estimation.feglm_.Feglm",
    "section": "",
    "text": "estimation.feglm_.Feglm(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    tol,\n    maxiter,\n    solver,\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    sample_split_var=None,\n    sample_split_value=None,\n    separation_check=None,\n    context=0,\n    accelerate=True,\n)\nAbstract base class for the estimation of a fixed-effects GLM model.\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_fit\nFit the GLM model via iterated weighted least squares.\n\n\npredict\nReturn predicted values from regression model.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nresidualize\nResidualize v and X by flist using weights.\n\n\nto_array\nTurn estimation DataFrames to np arrays.\n\n\n\n\n\nestimation.feglm_.Feglm.get_fit()\nFit the GLM model via iterated weighted least squares.\nThe implementation follows ideas developed in - Bergé (2018): https://ideas.repec.org/p/luc/wpaper/18-13.html - Correia, Guimaraes, Zylkin (2019): https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909691 - Stamann (2018): https://arxiv.org/pdf/1707.01815\n\n\n\nestimation.feglm_.Feglm.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nReturn predicted values from regression model.\nReturn a flat np.array with predicted values of the regression model. If new fixed effect levels are introduced in newdata, predicted values for such observations will be set to NaN.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nUnion[None, pd.DataFrame]\nA pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.\nNone\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\ntype\nstr\nThe type of prediction to be computed. Can be either “response” (default) or “link”. If type=“response”, the output is at the level of the response variable, i.e., it is the expected predictor E(Y|X). If “link”, the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta.\n'link'\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.feglm_.Feglm.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.feglm_.Feglm.residualize(v, X, flist, weights, tol, maxiter)\nResidualize v and X by flist using weights.\n\n\n\nestimation.feglm_.Feglm.to_array()\nTurn estimation DataFrames to np arrays.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feglm_.Feglm"
    ]
  },
  {
    "objectID": "reference/estimation.feglm_.Feglm.html#methods",
    "href": "reference/estimation.feglm_.Feglm.html#methods",
    "title": "estimation.feglm_.Feglm",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_fit\nFit the GLM model via iterated weighted least squares.\n\n\npredict\nReturn predicted values from regression model.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nresidualize\nResidualize v and X by flist using weights.\n\n\nto_array\nTurn estimation DataFrames to np arrays.\n\n\n\n\n\nestimation.feglm_.Feglm.get_fit()\nFit the GLM model via iterated weighted least squares.\nThe implementation follows ideas developed in - Bergé (2018): https://ideas.repec.org/p/luc/wpaper/18-13.html - Correia, Guimaraes, Zylkin (2019): https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909691 - Stamann (2018): https://arxiv.org/pdf/1707.01815\n\n\n\nestimation.feglm_.Feglm.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nReturn predicted values from regression model.\nReturn a flat np.array with predicted values of the regression model. If new fixed effect levels are introduced in newdata, predicted values for such observations will be set to NaN.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nUnion[None, pd.DataFrame]\nA pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.\nNone\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\ntype\nstr\nThe type of prediction to be computed. Can be either “response” (default) or “link”. If type=“response”, the output is at the level of the response variable, i.e., it is the expected predictor E(Y|X). If “link”, the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta.\n'link'\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.feglm_.Feglm.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.feglm_.Feglm.residualize(v, X, flist, weights, tol, maxiter)\nResidualize v and X by flist using weights.\n\n\n\nestimation.feglm_.Feglm.to_array()\nTurn estimation DataFrames to np arrays.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feglm_.Feglm"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "PyFixest Function Reference",
    "section": "",
    "text": "User facing estimation functions\n\n\n\nestimation.api.feols.feols\nEstimate a linear regression models with fixed effects using fixest formula syntax.\n\n\nestimation.api.fepois.fepois\nEstimate Poisson regression model with fixed effects using the ppmlhdfe algorithm.\n\n\nestimation.api.feglm.feglm\nEstimate GLM regression models with fixed effects.\n\n\nestimation.api.quantreg.quantreg\nFit a quantile regression model using the interior point algorithm from Portnoy and Koenker (1997).\n\n\ndid.estimation.did2s\nEstimate a Difference-in-Differences model using Gardner’s two-step DID2S estimator.\n\n\ndid.estimation.lpdid\nLocal projections approach to estimation.\n\n\ndid.estimation.event_study\nEstimate Event Study Model.\n\n\nestimation.bonferroni\nCompute Bonferroni adjusted p-values for multiple hypothesis testing.\n\n\nestimation.rwolf\nCompute Romano-Wolf adjusted p-values for multiple hypothesis testing.\n\n\n\n\n\n\nDetails on Methods and Attributes\n\n\n\nestimation.feols_.Feols\nNon user-facing class to estimate a linear regression via OLS.\n\n\nestimation.fepois_.Fepois\nEstimate a Poisson regression model.\n\n\nestimation.feiv_.Feiv\nNon user-facing class to estimate an IV model using a 2SLS estimator.\n\n\nestimation.feglm_.Feglm\nAbstract base class for the estimation of a fixed-effects GLM model.\n\n\nestimation.felogit_.Felogit\nClass for the estimation of a fixed-effects logit model.\n\n\nestimation.feprobit_.Feprobit\nClass for the estimation of a fixed-effects probit model.\n\n\nestimation.fegaussian_.Fegaussian\nClass for the estimation of a fixed-effects GLM with normal errors.\n\n\nestimation.feols_compressed_.FeolsCompressed\nNon-user-facing class for compressed regression with fixed effects.\n\n\n\n\n\n\nPost-Processing of Estimation Results\n\n\n\ndid.visualize.panelview\nGenerate a panel view of the treatment variable over time for each unit.\n\n\nreport.summary\nPrint a summary of estimation results for each estimated model.\n\n\nreport.etable\nGenerate a table summarizing the results of multiple regression models.\n\n\nreport.dtable\nGenerate descriptive statistics tables and create a booktab style table in\n\n\nreport.coefplot\nPlot model coefficients with confidence intervals.\n\n\nreport.iplot\nPlot model coefficients for variables interacted via “i()” syntax, with\n\n\ndid.visualize.panelview\nGenerate a panel view of the treatment variable over time for each unit.\n\n\n\n\n\n\nPyFixest internals and utilities\n\n\n\nestimation.demean\nDemean an array.\n\n\nestimation.detect_singletons\nDetect singleton fixed effects in a dataset.\n\n\nestimation.model_matrix_fixest\nCreate model matrices for fixed effects estimation.",
    "crumbs": [
      "Function Reference",
      "PyFixest Function Reference"
    ]
  },
  {
    "objectID": "reference/index.html#estimation-functions",
    "href": "reference/index.html#estimation-functions",
    "title": "PyFixest Function Reference",
    "section": "",
    "text": "User facing estimation functions\n\n\n\nestimation.api.feols.feols\nEstimate a linear regression models with fixed effects using fixest formula syntax.\n\n\nestimation.api.fepois.fepois\nEstimate Poisson regression model with fixed effects using the ppmlhdfe algorithm.\n\n\nestimation.api.feglm.feglm\nEstimate GLM regression models with fixed effects.\n\n\nestimation.api.quantreg.quantreg\nFit a quantile regression model using the interior point algorithm from Portnoy and Koenker (1997).\n\n\ndid.estimation.did2s\nEstimate a Difference-in-Differences model using Gardner’s two-step DID2S estimator.\n\n\ndid.estimation.lpdid\nLocal projections approach to estimation.\n\n\ndid.estimation.event_study\nEstimate Event Study Model.\n\n\nestimation.bonferroni\nCompute Bonferroni adjusted p-values for multiple hypothesis testing.\n\n\nestimation.rwolf\nCompute Romano-Wolf adjusted p-values for multiple hypothesis testing.",
    "crumbs": [
      "Function Reference",
      "PyFixest Function Reference"
    ]
  },
  {
    "objectID": "reference/index.html#estimation-classes",
    "href": "reference/index.html#estimation-classes",
    "title": "PyFixest Function Reference",
    "section": "",
    "text": "Details on Methods and Attributes\n\n\n\nestimation.feols_.Feols\nNon user-facing class to estimate a linear regression via OLS.\n\n\nestimation.fepois_.Fepois\nEstimate a Poisson regression model.\n\n\nestimation.feiv_.Feiv\nNon user-facing class to estimate an IV model using a 2SLS estimator.\n\n\nestimation.feglm_.Feglm\nAbstract base class for the estimation of a fixed-effects GLM model.\n\n\nestimation.felogit_.Felogit\nClass for the estimation of a fixed-effects logit model.\n\n\nestimation.feprobit_.Feprobit\nClass for the estimation of a fixed-effects probit model.\n\n\nestimation.fegaussian_.Fegaussian\nClass for the estimation of a fixed-effects GLM with normal errors.\n\n\nestimation.feols_compressed_.FeolsCompressed\nNon-user-facing class for compressed regression with fixed effects.",
    "crumbs": [
      "Function Reference",
      "PyFixest Function Reference"
    ]
  },
  {
    "objectID": "reference/index.html#summarize-and-visualize",
    "href": "reference/index.html#summarize-and-visualize",
    "title": "PyFixest Function Reference",
    "section": "",
    "text": "Post-Processing of Estimation Results\n\n\n\ndid.visualize.panelview\nGenerate a panel view of the treatment variable over time for each unit.\n\n\nreport.summary\nPrint a summary of estimation results for each estimated model.\n\n\nreport.etable\nGenerate a table summarizing the results of multiple regression models.\n\n\nreport.dtable\nGenerate descriptive statistics tables and create a booktab style table in\n\n\nreport.coefplot\nPlot model coefficients with confidence intervals.\n\n\nreport.iplot\nPlot model coefficients for variables interacted via “i()” syntax, with\n\n\ndid.visualize.panelview\nGenerate a panel view of the treatment variable over time for each unit.",
    "crumbs": [
      "Function Reference",
      "PyFixest Function Reference"
    ]
  },
  {
    "objectID": "reference/index.html#misc-utilities",
    "href": "reference/index.html#misc-utilities",
    "title": "PyFixest Function Reference",
    "section": "",
    "text": "PyFixest internals and utilities\n\n\n\nestimation.demean\nDemean an array.\n\n\nestimation.detect_singletons\nDetect singleton fixed effects in a dataset.\n\n\nestimation.model_matrix_fixest\nCreate model matrices for fixed effects estimation.",
    "crumbs": [
      "Function Reference",
      "PyFixest Function Reference"
    ]
  },
  {
    "objectID": "reference/estimation.fepois_.Fepois.html",
    "href": "reference/estimation.fepois_.Fepois.html",
    "title": "estimation.fepois_.Fepois",
    "section": "",
    "text": "estimation.fepois_.Fepois(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    tol,\n    maxiter,\n    solver='np.linalg.solve',\n    demeaner_backend='numba',\n    context=0,\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    sample_split_var=None,\n    sample_split_value=None,\n    separation_check=None,\n)\nEstimate a Poisson regression model.\nNon user-facing class to estimate a Poisson regression model via Iterated Weighted Least Squares (IWLS).\nInherits from the Feols class. Users should not directly instantiate this class, but rather use the fepois() function. Note that no demeaning is performed in this class: demeaning is performed in the FixestMulti class (to allow for caching of demeaned variables for multiple estimation).\nThe method implements the algorithm from Stata’s ppmlhdfe module.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_Y\nnp.ndarray\nThe demeaned dependent variable, a two-dimensional numpy array.\n\n\n_X\nnp.ndarray\nThe demeaned independent variables, a two-dimensional numpy array.\n\n\n_fe\nnp.ndarray\nFixed effects, a two-dimensional numpy array or None.\n\n\nweights\nnp.ndarray\nWeights, a one-dimensional numpy array or None.\n\n\ncoefnames\nlist[str]\nNames of the coefficients in the design matrix X.\n\n\ndrop_singletons\nbool\nWhether to drop singleton fixed effects.\n\n\ncollin_tol\nfloat\nTolerance level for the detection of collinearity.\n\n\nmaxiter\nOptional[int], default=25\nMaximum number of iterations for the IRLS algorithm.\n\n\ntol\nOptional[float], default=1e-08\nTolerance level for the convergence of the IRLS algorithm.\n\n\nsolver\nstr, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n\n\ndemeaner_backend\nDemeanerBackendOptions.\nThe backend used for demeaning.\n\n\nfixef_tol\nfloat, default = 1e-06.\nTolerance level for the convergence of the demeaning algorithm.\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n\n\nweights_name\nOptional[str]\nName of the weights variable.\n\n\nweights_type\nOptional[str]\nType of weights variable.\n\n\n_data\npd.DataFrame\nThe data frame used in the estimation. None if arguments lean = True or store_data = False.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_fit\nFit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS).\n\n\npredict\nReturn predicted values from regression model.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nresid\nReturn residuals from regression model.\n\n\nto_array\nTurn estimation DataFrames to np arrays.\n\n\n\n\n\nestimation.fepois_.Fepois.get_fit()\nFit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbeta_hat\nnp.ndarray\nEstimated coefficients.\n\n\nY_hat\nnp.ndarray\nEstimated dependent variable.\n\n\nu_hat\nnp.ndarray\nEstimated residuals.\n\n\nweights\nnp.ndarray\nWeights (from the last iteration of the IRLS algorithm).\n\n\nX\nnp.ndarray\nDemeaned independent variables (from the last iteration of the IRLS algorithm).\n\n\nZ\nnp.ndarray\nDemeaned independent variables (from the last iteration of the IRLS algorithm).\n\n\nY\nnp.ndarray\nDemeaned dependent variable (from the last iteration of the IRLS algorithm).\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nReturn predicted values from regression model.\nReturn a flat np.array with predicted values of the regression model. If new fixed effect levels are introduced in newdata, predicted values for such observations will be set to NaN.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nUnion[None, pd.DataFrame]\nA pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.\nNone\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\ntype\nstr\nThe type of prediction to be computed. Can be either “response” (default) or “link”. If type=“response”, the output is at the level of the response variable, i.e., it is the expected predictor E(Y|X). If “link”, the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta.\n'link'\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.fepois_.Fepois.resid(type='response')\nReturn residuals from regression model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nThe type of residuals to be computed. Can be either “response” (default) or “working”.\n'response'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nA flat array with the residuals of the regression model.\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.to_array()\nTurn estimation DataFrames to np arrays.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.fepois_.Fepois"
    ]
  },
  {
    "objectID": "reference/estimation.fepois_.Fepois.html#attributes",
    "href": "reference/estimation.fepois_.Fepois.html#attributes",
    "title": "estimation.fepois_.Fepois",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n_Y\nnp.ndarray\nThe demeaned dependent variable, a two-dimensional numpy array.\n\n\n_X\nnp.ndarray\nThe demeaned independent variables, a two-dimensional numpy array.\n\n\n_fe\nnp.ndarray\nFixed effects, a two-dimensional numpy array or None.\n\n\nweights\nnp.ndarray\nWeights, a one-dimensional numpy array or None.\n\n\ncoefnames\nlist[str]\nNames of the coefficients in the design matrix X.\n\n\ndrop_singletons\nbool\nWhether to drop singleton fixed effects.\n\n\ncollin_tol\nfloat\nTolerance level for the detection of collinearity.\n\n\nmaxiter\nOptional[int], default=25\nMaximum number of iterations for the IRLS algorithm.\n\n\ntol\nOptional[float], default=1e-08\nTolerance level for the convergence of the IRLS algorithm.\n\n\nsolver\nstr, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n\n\ndemeaner_backend\nDemeanerBackendOptions.\nThe backend used for demeaning.\n\n\nfixef_tol\nfloat, default = 1e-06.\nTolerance level for the convergence of the demeaning algorithm.\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n\n\nweights_name\nOptional[str]\nName of the weights variable.\n\n\nweights_type\nOptional[str]\nType of weights variable.\n\n\n_data\npd.DataFrame\nThe data frame used in the estimation. None if arguments lean = True or store_data = False.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.fepois_.Fepois"
    ]
  },
  {
    "objectID": "reference/estimation.fepois_.Fepois.html#methods",
    "href": "reference/estimation.fepois_.Fepois.html#methods",
    "title": "estimation.fepois_.Fepois",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_fit\nFit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS).\n\n\npredict\nReturn predicted values from regression model.\n\n\nprepare_model_matrix\nPrepare model inputs for estimation.\n\n\nresid\nReturn residuals from regression model.\n\n\nto_array\nTurn estimation DataFrames to np arrays.\n\n\n\n\n\nestimation.fepois_.Fepois.get_fit()\nFit a Poisson Regression Model via Iterated Weighted Least Squares (IWLS).\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbeta_hat\nnp.ndarray\nEstimated coefficients.\n\n\nY_hat\nnp.ndarray\nEstimated dependent variable.\n\n\nu_hat\nnp.ndarray\nEstimated residuals.\n\n\nweights\nnp.ndarray\nWeights (from the last iteration of the IRLS algorithm).\n\n\nX\nnp.ndarray\nDemeaned independent variables (from the last iteration of the IRLS algorithm).\n\n\nZ\nnp.ndarray\nDemeaned independent variables (from the last iteration of the IRLS algorithm).\n\n\nY\nnp.ndarray\nDemeaned dependent variable (from the last iteration of the IRLS algorithm).\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nReturn predicted values from regression model.\nReturn a flat np.array with predicted values of the regression model. If new fixed effect levels are introduced in newdata, predicted values for such observations will be set to NaN.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nUnion[None, pd.DataFrame]\nA pd.DataFrame with the new data, to be used for prediction. If None (default), uses the data used for fitting the model.\nNone\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\ntype\nstr\nThe type of prediction to be computed. Can be either “response” (default) or “link”. If type=“response”, the output is at the level of the response variable, i.e., it is the expected predictor E(Y|X). If “link”, the output is at the level of the explanatory variables, i.e., the linear predictor X @ beta.\n'link'\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.prepare_model_matrix()\nPrepare model inputs for estimation.\n\n\n\nestimation.fepois_.Fepois.resid(type='response')\nReturn residuals from regression model.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\nstr\nThe type of residuals to be computed. Can be either “response” (default) or “working”.\n'response'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nA flat array with the residuals of the regression model.\n\n\n\n\n\n\n\nestimation.fepois_.Fepois.to_array()\nTurn estimation DataFrames to np arrays.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.fepois_.Fepois"
    ]
  },
  {
    "objectID": "reference/estimation.feiv_.Feiv.html",
    "href": "reference/estimation.feiv_.Feiv.html",
    "title": "estimation.feiv_.Feiv",
    "section": "",
    "text": "estimation.feiv_.Feiv(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    solver='scipy.linalg.solve',\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    context=0,\n    sample_split_var=None,\n    sample_split_value=None,\n)\nNon user-facing class to estimate an IV model using a 2SLS estimator.\nInherits from the Feols class. Users should not directly instantiate this class, but rather use the feols() function. Note that no demeaning is performed in this class: demeaning is performed in the FixestMulti class (to allow for caching of demeaned variables for multiple estimation).",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feiv_.Feiv"
    ]
  },
  {
    "objectID": "reference/estimation.feiv_.Feiv.html#parameters",
    "href": "reference/estimation.feiv_.Feiv.html#parameters",
    "title": "estimation.feiv_.Feiv",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nY\nnp.ndarray\nDependent variable, a two-dimensional np.array.\nrequired\n\n\nX\nnp.ndarray\nIndependent variables, a two-dimensional np.array.\nrequired\n\n\nendgvar\nnp.ndarray\nEndogenous Indenpendent variables, a two-dimensional np.array.\nrequired\n\n\nZ\nnp.ndarray\nInstruments, a two-dimensional np.array.\nrequired\n\n\nweights\nnp.ndarray\nWeights, a one-dimensional np.array.\nrequired\n\n\ncoefnames_x\nlist\nNames of the coefficients of X.\nrequired\n\n\ncoefnames_z\nlist\nNames of the coefficients of Z.\nrequired\n\n\ncollin_tol\nfloat\nTolerance for collinearity check.\nrequired\n\n\nsolver\nLiteral['np.linalg.lstsq', 'np.linalg.solve', 'scipy.linalg.solve', 'scipy.sparse.linalg.lsqr', 'jax']\n“scipy.sparse.linalg.lsqr”, “jax”], default is “scipy.linalg.solve”. Solver to use for the estimation.\n'scipy.linalg.solve'\n\n\ndemeaner_backend\nDemeanerBackendOptions\nThe backend to use for demeaning. Can be either “numba”, “jax”, or “rust”. Defaults to “numba”.\n'numba'\n\n\nweights_name\nOptional[str]\nName of the weights variable.\nrequired\n\n\nweights_type\nOptional[str]\nType of the weights variable. Either “aweights” for analytic weights or “fweights” for frequency weights.\nrequired",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feiv_.Feiv"
    ]
  },
  {
    "objectID": "reference/estimation.feiv_.Feiv.html#attributes",
    "href": "reference/estimation.feiv_.Feiv.html#attributes",
    "title": "estimation.feiv_.Feiv",
    "section": "Attributes",
    "text": "Attributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_Z\nnp.ndarray\nProcessed instruments after handling multicollinearity.\n\n\n_weights_type_feiv\nstr\nType of the weights variable defined in Feiv class. Either “aweights” for analytic weights or “fweights” for frequency weights.\n\n\n_coefnames_z\nlist\nNames of coefficients for Z after handling multicollinearity.\n\n\n_collin_vars_z\nlist\nVariables identified as collinear in Z.\n\n\n_collin_index_z\nlist\nIndices of collinear variables in Z.\n\n\n_is_iv\nbool\nIndicator if instrumental variables are used.\n\n\n_support_crv3_inference\nbool\nIndicator for supporting CRV3 inference.\n\n\n_support_iid_inference\nbool\nIndicator for supporting IID inference.\n\n\n_tZX\nnp.ndarray\nTranspose of Z times X.\n\n\n_tXZ\nnp.ndarray\nTranspose of X times Z.\n\n\n_tZy\nnp.ndarray\nTranspose of Z times Y.\n\n\n_tZZinv\nnp.ndarray\nInverse of transpose of Z times Z.\n\n\n_beta_hat\nnp.ndarray\nEstimated regression coefficients.\n\n\n_Y_hat_link\nnp.ndarray\nPredicted values of the regression model.\n\n\n_u_hat\nnp.ndarray\nResiduals of the regression model.\n\n\n_scores\nnp.ndarray\nScores used in the regression.\n\n\n_hessian\nnp.ndarray\nHessian matrix used in the regression.\n\n\n_bread\nnp.ndarray\nBread matrix used in the regression.\n\n\n_pi_hat\nnp.ndarray\nEstimated coefficients from 1st stage regression\n\n\n_X_hat\nnp.ndarray\nPredicted values of the 1st stage regression\n\n\n_v_hat\nnp.ndarray\nResiduals of the 1st stage regression\n\n\n_model_1st_stage\nAny\nfeols object of 1st stage regression. It contains various results and diagnostics from the fixed effects OLS regression.\n\n\n_endogvar_1st_stage\nnp.ndarray\nUnweihgted Endogenous independent variable vector\n\n\n_Z_1st_stage\nnp.ndarray\nUnweighted instruments vector to be used for 1st stage\n\n\n_non_exo_instruments\nlist\nList of instruments name excluding exogenous independent vars.\n\n\n__p_iv\nscalar\nNumber of instruments listed in _non_exo_instruments\n\n\n_f_stat_1st_stage\nscalar\nF-statistics of First Stage regression for evaluation of IV weakness. The computed F-statistics test the following null hypothesis : # H0 : β_{z_1} = 0 & … & β_{z_{p_iv}} = 0 where z_1, …, z_{p_iv} # are the instrument variables # H1 : H0 does not hold Note that this F-statistics is adjusted to heteroskedasticity / clusters if users set specification of variance-covariance matrix type\n\n\n_eff_F\nscalar\nEffective F-statistics of first stage regression as in Olea and Pflueger 2013\n\n\n_data\npd.DataFrame\nThe data frame used in the estimation. None if arguments lean = True or store_data = False.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feiv_.Feiv"
    ]
  },
  {
    "objectID": "reference/estimation.feiv_.Feiv.html#raises",
    "href": "reference/estimation.feiv_.Feiv.html#raises",
    "title": "estimation.feiv_.Feiv",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf Z is not a two-dimensional array.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feiv_.Feiv"
    ]
  },
  {
    "objectID": "reference/estimation.feiv_.Feiv.html#methods",
    "href": "reference/estimation.feiv_.Feiv.html#methods",
    "title": "estimation.feiv_.Feiv",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nIV_Diag\nImplement IV diagnostic tests.\n\n\nIV_weakness_test\nImplement IV weakness test (F-test).\n\n\ndemean\nDemean instruments and endogeneous variable.\n\n\ndrop_multicol_vars\nDrop multicollinear variables in matrix of instruments Z.\n\n\neff_F\nCompute Effective F stat (Olea and Pflueger 2013).\n\n\nfirst_stage\nImplement First stage regression.\n\n\nget_fit\nFit a IV model using a 2SLS estimator.\n\n\nto_array\nTransform estimation DataFrames to arrays.\n\n\nwls_transform\nTransform variables for WLS estimation.\n\n\n\n\nIV_Diag\nestimation.feiv_.Feiv.IV_Diag(statistics=None)\nImplement IV diagnostic tests.\n\nNotes\nThis method covers diagnostic tests related with IV regression. We currently have IV weak tests only. More test will be updated in future updates!\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstatistics\nlist[str]\nList of IV diagnostic statistics\nNone\n\n\n\n\n\nExample\nThe following is an example usage of this method:\n::: {#a4f8f095 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom pyfixest.estimation import feols\n\n# Set random seed for reproducibility\nnp.random.seed(1)\n\n# Number of observations\nn = 1000\n\n# Simulate the data\n# Instrumental variable\nz = np.random.binomial(1, 0.5, size=n)\nz2 = np.random.binomial(1, 0.5, size=n)\n\n# Endogenous variable\nd = 0.5 * z + 1.5 * z2 + np.random.normal(size=n)\n\n# Control variables\nc1 = np.random.normal(size=n)\nc2 = np.random.normal(size=n)\n\n# Outcome variable\ny = 1.0 + 1.5 * d + 0.8 * c1 + 0.5 * c2 + np.random.normal(size=n)\n\n# Cluster variable\ncluster = np.random.randint(1, 50, size=n)\nweights = np.random.uniform(1, 3, size=n)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'd': d,\n    'y': y,\n    'z': z,\n    'z2': z2,\n    'c1': c1,\n    'c2': c2,\n    'cluster': cluster,\n    'weights': weights\n})\n\nvcov_detail = \"iid\"\n\n# Fit OLS model\nfit_ols = feols(\"y ~ 1 + d + c1 + c2\", data=data, vcov=vcov_detail)\n\n# Fit IV model\nfit_iv = feols(\"y ~ 1 + c1 + c2 | d ~ z\", data=data,\n         vcov=vcov_detail,\n         weights=\"weights\")\nfit_iv.first_stage()\nF_stat_pf = fit_iv._f_stat_1st_stage\nfit_iv.IV_Diag()\nF_stat_eff_pf = fit_iv._eff_F\n\nprint(\"(Unadjusted) F stat :\", F_stat_pf)\nprint(\"Effective F stat :\", F_stat_eff_pf)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n            &lt;div id=\"R02a38\"&gt;&lt;/div&gt;\n            &lt;script type=\"text/javascript\" data-lets-plot-script=\"library\"&gt;\n                if(!window.letsPlotCallQueue) {\n                    window.letsPlotCallQueue = [];\n                }; \n                window.letsPlotCall = function(f) {\n                    window.letsPlotCallQueue.push(f);\n                };\n                (function() {\n                    var script = document.createElement(\"script\");\n                    script.type = \"text/javascript\";\n                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.7.3/js-package/distr/lets-plot.min.js\";\n                    script.onload = function() {\n                        window.letsPlotCall = function(f) {f();};\n                        window.letsPlotCallQueue.forEach(function(f) {f();});\n                        window.letsPlotCallQueue = [];\n                        \n                    };\n                    script.onerror = function(event) {\n                        window.letsPlotCall = function(f) {};    // noop\n                        window.letsPlotCallQueue = [];\n                        var div = document.createElement(\"div\");\n                        div.style.color = 'darkred';\n                        div.textContent = 'Error loading Lets-Plot JS';\n                        document.getElementById(\"R02a38\").appendChild(div);\n                    };\n                    var e = document.getElementById(\"R02a38\");\n                    e.appendChild(script);\n                })()\n            &lt;/script&gt;\n            \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n            &lt;div id=\"fn79yK\"&gt;&lt;/div&gt;\n            &lt;script type=\"text/javascript\" data-lets-plot-script=\"library\"&gt;\n                if(!window.letsPlotCallQueue) {\n                    window.letsPlotCallQueue = [];\n                }; \n                window.letsPlotCall = function(f) {\n                    window.letsPlotCallQueue.push(f);\n                };\n                (function() {\n                    var script = document.createElement(\"script\");\n                    script.type = \"text/javascript\";\n                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.7.3/js-package/distr/lets-plot.min.js\";\n                    script.onload = function() {\n                        window.letsPlotCall = function(f) {f();};\n                        window.letsPlotCallQueue.forEach(function(f) {f();});\n                        window.letsPlotCallQueue = [];\n                        \n                    };\n                    script.onerror = function(event) {\n                        window.letsPlotCall = function(f) {};    // noop\n                        window.letsPlotCallQueue = [];\n                        var div = document.createElement(\"div\");\n                        div.style.color = 'darkred';\n                        div.textContent = 'Error loading Lets-Plot JS';\n                        document.getElementById(\"fn79yK\").appendChild(div);\n                    };\n                    var e = document.getElementById(\"fn79yK\");\n                    e.appendChild(script);\n                })()\n            &lt;/script&gt;\n            \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Unadjusted) F stat : 52.81535560457488\nEffective F stat : 48.61288119858695\n```\n:::\n:::\n\n\n\nIV_weakness_test\nestimation.feiv_.Feiv.IV_weakness_test(iv_diag_statistics=None)\nImplement IV weakness test (F-test).\nThis method covers hetero-robust and clustered-robust F statistics. It produces two statistics:\n\nself._f_stat_1st_stage: F statistics of first stage regression\nself._eff_F: Effective F statistics (Olea and Pflueger 2013) of first stage regression\n\n\nNotes\n“self._f_stat_1st_stage” is adjusted to the specification of vcov. If vcov_detail = “iid”, F statistics is not adjusted, otherwise it is always adjusted.\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niv_diag_statistics\nlist\nList of IV weakness statistics\nNone\n\n\n\n\n\n\ndemean\nestimation.feiv_.Feiv.demean()\nDemean instruments and endogeneous variable.\n\n\ndrop_multicol_vars\nestimation.feiv_.Feiv.drop_multicol_vars()\nDrop multicollinear variables in matrix of instruments Z.\n\n\neff_F\nestimation.feiv_.Feiv.eff_F()\nCompute Effective F stat (Olea and Pflueger 2013).\n\n\nfirst_stage\nestimation.feiv_.Feiv.first_stage()\nImplement First stage regression.\n\n\nget_fit\nestimation.feiv_.Feiv.get_fit()\nFit a IV model using a 2SLS estimator.\n\n\nto_array\nestimation.feiv_.Feiv.to_array()\nTransform estimation DataFrames to arrays.\n\n\nwls_transform\nestimation.feiv_.Feiv.wls_transform()\nTransform variables for WLS estimation.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feiv_.Feiv"
    ]
  },
  {
    "objectID": "marginaleffects.html",
    "href": "marginaleffects.html",
    "title": "Marginal Effects and Hypothesis Tests via marginaleffects",
    "section": "",
    "text": "We can compute marginal effects and linear and non-linear hypothesis tests via the excellent marginaleffects package.\nfrom marginaleffects import hypotheses\n\nimport pyfixest as pf\n\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1 + X2\", data=data)\n\nfit.tidy()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Intercept\n      0.888779\n      0.108422\n      8.197374\n      8.881784e-16\n      0.676016\n      1.101542\n    \n    \n      X1\n      -0.992936\n      0.082117\n      -12.091650\n      0.000000e+00\n      -1.154079\n      -0.831792\n    \n    \n      X2\n      -0.176342\n      0.021766\n      -8.101743\n      1.554312e-15\n      -0.219055\n      -0.133630\nSuppose we were interested in testing the hypothesis that \\(X_{1} = X_{2}\\). Given the relatively large differences in coefficients and small standard errors, we will likely reject the null that the two parameters are equal.\nWe can run the formal test via the hypotheses function from the marginaleffects package.\nhypotheses(fit, \"X1 - X2 = 0\")\n\n\nshape: (1, 8)termestimatestd_errorstatisticp_values_valueconf_lowconf_highstrf64f64f64f64f64f64f64\"X1-X2=0\"-0.8165930.085179-9.5867970.0inf-0.983541-0.649646\nAnd indeed, we reject the null of equality of coefficients: we get a p-value of zero and a confidence interval that does not contain 0."
  },
  {
    "objectID": "marginaleffects.html#non-linear-hypothesis-tests-ratio-estimates",
    "href": "marginaleffects.html#non-linear-hypothesis-tests-ratio-estimates",
    "title": "Marginal Effects and Hypothesis Tests via marginaleffects",
    "section": "Non-Linear Hypothesis Tests: Ratio Estimates",
    "text": "Non-Linear Hypothesis Tests: Ratio Estimates\nWe can also test run-linear hypotheses, in which case marginaleffects will automatically compute correct standard errors based on the estimated covariance matrix and the Delta method. This is for example useful for computing inferential statistics for the “relative uplift” in an AB test.\nFor the moment, let’s assume that \\(X1\\) is a randomly assigned treatment variable. As before, \\(Y\\) is our variable / KPI of interest.\nUnder randomization, the model intercept measures the “baseline”, i.e. the population average of \\(Y\\) in the absence of treatment. To compute a relative uplift, we might compute\n\n(fit.coef().xs(\"X1\") / fit.coef().xs(\"Intercept\") - 1) * 100\n\nnp.float64(-211.71906665561212)\n\n\nSo we have a really big negative treatment effect of around minus 212%! To conduct correct inference on this ratio statistic, we need to use the delta method.\n\nThe Multivariate Delta Method\nIn a nutshell, the delta method provides a way to approximate the asympotic distribution of any non-linear transformation \\(g()\\) or one or more random variables.\nIn the case of the ratio statistics, this non-linear transformation can be denoted as \\(g(\\theta_{1}, \\theta_{2}) = \\theta_{1} / \\theta_{2}\\).\nHere’s the Delta Method theorem:\nFirst, we define \\(\\theta = (\\theta_{1}, \\theta_{2})'\\) and \\(\\mu = (\\mu_{1}, \\mu_{2})'\\).\nBy the law of large numbers, we know that\n\\[\n\\sqrt{N} (\\theta - \\mu) \\rightarrow_{d} N(0_{2}, \\Sigma_{2,2}) \\text{ if } N \\rightarrow \\infty.\n\\]\nBy the Delta Method, we can then approximate the limit distribution of \\(g(\\theta)\\) as\n\\[\n\\sqrt{N}  (g(\\theta) - g(\\mu)) \\rightarrow_{d} N(0_{1}, g'(\\theta) \\times \\Sigma \\times g(\\theta)) \\text{ if } N \\rightarrow \\infty.\n\\].\nHere’s a long derivation of how to use the the delta method for inference of ratio statistics.. The key steps from the formula above is to derive the expression for the asymptotic variance $ g’() g()$.\nBut hey - we’re lucky, because marginaleffects will do all this work for us: we don’t have to derive analytic gradients ourselves =)\n\n\nUsing the Delta Method via marginaleffects:\nWe can employ the Delta Method via marginaleffects via the hypotheses function:\n\nhypotheses(fit, \"(X1 / Intercept - 1) * 100 = 0\")\n\n\nshape: (1, 8)termestimatestd_errorstatisticp_values_valueconf_lowconf_highstrf64f64f64f64f64f64f64\"(X1/Intercept-1)*100=0\"-211.7190678.478682-24.9707520.0inf-228.336978-195.101155\n\n\nAs before, we get an estimate of around -212%. Additionally, we obtain a 95% CI via the Delta Method of [-228%, -195%].\nBesides hypopotheses testing, you can do a range of other cool things with the marginaleffects package. For example (and likely unsurprisingly), you can easily compute all sorts of marginal effects for your regression models. For all the details, we highly recommend to take a look at the marginaleffects zoo book!."
  },
  {
    "objectID": "pyfixest-sprint.html",
    "href": "pyfixest-sprint.html",
    "title": "PyFixest Sprint in Heilbronn",
    "section": "",
    "text": "PyFixest Sprint in Heilbronn\nWe’re organizing a PyFixest development sprint in partnership with the appliedAI Institute at their Heilbronn office.\nDates: March 4th–6th 2026.\nInterested in joining? Reach out to Alex with a brief note about your background and motivation. We have some funding available to support student participation.\n\nWhat we’re working on\nOur main goals for the sprint:\n\nRust backend: Finalize the port from Numba to Rust and deprecate the Numba dependency, with continued optimization of our core demeaning algorithm. We still have to port logic for HAC standard errors and randomisation inference.\nGPU acceleration: Continue building out JAX, CuPy, and potentially PyTorch backends (for Mac users), potentially re-implementing the LSMR algorithm by hand & run experiments on pre-conditioning\nInternal refactor: Introduce a cleaner class hierarchy with a proper base estimation class. Currently, all estimation classes inherit from Feols.\nNumPy-style API: Rewrite estimation classes (Feols, Fepois, etc.) to follow sklearn-style conventions. Users should be able to fit a regression model by passing data, X or y to Feols. If data is passed, a Feols.from_formula method creates the design matrix. Core functional estimation APIs (feols(), fepois, etc) remain as they are.\nClean Handling of Regression Weights: The logic for weighted least squares (WLS) is currently a bit hard to follow, as dependent variable and design matrix are pre-multiplied with np.sqrt(weights). By reading the code, it is not always immediately clear if self._X is already weighted, or not yet. This of course can lead to confusion and bugs, which we should strive to avoid =)\n\nWe’re also hoping to make progress on:\n\nStandalone demeaning package: Spin out the demeaning algorithms into a lightweight, cross-language package\nVarying slopes support: Add varying slopes to the demeaning algorithm and extend the formula API\nNarwhals integration: Better support for running analyses with either pandas or polars\nmaketables cleanup: Refactor the codebase and open PRs to third-party packages (doubleML, CausalPy, econML, etc.)\nDocumentation: Add how-to guides and add a statistical documentation in which we explan the math behind all estimators\nInstrumental Variables: We want to support IV models with more than one endogenous variable, and implement a range of diagnostics.\nmoderndid contributions: Port our DiD estimators (Gardner, local projections, Sun & Abraham) to the moderndid package\nTests: The pyfixest test suite has grown quite a lot over time, and resembles more of a djungle than an english garden. It would be great to clean things up little bit. In particular, it would be lovely to move away from rpy2/ use modern features of it.\n\n\n\nWhat would be helpful\nIf you’re excited about econometrics tooling and want to contribute, we’d love to have you. You don’t need to be an expert in Rust or GPU programming. If you’ve been looking for a way to get started with open source, we’d be happy to help you make your first contributions."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Thanks for showing interest in contributing to pyfixest! We appreciate all contributions and constructive feedback, whether that be reporting bugs, requesting new features, or suggesting improvements to documentation.\nIf you’d like to get involved, but are not yet sure how, please feel free to send us an email. Some familiarity with either Python or econometrics will help, but you really don’t need to be a numpy core developer or have published in Econometrica =) We’d be more than happy to invest time to help you get started!\n\n\n\n\n\n\nNotePyFixest Sprint in Heilbronn\n\n\n\nWe’re hosting a PyFixest Sprint with AppliedAI in late February/early March 2026. If you’re interested in contributing to PyFixest in person, learn more and get in touch!\n\n\nFor a comprehensive overview of the codebase architecture and internals, check out the DeepWiki. While not perfect and correct in all regards, we think it is a pretty good starting point to learn about the codebase!"
  },
  {
    "objectID": "contributing.html#overview",
    "href": "contributing.html#overview",
    "title": "Contributing",
    "section": "",
    "text": "Thanks for showing interest in contributing to pyfixest! We appreciate all contributions and constructive feedback, whether that be reporting bugs, requesting new features, or suggesting improvements to documentation.\nIf you’d like to get involved, but are not yet sure how, please feel free to send us an email. Some familiarity with either Python or econometrics will help, but you really don’t need to be a numpy core developer or have published in Econometrica =) We’d be more than happy to invest time to help you get started!\n\n\n\n\n\n\nNotePyFixest Sprint in Heilbronn\n\n\n\nWe’re hosting a PyFixest Sprint with AppliedAI in late February/early March 2026. If you’re interested in contributing to PyFixest in person, learn more and get in touch!\n\n\nFor a comprehensive overview of the codebase architecture and internals, check out the DeepWiki. While not perfect and correct in all regards, we think it is a pretty good starting point to learn about the codebase!"
  },
  {
    "objectID": "contributing.html#quick-start-with-github-codespaces",
    "href": "contributing.html#quick-start-with-github-codespaces",
    "title": "Contributing",
    "section": "Quick Start with GitHub Codespaces",
    "text": "Quick Start with GitHub Codespaces\nIf you are new to open source / Python development, the fastest way to start contributing might be with GitHub Codespaces. You start by forking the repository. You can then launch a codespace by clicking\n\n\n\nOpen in Codespaces\n\n\nThis will launch a github codespace (there is a free tier for 60h a month, thank you github!).\nIf you run\n# set up env and install dependencies\npixi install -e dev\n# compile rust dependencies\npixi run -e dev maturin-develop\npixi will install the development environment and all dependencies.\nNow, create a new Python script debug.py at the root of the repository and paste the following:\n\nimport pyfixest as pf\n\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1\", data=data)\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\nYou can now run the script by typing\npixi r -e dev python3 debug.py\nor run unit tests\npixi run tests-regular\nAfter changing the docs, you can build them by running\n# compile rust in docs environment\npixi run -e dev maturin-develop\n# build, render, preview docs\ndocs-build\ndocs-render\ndocs-preview\nNote that the entire process might take a few minutes."
  },
  {
    "objectID": "contributing.html#reporting-bugs",
    "href": "contributing.html#reporting-bugs",
    "title": "Contributing",
    "section": "Reporting bugs",
    "text": "Reporting bugs\nWe use GitHub issues to track bugs. You can report a bug by opening a new issue or contribute to an existing issue if related to the bug you are reporting.\nBefore creating a bug report, please check that your bug has not already been reported, and that your bug exists on the latest version of pyfixest. If you find a closed issue that seems to report the same bug you’re experiencing, open a new issue and include a link to the original issue in your issue description.\nPlease include as many details as possible in your bug report. The information helps the maintainers resolve the issue faster."
  },
  {
    "objectID": "contributing.html#suggesting-enhancements",
    "href": "contributing.html#suggesting-enhancements",
    "title": "Contributing",
    "section": "Suggesting enhancements",
    "text": "Suggesting enhancements\nWe use GitHub issues to track bugs and suggested enhancements. You can suggest an enhancement by opening a new feature request. Before creating an enhancement suggestion, please check that a similar issue does not already exist.\nPlease describe the behavior you want and why, and provide examples of how pyfixest would be used if your feature were added."
  },
  {
    "objectID": "contributing.html#contributing-to-the-codebase",
    "href": "contributing.html#contributing-to-the-codebase",
    "title": "Contributing",
    "section": "Contributing to the codebase",
    "text": "Contributing to the codebase\n\nSetting up your local environment\nWould you like to contribute to pyfixest, or run some of the unit tests locally? Awesome! Here’s how you can get started:\nFirst, you’ll want to fork the pyfixest GitHub repository. Then, clone your forked repo with git:\ngit clone https://github.com/&lt;username&gt;/pyfixest.git\ncd pyfixest\nTo work on pyfixest, you’ll need Python and R installed. If you’re planning to work on the documentation, be sure to have Quarto installed as well. Note: an R installation is only needed if you plan to run the unit tests locally.\nFor guidance on installing R and Python, check out the sections below.\n\n\nPackage Management via pixi\nPyFixest is using pixi.\nTo install pixi, just follow the installation instructions from the pixi documentation.\nOnce pixi is installed, you can initialize the project environment and install all dependencies with\ncd path-to-pyfixest\npixi install\nAfter installation, you can activate a custom pixi environment for pyfixest by typing:\npixi shell\nYou’ll now be in the pixi environment and ready to go!\nFor most development tasks, it’s best to activate the development environment since it includes all the necessary dependencies for development.\npixi shell --environment dev    # open the dev environment\n\n\nPixi tasks\nTo help with development, we’ve included several handy pixi tasks.\nFor example, we use ruff and pre-commit to ensure code consistency across the project.\nTo run (and install) all pre-commit hooks, you can run\npixi run -e lint pre-commit\nWe’ve included other tasks to help with testing. Almost all the necessary dependencies to run tests are included in the dev environment, except for R packages unavailable through conda-forge.\nIn a first step, we need to compile the provided Rust code, which we can do by running\npixi run -e dev maturin-develop\nNote that you do not require a global Rust installation - we install Rust via conda!\nWe can then run all dev tasks:\n# attempt to install non-conda R dependencies\npixi run install-r-extended\n# run all tests via pytest\npixi run -e dev tests\n# run all tests excluding very computationally demanding tests or R-based tests\npixi run -e dev tests-regular\n# run all -e dev tests that depend on the extra R dependencies\n# rerun failed tests\npixi run -e dev tests-against-r-extended\npixi run -e dev tests-rerun\nBuilding the documentation is also straightforward. We’ve got tasks to build, render, and preview the docs. As before, we first need to compile the Rust extensions, this time in the docs env, by running\npixi run -e docs maturin-develop\nThen we can build the documentation with quartodoc.\n# Build documentation and website\npixi run -e docs docs-build\n# render the docs\npixi run -e docs docs-render\n# preview the docs\npixi run -e docs docs-preview\nKeep in mind that you’ll need quarto installed to build the documentation locally."
  },
  {
    "objectID": "contributing.html#installing-python-r-and-quarto",
    "href": "contributing.html#installing-python-r-and-quarto",
    "title": "Contributing",
    "section": "Installing Python, R and Quarto",
    "text": "Installing Python, R and Quarto\n\nInstalling Python\nThe minimal Python version to develop pyfixest is 3.9. You can installed it on Mac/Linux via Hombrew:\nbrew install python@3.11 # specify the version of python you prefer\nOn Windows via Winget:\nwinget install -e --id Python.Python.3.11\n\n\nInstalling R\nNote that R and R dependencies available through conda-forge are installed by pixi to the local project if you use the dev environment. Some extra R dependencies may require additional development tools not included in the environment, for example:\nDepending on your local set up, you may need to install additional libraries to compile those extra dependencies, like a version of gcc and cmake.\n\n\nInstalling Quarto\nDocumentation for pyfixest is written, compiled, and published using Quarto.\nTo install Quarto, run:\nOn MacOS via Homebrew:\nbrew install --cask quarto\nOn Linux (Ubuntu using gdebi):\nsudo curl -o quarto-linux-amd64.deb -L &lt;https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb&gt;\nsudo gdebi quarto-linux-amd64.deb\nOn Windows:\nscoop bucket add extras\nscoop install extras/quarto"
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "PyFixest is a Python package for fast high-dimensional fixed effects regression. Like many open-source software projects, PyFixest builds on work and ideas first developed in other packages. In this section, we want to acknowledge and express our appreciation for the authors of these packages and their creativity and hard work."
  },
  {
    "objectID": "acknowledgements.html#by-functionality",
    "href": "acknowledgements.html#by-functionality",
    "title": "Acknowledgements",
    "section": "By functionality",
    "text": "By functionality\n\nPoisson regression\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nppmlhdfe\nStata\nPyFixest’s Poisson estimator (fepois) implements the ppmlhdfe algorithm as described in Correia, Guimaraes & Zylkin (2020), including its separation detection and acceleration strategies. Test datasets for separation examples are taken from the ppmlhdfe repository (MIT license)\n\n\n\n\n\nInstrumental variables\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nivDiag\nR\nThe IV diagnostics implementations are validated against ivDiag.\n\n\n\n\n\nQuantile regression\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nquantreg\nR\nPyFixest’s quantreg() implementation is tested against R’s quantreg package (by Roger Koenker) for coefficient and NID standard error equivalence\n\n\nqreg2\nStata\nPyFixest’s cluster-robust standard errors for quantile regression are tested against Stata’s qreg2 output, which is based on work by Parente & Santos Silva (2016).\n\n\n\n\n\nDifference-in-Differences\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\ndid2s\nR\nPyFixest’s DID2S estimator’s API is strongly inspired by Kyle Butts’ R package (MIT license) and we have relied on Kyle’s writeup of the method for our own implementation. Tests compare coefficients and standard errors against the R implementation\n\n\nlpdid\nR\nPyFixest’s local-projections DID estimator is highly influenced by Alex Cardazzi’s R code (published under MIT) for the lpdid package. We also test against the R implementation\n\n\nlpdid\nStata\nWe also test our implementation against Daniel Busch’s and Daniele Girardi’s Stata implementation of local-projections DID.\n\n\n\n\n\nPanel data visualization\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\npanelView\nR\nPyFixest’s panelview() function for visualizing treatment patterns and outcomes in panel data is inspired by the panelView R package by Mou, Liu and Xu.\n\n\n\n\n\nRandomization inference\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nritest\nR\nPyFixest’s ritest() method’s API heavily borrows from Grant McDermott’s R port and is tested against it.\n\n\nritest\nStata\nGrant’s ritest is itself inspired by Simon Heß ritest Stata package.\n\n\n\n\n\nWild cluster bootstrap\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nwildboottest\nPython\nPyFixest loads classes from wildboottest to run wild bootstrap inference. wildboottest is a Python port of fwildclusterboot.\n\n\nfwildclusterboot\nR\nAn R implementation of the “fast and wild” algorithm by Roodman et al.\n\n\nboottest\nStata\nRoodman et al. (2019). The fast wild cluster bootstrap methodology traces back to Roodman’s Stata boottest package\n\n\n\n\n\nMultiple hypothesis testing (Romano-Wolf)\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nwildrwolf\nR\nPyFixest’s rwolf() Romano-Wolf correction is tested against the wildrwolf R package for both HC and CRV inference.\n\n\nwildwyoung\nR\nAn R implementation of the Westfall-Young correction using the wild bootstrap\n\n\nrwolf\nStata\nA Stata implementation of the Romano-Wolf stepdown procedure that inspired development of rwolf.\n\n\nwyoung\nStata\nA Stata implementation of the Westfall-Young stepdown procedure by Jones, Molitor & Reif.\n\n\n\n\n\nCausal cluster variance\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nTSCB-CCV\nStata\nPailanir & Clarke. PyFixest’s CCV implementation (Abadie et al., QJE 2023) is tested against Daniel Pailanir and Damian Clarke’s Stata implementation. Test data is loaded from Stata .dta files\n\n\n\n\n\nGelbach decomposition\n\n\n\nPackage / Author\nLanguage\nRole\n\n\n\n\nb1x2\nStata\nPyFixest’s decompose() method is tested against hardcoded results from Gelbach’s b1x2 Stata package.\n\n\nApoorva’s Linear Mediation Gist\nPython\nThe initial implementation of Gelbach’s decomposition was based on Apoorva’s gist\n\n\n\n\n\nDemeaning and fixed effects recovery\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nlfe\nR\nWe based our first implementation of the MAP algorithm on the description in the “how lfe works” vignette.\n\n\npyhdfe\nPython\nPyFixest’s demeaning results are tested against pyhdfe to ensure equivalence. pyfixest’s first MVP was built using pyhdfe it ran its demeaning algorithm via pyhdfe MAP algo."
  },
  {
    "objectID": "acknowledgements.html#test-infrastructure",
    "href": "acknowledgements.html#test-infrastructure",
    "title": "Acknowledgements",
    "section": "Test infrastructure",
    "text": "Test infrastructure\nThe following packages are used in PyFixest’s test suite to bridge between Python and R:\n\n\n\nPackage\nLanguage\nRole\n\n\n\n\nrpy2\nPython\nThe bridge between Python and R that powers all cross-language test comparisons"
  },
  {
    "objectID": "acknowledgements.html#other-software",
    "href": "acknowledgements.html#other-software",
    "title": "Acknowledgements",
    "section": "Other Software",
    "text": "Other Software\nHere we list other foundational software without which a project like PyFixest would not be possible:\n\nformulaic\nnumpy\nnumba\npandas\nscipy\nmatplotlib\ngreat-tables / maketables\npyo3"
  },
  {
    "objectID": "reference/did.estimation.lpdid.html",
    "href": "reference/did.estimation.lpdid.html",
    "title": "did.estimation.lpdid",
    "section": "",
    "text": "did.estimation.lpdid(\n    data,\n    yname,\n    idname,\n    tname,\n    gname,\n    vcov=None,\n    pre_window=None,\n    post_window=None,\n    never_treated=0,\n    att=True,\n    xfml=None,\n)\nLocal projections approach to estimation.\nEstimate a Difference-in-Differences / Event Study Model via the Local Projections Approach.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.lpdid"
    ]
  },
  {
    "objectID": "reference/did.estimation.lpdid.html#parameters",
    "href": "reference/did.estimation.lpdid.html#parameters",
    "title": "did.estimation.lpdid",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nDataFrame\nThe DataFrame containing all variables.\nrequired\n\n\nyname\nstr\nThe name of the dependent variable.\nrequired\n\n\nidname\nstr\nThe name of the id variable.\nrequired\n\n\ntname\nstr\nVariable name for calendar period.\nrequired\n\n\ngname\nstr\nUnit-specific time of initial treatment.\nrequired\n\n\nvcov\n(str, dict)\nThe type of inference to employ. Defaults to {“CRV1”: idname}. Options include “iid”, “hetero”, or a dictionary like {“CRV1”: idname}.\nNone\n\n\npre_window\nint\nThe number of periods before the treatment to include in the estimation. Default is the minimum relative year in the data.\nNone\n\n\npost_window\nint\nThe number of periods after the treatment to include in the estimation. Default is the maximum relative year in the data.\nNone\n\n\nnever_treated\nint\nValue in gname indicating units never treated. Default is 0.\n0\n\n\natt\nbool\nIf True, estimates the pooled average treatment effect on the treated (ATT). Default is False.\nTrue\n\n\nxfml\nstr\nFormula for the covariates. Not yet supported.\nNone",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.lpdid"
    ]
  },
  {
    "objectID": "reference/did.estimation.lpdid.html#returns",
    "href": "reference/did.estimation.lpdid.html#returns",
    "title": "did.estimation.lpdid",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataFrame\nA DataFrame with the estimated coefficients.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.lpdid"
    ]
  },
  {
    "objectID": "reference/did.estimation.lpdid.html#examples",
    "href": "reference/did.estimation.lpdid.html#examples",
    "title": "did.estimation.lpdid",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pyfixest as pf\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\n\nfit = pf.lpdid(\n    df_het,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-20,\n    post_window=20,\n    att=False\n)\n\nfit.tidy().head()\nfit.iplot(figsize= [1200, 400], coord_flip=False).show()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n   \n   \n\n\nTo get the ATT, set att=True:\n\nfit = pf.lpdid(\n    df_het,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-20,\n    post_window=20,\n    att=True\n)\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\nN\n\n\n\n\ntreat_diff\n2.506746\n0.07142\n35.0989\n0.0\n2.362287\n2.651206\n5716.0",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.lpdid"
    ]
  },
  {
    "objectID": "reference/estimation.detect_singletons.html",
    "href": "reference/estimation.detect_singletons.html",
    "title": "estimation.detect_singletons",
    "section": "",
    "text": "estimation.detect_singletons(ids)\nDetect singleton fixed effects in a dataset.\nThis function iterates over the columns of a 2D numpy array representing fixed effects to identify singleton fixed effects. An observation is considered a singleton if it is the only one in its group (fixed effect identifier).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nids\nnp.ndarray\nA 2D numpy array representing fixed effects, with a shape of (n_samples, n_features). Elements should be non-negative integers representing fixed effect identifiers.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnumpy.ndarray\nA boolean array of shape (n_samples,), indicating which observations have a singleton fixed effect.\n\n\n\n\n\n\nThe algorithm iterates over columns to identify fixed effects. After each column is processed, it updates the record of non-singleton rows. This approach accounts for the possibility that removing an observation in one column can lead to the emergence of new singletons in subsequent columns.\nFor performance reasons, the input array should be in column-major order. Operating on a row-major array can lead to significant performance losses.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.detect_singletons"
    ]
  },
  {
    "objectID": "reference/estimation.detect_singletons.html#parameters",
    "href": "reference/estimation.detect_singletons.html#parameters",
    "title": "estimation.detect_singletons",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nids\nnp.ndarray\nA 2D numpy array representing fixed effects, with a shape of (n_samples, n_features). Elements should be non-negative integers representing fixed effect identifiers.\nrequired",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.detect_singletons"
    ]
  },
  {
    "objectID": "reference/estimation.detect_singletons.html#returns",
    "href": "reference/estimation.detect_singletons.html#returns",
    "title": "estimation.detect_singletons",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnumpy.ndarray\nA boolean array of shape (n_samples,), indicating which observations have a singleton fixed effect.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.detect_singletons"
    ]
  },
  {
    "objectID": "reference/estimation.detect_singletons.html#notes",
    "href": "reference/estimation.detect_singletons.html#notes",
    "title": "estimation.detect_singletons",
    "section": "",
    "text": "The algorithm iterates over columns to identify fixed effects. After each column is processed, it updates the record of non-singleton rows. This approach accounts for the possibility that removing an observation in one column can lead to the emergence of new singletons in subsequent columns.\nFor performance reasons, the input array should be in column-major order. Operating on a row-major array can lead to significant performance losses.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.detect_singletons"
    ]
  },
  {
    "objectID": "reference/estimation.api.fepois.fepois.html",
    "href": "reference/estimation.api.fepois.fepois.html",
    "title": "estimation.api.fepois.fepois",
    "section": "",
    "text": "estimation.api.fepois.fepois(\n    fml,\n    data,\n    vcov=None,\n    vcov_kwargs=None,\n    weights=None,\n    weights_type='aweights',\n    ssc=None,\n    fixef_rm='singleton',\n    fixef_tol=1e-06,\n    fixef_maxiter=10000,\n    iwls_tol=1e-08,\n    iwls_maxiter=25,\n    collin_tol=1e-09,\n    separation_check=None,\n    solver='scipy.linalg.solve',\n    demeaner_backend='numba',\n    drop_intercept=False,\n    copy_data=True,\n    store_data=True,\n    lean=False,\n    context=None,\n    split=None,\n    fsplit=None,\n)\nEstimate Poisson regression model with fixed effects using the ppmlhdfe algorithm.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.fepois.fepois"
    ]
  },
  {
    "objectID": "reference/estimation.api.fepois.fepois.html#parameters",
    "href": "reference/estimation.api.fepois.fepois.html#parameters",
    "title": "estimation.api.fepois.fepois",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfml\nstr\nA two-sided formula string using fixest formula syntax. Syntax: “Y ~ X1 + X2 | FE1 + FE2”. “|” separates left-hand side and fixed effects. Special syntax includes: - Stepwise regressions (sw, sw0) - Cumulative stepwise regression (csw, csw0) - Multiple dependent variables (Y1 + Y2 ~ X) - Interaction of variables (i(X1,X2)) - Interacted fixed effects (fe1^fe2) Compatible with formula parsing via the formulaic module.\nrequired\n\n\ndata\nDataFrameType\nA pandas or polars dataframe containing the variables in the formula.\nrequired\n\n\nvcov\nUnion[VcovTypeOptions, dict[str, str]]\nType of variance-covariance matrix for inference. Options include “iid”, “hetero”, “HC1”, “HC2”, “HC3”, “NW” for Newey-West HAC standard errors, “DK” for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the vcov_kwargs argument. For time-series HAC, you need to pass the ‘time_id’ column. For panel-HAC, you need to add pass both ‘time_id’ and ‘panel_id’. See vcov_kwargs for details.\nNone\n\n\nvcov_kwargs\nOptional[dict[str, any]]\nAdditional keyword arguments to pass to the vcov function. These keywoards include “lag” for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. “time_id” for the time ID used for NW and DK standard errors, and “panel_id” for the panel identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time periods is always treated as 1. More flexible time-step selection is work in progress.\nNone\n\n\nweights\nUnion[None, str], optional.\nDefault is None. Weights for weighted Poisson regression. If None, all observations are weighted equally. If a string, the name of the column in data that contains the weights.\nNone\n\n\nweights_type\nWeightsTypeOptions\nOptions include aweights or fweights. aweights implement analytic or precision weights, while fweights implement frequency weights. Frequency weights are useful for compressed count data where identical observations are aggregated. For details see this blog post: https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/.\n'aweights'\n\n\nssc\nstr\nA ssc object specifying the small sample correction for inference.\nNone\n\n\nfixef_rm\nFixedRmOptions\nSpecifies whether to drop singleton fixed effects. Can be equal to “singletons” (default) or “none”. “singletons” will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.\n'singleton'\n\n\nfixef_tol\nfloat\nTolerance for the fixed effects demeaning algorithm. Defaults to 1e-06.\n1e-06\n\n\nfixef_maxiter\nint\nMaximum number of iterations for the demeaning algorithm. Defaults to 100,000.\n10000\n\n\niwls_tol\nOptional[float]\nTolerance for IWLS convergence, by default 1e-08.\n1e-08\n\n\niwls_maxiter\nOptional[float]\nMaximum number of iterations for IWLS convergence, by default 25.\n25\n\n\ncollin_tol\nfloat\nTolerance for collinearity check, by default 1e-10.\n1e-09\n\n\nseparation_check\nOptional[list[str]]\nMethods to identify and drop separated observations. Either “fe” or “ir”. Executes “fe” by default (when None).\nNone\n\n\nsolver\nSolverOptions, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n'scipy.linalg.solve'\n\n\ndemeaner_backend\nDemeanerBackendOptions\nThe backend to use for demeaning. Options include: - “numba” (default): CPU-based demeaning using Numba JIT via the Alternating Projections Algorithm. - “rust”: CPU-based demeaning implemented in Rust via the Alternating Projections Algorithm. - “jax”: CPU or GPU-accelerated using JAX (requires jax/jaxlib) via the Alternating Projections Algorithm. - “cupy” or “cupy64”: GPU-accelerated using CuPy with float64 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse matrices (requires cupy & GPU, defaults to scipy/CPU if no GPU available) - “cupy32”: GPU-accelerated using CuPy with float32 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse matrices (requires cupy & GPU, defaults to scipy/CPU and float64 if no GPU available) - “scipy”: Direct application of the Frisch-Waugh-Lovell Theorem on sparse matrice. Forces to use a scipy-sparse backend even when cupy is installed and GPU is available. Defaults to “numba”.\n'numba'\n\n\ndrop_intercept\nbool\nWhether to drop the intercept from the model, by default False.\nFalse\n\n\ncopy_data\nbool\nWhether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of fepois. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you’ll find a column with interacted fixed effects in the data set.\nTrue\n\n\nstore_data\nbool\nWhether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the data attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. predict() or vcov().\nTrue\n\n\nlean\nbool\nFalse by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won’t be possible afterwards.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\nNone\n\n\nsplit\nOptional[str]\nA character string, i.e. ‘split = var’. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.\nNone\n\n\nfsplit\nOptional[str]\nThis argument is the same as split but also includes the full sample as the first estimation.\nNone",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.fepois.fepois"
    ]
  },
  {
    "objectID": "reference/estimation.api.fepois.fepois.html#returns",
    "href": "reference/estimation.api.fepois.fepois.html#returns",
    "title": "estimation.api.fepois.fepois",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nAn instance of the Fepois class or an instance of class FixestMulti for multiple models specified via fml.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.fepois.fepois"
    ]
  },
  {
    "objectID": "reference/estimation.api.fepois.fepois.html#examples",
    "href": "reference/estimation.api.fepois.fepois.html#examples",
    "title": "estimation.api.fepois.fepois",
    "section": "Examples",
    "text": "Examples\nThe fepois() function can be used to estimate a simple Poisson regression model with fixed effects. The following example regresses Y on X1 and X2 with fixed effects for f1 and f2: fixed effects are specified after the | symbol.\n\nimport pyfixest as pf\n\ndata = pf.get_data(model = \"Fepois\")\nfit = pf.fepois(\"Y ~ X1 + X2 | f1 + f2\", data)\nfit.summary()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 2 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\n###\n\nEstimation:  Poisson\nDep. var.: Y, Fixed effects: f1+f2\nInference:  iid\nObservations:  995\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.007 |        0.042 |    -0.157 |      0.875 | -0.089 |   0.076 |\n| X2            |     -0.015 |        0.011 |    -1.317 |      0.188 | -0.037 |   0.007 |\n---\nDeviance: 1068.169 \n\n\nFor more examples on the use of other function arguments, please take a look at the documentation of the feols() function.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.fepois.fepois"
    ]
  },
  {
    "objectID": "reference/estimation.api.feols.feols.html",
    "href": "reference/estimation.api.feols.feols.html",
    "title": "estimation.api.feols.feols",
    "section": "",
    "text": "estimation.api.feols.feols(\n    fml,\n    data,\n    vcov=None,\n    vcov_kwargs=None,\n    weights=None,\n    ssc=None,\n    fixef_rm='singleton',\n    fixef_tol=1e-06,\n    fixef_maxiter=10000,\n    collin_tol=1e-09,\n    drop_intercept=False,\n    copy_data=True,\n    store_data=True,\n    lean=False,\n    weights_type='aweights',\n    solver='scipy.linalg.solve',\n    demeaner_backend='numba',\n    use_compression=False,\n    reps=100,\n    context=None,\n    seed=None,\n    split=None,\n    fsplit=None,\n)\nEstimate a linear regression models with fixed effects using fixest formula syntax.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feols.feols"
    ]
  },
  {
    "objectID": "reference/estimation.api.feols.feols.html#parameters",
    "href": "reference/estimation.api.feols.feols.html#parameters",
    "title": "estimation.api.feols.feols",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfml\nstr\nA three-sided formula string using fixest formula syntax. Syntax: “Y ~ X1 + X2 | FE1 + FE2 | X1 ~ Z1”. “|” separates dependent variable, fixed effects, and instruments. Special syntax includes stepwise regressions, cumulative stepwise regression, multiple dependent variables, interaction of variables (i(X1,X2)), and interacted fixed effects (fe1^fe2).\nrequired\n\n\ndata\nDataFrameType\nA pandas or polars dataframe containing the variables in the formula.\nrequired\n\n\nvcov\nUnion[VcovTypeOptions, dict[str, str]]\nType of variance-covariance matrix for inference. Options include “iid”, “hetero”, “HC1”, “HC2”, “HC3”, “NW” for Newey-West HAC standard errors, “DK” for Driscoll-Kraay HAC standard errors, or a dictionary for CRV1/CRV3 inference. Note that NW and DK require to pass additional keyword arguments via the vcov_kwargs argument. For time-series HAC, you need to pass the ‘time_id’ column. For panel-HAC, you need to add pass both ‘time_id’ and ‘panel_id’. See vcov_kwargs for details.\nNone\n\n\nvcov_kwargs\nOptional[dict[str, any]]\nAdditional keyword arguments to pass to the vcov function. These keywoards include “lag” for the number of lag to use in the Newey-West (NW) and Driscoll-Kraay (DK) HAC standard errors. “time_id” for the time ID used for NW and DK standard errors, and “panel_id” for the panel identifier used for NW and DK standard errors. Currently, the the time difference between consecutive time periods is always treated as 1. More flexible time-step selection is work in progress.\nNone\n\n\nweights\nUnion[None, str], optional.\nDefault is None. Weights for WLS estimation. If None, all observations are weighted equally. If a string, the name of the column in data that contains the weights.\nNone\n\n\nssc\nstr\nA ssc object specifying the small sample correction for inference.\nNone\n\n\nfixef_rm\nFixedRmOptions\nSpecifies whether to drop singleton fixed effects. Can be equal to “singleton” (default), or “none”. “singletons” will drop singleton fixed effects. This will not impact point estimates but it will impact standard errors.\n'singleton'\n\n\ncollin_tol\nfloat\nTolerance for collinearity check, by default 1e-10.\n1e-09\n\n\nfixef_tol\n\nTolerance for the fixed effects demeaning algorithm. Defaults to 1e-06.\n1e-06\n\n\nfixef_maxiter\nint\nMaximum number of iterations for the demeaning algorithm. Defaults to 100,000.\n10000\n\n\ndrop_intercept\nbool\nWhether to drop the intercept from the model, by default False.\nFalse\n\n\ncopy_data\nbool\nWhether to copy the data before estimation, by default True. If set to False, the data is not copied, which can save memory but may lead to unintended changes in the input data outside of fepois. For example, the input data set is re-index within the function. As far as I know, the only other relevant case is when using interacted fixed effects, in which case you’ll find a column with interacted fixed effects in the data set.\nTrue\n\n\nstore_data\nbool\nWhether to store the data in the model object, by default True. If set to False, the data is not stored in the model object, which can improve performance and save memory. However, it will no longer be possible to access the data via the data attribute of the model object. This has impact on post-estimation capabilities that rely on the data, e.g. predict() or vcov().\nTrue\n\n\nlean\nbool\nFalse by default. If True, then all large objects are removed from the returned result: this will save memory but will block the possibility to use many methods. It is recommended to use the argument vcov to obtain the appropriate standard-errors at estimation time, since obtaining different SEs won’t be possible afterwards.\nFalse\n\n\nweights_type\nWeightsTypeOptions\nOptions include aweights or fweights. aweights implement analytic or precision weights, while fweights implement frequency weights. For details see this blog post: https://notstatschat.rbind.io/2020/08/04/weights-in-statistics/.\n'aweights'\n\n\nsolver\nSolverOptions, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n'scipy.linalg.solve'\n\n\ndemeaner_backend\nDemeanerBackendOptions\nThe backend to use for demeaning. Options include: - “numba” (default): CPU-based demeaning using Numba JIT via the Alternating Projections Algorithm. - “rust”: CPU-based demeaning implemented in Rust via the Alternating Projections Algorithm. - “jax”: CPU or GPU-accelerated using JAX (requires jax/jaxlib) via the Alternating Projections Algorithm. - “cupy” or “cupy64”: GPU-accelerated using CuPy with float64 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse matrices (requires cupy & GPU, defaults to scipy/CPU if no GPU available) - “cupy32”: GPU-accelerated using CuPy with float32 precision via direct application of the Frisch-Waugh-Lovell Theorem on sparse matrices (requires cupy & GPU, defaults to scipy/CPU and float64 if no GPU available) - “scipy”: Direct application of the Frisch-Waugh-Lovell Theorem on sparse matrice. Forces to use a scipy-sparse backend even when cupy is installed and GPU is available. Defaults to “numba”.\n'numba'\n\n\nuse_compression\nbool\nWhether to use sufficient statistics to losslessly fit the regression model on compressed data. False by default. If True, the model is estimated on compressed data, which can lead to a significant speed-up for large data sets. See the paper by Wong et al (2021) for more details https://arxiv.org/abs/2102.11297. Note that if use_compression = True, inference is lossless. If standard errors are clustered, a wild cluster bootstrap is employed. Parameters for the wild bootstrap can be specified via the reps and seed arguments. Additionally, note that for one-way fixed effects, the estimation method uses a Mundlak transform to “control” for the fixed effects. For two-way fixed effects, a two-way Mundlak transform is employed. For two-way fixed effects, the Mundlak transform is only identical to a two-way fixed effects model if the data set is a panel. We do not provide any checks for the panel status of the data set.\nFalse\n\n\nreps\nint\nNumber of bootstrap repetitions. Only relevant for boostrap inference applied to compute cluster robust errors when use_compression = True.\n100\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\nNone\n\n\nseed\nOptional[int]\nSeed for the random number generator. Only relevant for boostrap inference applied to compute cluster robust errors when use_compression = True.\nNone\n\n\nsplit\nOptional[str]\nA character string, i.e. ‘split = var’. If provided, the sample is split according to the variable and one estimation is performed for each value of that variable. If you also want to include the estimation for the full sample, use the argument fsplit instead.\nNone\n\n\nfsplit\nOptional[str]\nThis argument is the same as split but also includes the full sample as the first estimation.\nNone",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feols.feols"
    ]
  },
  {
    "objectID": "reference/estimation.api.feols.feols.html#returns",
    "href": "reference/estimation.api.feols.feols.html#returns",
    "title": "estimation.api.feols.feols",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nAn instance of the [Feols(/reference/Feols.qmd) class or FixestMulti class for multiple models specified via fml.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feols.feols"
    ]
  },
  {
    "objectID": "reference/estimation.api.feols.feols.html#examples",
    "href": "reference/estimation.api.feols.feols.html#examples",
    "title": "estimation.api.feols.feols",
    "section": "Examples",
    "text": "Examples\nAs in fixest, the [Feols(/reference/Feols.qmd) function can be used to estimate a simple linear regression model with fixed effects. The following example regresses Y on X1 and X2 with fixed effects for f1 and f2: fixed effects are specified after the | symbol.\n\nimport pyfixest as pf\nimport pandas as pd\nimport numpy as np\n\ndata = pf.get_data()\n\nfit = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data)\nfit.summary()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.924 |        0.056 |   -16.483 |      0.000 | -1.034 |  -0.814 |\n| X2            |     -0.174 |        0.015 |   -11.717 |      0.000 | -0.203 |  -0.145 |\n---\nRMSE: 1.346 R2: 0.659 R2 Within: 0.303 \n\n\nCalling feols() returns an instance of the [Feols(/reference/Feols.qmd) class. The summary() method can be used to print the results.\nAn alternative way to retrieve model results is via the tidy() method, which returns a pandas dataframe with the estimated coefficients, standard errors, t-statistics, and p-values.\n\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n-0.924046\n0.056060\n-16.483063\n0.0\n-1.034065\n-0.814028\n\n\nX2\n-0.174107\n0.014859\n-11.717062\n0.0\n-0.203269\n-0.144946\n\n\n\n\n\n\n\nYou can also access all elements in the tidy data frame by dedicated methods, e.g. fit.coef() for the coefficients, fit.se() for the standard errors, fit.tstat() for the t-statistics, and fit.pval() for the p-values, and fit.confint() for the confidence intervals.\nThe employed type of inference can be specified via the vcov argument. For compatibility with fixest, if vcov is not provided, PyFixest always employs “iid” inference by default starting with pyfixest 0.31.0. Prior to pyfixest 0.31.0, if vcov was not provided, PyFixest would cluster by the first fixed effect if no vcov was provided.\n\nfit1 = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data, vcov=\"iid\")\nfit2 = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data, vcov=\"hetero\")\nfit3 = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data, vcov={\"CRV1\": \"f1\"})\n\nSupported inference types are “iid”, “hetero”, “HC1”, “HC2”, “HC3”, and “CRV1”/“CRV3”. Clustered standard errors are specified via a dictionary, e.g. {\"CRV1\": \"f1\"} for CRV1 inference with clustering by f1 or {\"CRV3\": \"f1\"} for CRV3 inference with clustering by f1. For two-way clustering, you can provide a formula string, e.g. {\"CRV1\": \"f1 + f2\"} for CRV1 inference with clustering by f1.\n\nfit4 = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data, vcov={\"CRV1\": \"f1 + f2\"})\n\nInference can be adjusted post estimation via the vcov method:\n\nfit.summary()\nfit.vcov(\"iid\").summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.924 |        0.056 |   -16.483 |      0.000 | -1.034 |  -0.814 |\n| X2            |     -0.174 |        0.015 |   -11.717 |      0.000 | -0.203 |  -0.145 |\n---\nRMSE: 1.346 R2: 0.659 R2 Within: 0.303 \n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: f1+f2\nInference:  iid\nObservations:  997\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -0.924 |        0.056 |   -16.483 |      0.000 | -1.034 |  -0.814 |\n| X2            |     -0.174 |        0.015 |   -11.717 |      0.000 | -0.203 |  -0.145 |\n---\nRMSE: 1.346 R2: 0.659 R2 Within: 0.303 \n\n\nThe ssc argument specifies the small sample correction for inference. In general, feols() uses all of fixest::feols() defaults, but sets the fixef.K argument to \"none\" whereas the fixest::feols() default is \"nested\". See here for more details: link to github.\nfeols() supports a range of multiple estimation syntax, i.e. you can estimate multiple models in one call. The following example estimates two models, one with fixed effects for f1 and one with fixed effects for f2 using the sw() syntax.\n\nfit = pf.feols(\"Y ~ X1 + X2 | sw(f1, f2)\", data)\ntype(fit)\n\npyfixest.estimation.FixestMulti_.FixestMulti\n\n\nThe returned object is an instance of the FixestMulti class. You can access the results of the first model via fit.fetch_model(0) and the results of the second model via fit.fetch_model(1). You can compare the model results via the etable() function:\n\npf.etable(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n-0.95***\n(0.066)\n-0.979***\n(0.075)\n\n\nX2\n-0.174***\n(0.018)\n-0.175***\n(0.02)\n\n\nfe\n\n\nf2\n-\nx\n\n\nf1\nx\n-\n\n\nstats\n\n\nObservations\n997\n998\n\n\nR2\n0.489\n0.354\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nOther supported multiple estimation syntax include sw0(), csw() and csw0(). While sw() adds variables in a “stepwise” fashion, csw() does so cumulatively.\n\nfit = pf.feols(\"Y ~ X1 + X2 | csw(f1, f2)\", data)\npf.etable(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n-0.95***\n(0.066)\n-0.924***\n(0.056)\n\n\nX2\n-0.174***\n(0.018)\n-0.174***\n(0.015)\n\n\nfe\n\n\nf2\n-\nx\n\n\nf1\nx\nx\n\n\nstats\n\n\nObservations\n997\n997\n\n\nR2\n0.489\n0.659\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nThe sw0() and csw0() syntax are similar to sw() and csw(), but start with a model that excludes the variables specified in sw() and csw():\n\nfit = pf.feols(\"Y ~ X1 + X2 | sw0(f1, f2)\", data)\npf.etable(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n(3)\n\n\n\n\ncoef\n\n\nX1\n-0.993***\n(0.082)\n-0.95***\n(0.066)\n-0.979***\n(0.075)\n\n\nX2\n-0.176***\n(0.022)\n-0.174***\n(0.018)\n-0.175***\n(0.02)\n\n\nIntercept\n0.889***\n(0.108)\n\n\n\n\nfe\n\n\nf2\n-\n-\nx\n\n\nf1\n-\nx\n-\n\n\nstats\n\n\nObservations\n998\n997\n998\n\n\nR2\n0.177\n0.489\n0.354\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nThe feols() function also supports multiple dependent variables. The following example estimates two models, one with Y1 as the dependent variable and one with Y2 as the dependent variable.\n\nfit = pf.feols(\"Y + Y2 ~ X1 | f1 + f2\", data)\npf.etable(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\nY2\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n-0.919***\n(0.06)\n-1.228***\n(0.212)\n\n\nfe\n\n\nf1\nx\nx\n\n\nf2\nx\nx\n\n\nstats\n\n\nObservations\n997\n998\n\n\nR2\n0.609\n0.168\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nIt is possible to combine different multiple estimation operators:\n\nfit = pf.feols(\"Y + Y2 ~ X1 | sw(f1, f2)\", data)\npf.etable(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\nY2\nY\nY2\n\n\n(1)\n(2)\n(3)\n(4)\n\n\n\n\ncoef\n\n\nX1\n-0.949***\n(0.07)\n-1.266***\n(0.211)\n-0.982***\n(0.078)\n-1.301***\n(0.216)\n\n\nfe\n\n\nf2\n-\n-\nx\nx\n\n\nf1\nx\nx\n-\n-\n\n\nstats\n\n\nObservations\n997\n998\n998\n999\n\n\nR2\n0.437\n0.115\n0.302\n0.09\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nIn general, using muliple estimation syntax can improve the estimation time as covariates that are demeaned in one model and are used in another model do not need to be demeaned again: feols() implements a caching mechanism that stores the demeaned covariates.\nAdditionally, you can fit models on different samples via the split and fsplit arguments. The split argument splits the sample according to the variable specified in the argument, while the fsplit argument also includes the full sample in the estimation.\n\nfit = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data, split = \"f1\")\npf.etable(fit)\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 8 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 12 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 5 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 12 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 14 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 9 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 13 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 6 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 6 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 11 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 9 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 14 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 15 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 8 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 6 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 8 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 12 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 9 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 14 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 8 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 8 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 5 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 11 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 6 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 10 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)\n(11)\n(12)\n(13)\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n(20)\n(21)\n(22)\n(23)\n(24)\n(25)\n(26)\n(27)\n(28)\n(29)\n(30)\n\n\n\n\ncoef\n\n\nX1\n-1.357*\n(0.46)\n-1.137\n(0.748)\n-0.455\n(0.359)\n-1.138*\n(0.499)\n0.201\n(0.349)\n-0.306\n(0.395)\n-0.597\n(0.403)\n-0.824\n(0.42)\n-1.482**\n(0.388)\n-1.117*\n(0.418)\n-1.142\n(0.558)\n-1.334\n(0.607)\n-3.531**\n(0.839)\n-1.102\n(0.783)\n-0.826\n(1.099)\n-0.773\n(0.479)\n-1.501***\n(0.404)\n-1.226*\n(0.478)\n-0.641\n(0.682)\n-0.378\n(0.451)\n-0.652\n(0.365)\n-1.508**\n(0.374)\n-0.941\n(0.775)\n-0.206\n(0.377)\n-0.195\n(0.301)\n-0.702\n(0.58)\n-1.141***\n(0.285)\n-1.349*\n(0.545)\n-0.537\n(0.78)\n-1.141\n(0.632)\n\n\nX2\n-0.25\n(0.129)\n0.198\n(0.352)\n-0.145\n(0.087)\n-0.33\n(0.148)\n-0.177\n(0.103)\n-0.187*\n(0.082)\n-0.118\n(0.098)\n-0.292\n(0.169)\n-0.029\n(0.124)\n-0.264**\n(0.07)\n-0.148\n(0.105)\n-0.313\n(0.154)\n-0.152\n(0.188)\n-0.296\n(0.248)\n0.13\n(0.187)\n-0.059\n(0.13)\n-0.223*\n(0.104)\n-0.113\n(0.082)\n-0.261\n(0.173)\n0.089\n(0.132)\n-0.148\n(0.174)\n-0.267\n(0.126)\n-0.125\n(0.168)\n-0.282*\n(0.099)\n-0.153\n(0.086)\n0.004\n(0.196)\n0.083\n(0.128)\n-0.226\n(0.116)\n-0.158\n(0.153)\n-0.16\n(0.145)\n\n\nfe\n\n\nf1\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nf2\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nstats\n\n\nObservations\n22\n17\n39\n18\n17\n27\n23\n24\n26\n29\n21\n21\n13\n14\n19\n26\n42\n32\n24\n25\n25\n23\n17\n27\n21\n22\n32\n26\n18\n18\n\n\nR2\n0.747\n0.545\n0.509\n0.637\n0.875\n0.591\n0.649\n0.756\n0.9\n0.724\n0.684\n0.77\n0.873\n0.619\n0.485\n0.755\n0.642\n0.732\n0.676\n0.606\n0.802\n0.778\n0.594\n0.466\n0.849\n0.79\n0.769\n0.735\n0.601\n0.774\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nBesides OLS, feols() also supports IV estimation via three part formulas:\n\nfit = pf.feols(\"Y ~  X2 | f1 + f2 | X1 ~ Z1\", data)\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n-1.050097\n0.089046\n-11.792698\n0.0\n-1.224851\n-0.875344\n\n\nX2\n-0.174351\n0.014900\n-11.701432\n0.0\n-0.203592\n-0.145110\n\n\n\n\n\n\n\nHere, X1 is the endogenous variable and Z1 is the instrument. f1 and f2 are the fixed effects, as before. To estimate IV models without fixed effects, simply omit the fixed effects part of the formula:\n\nfit = pf.feols(\"Y ~  X2 | X1 ~ Z1\", data)\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nIntercept\n0.861939\n0.151187\n5.701137\n1.567858e-08\n0.565257\n1.158622\n\n\nX1\n-0.967238\n0.130078\n-7.435847\n2.238210e-13\n-1.222497\n-0.711980\n\n\nX2\n-0.176416\n0.021769\n-8.104001\n1.554312e-15\n-0.219134\n-0.133697\n\n\n\n\n\n\n\nLast, feols() supports interaction of variables via the i() syntax. Documentation on this is tba.\nYou can pass custom transforms via the context argument. If you set context = 0, all functions from the level of the call to feols() will be available:\n\ndef _lspline(series: pd.Series, knots: list[float]) -&gt; np.array:\n    'Generate a linear spline design matrix for the input series based on knots.'\n    vector = series.values\n    columns = []\n\n    for i, knot in enumerate(knots):\n        column = np.minimum(vector, knot if i == 0 else knot - knots[i - 1])\n        columns.append(column)\n        vector = vector - column\n\n    # Add the remainder as the last column\n    columns.append(vector)\n\n    # Combine columns into a design matrix\n    return np.column_stack(columns)\n\nspline_split = _lspline(data[\"X2\"], [0, 1])\ndata[\"X2_0\"] = spline_split[:, 0]\ndata[\"0_X2_1\"] = spline_split[:, 1]\ndata[\"1_X2\"] = spline_split[:, 2]\n\nexplicit_fit = pf.feols(\"Y ~ X2_0 + 0_X2_1 + 1_X2 | f1 + f2\", data=data)\n# set context = 0 to make _lspline available for feols' internal call to Formulaic.model_matrix\ncontext_captured_fit = pf.feols(\"Y ~ _lspline(X2,[0,1]) | f1 + f2\", data=data, context = 0)\n# or provide it as a dict / mapping\ncontext_captured_fit_map = pf.feols(\"Y ~ _lspline(X2,[0,1]) | f1 + f2\", data=data, context = {\"_lspline\":_lspline})\n\npf.etable([explicit_fit, context_captured_fit, context_captured_fit_map])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n(3)\n\n\n\n\ncoef\n\n\nX2_0\n-0.177***\n(0.037)\n\n\n\n\n0_X2_1\n-0.24\n(0.171)\n\n\n\n\n1_X2\n-0.14**\n(0.05)\n\n\n\n\n_lspline(X2, =0, 1\n\n-0.177***\n(0.037)\n-0.177***\n(0.037)\n\n\n_lspline(X2, =0, 1\n\n-0.24\n(0.171)\n-0.24\n(0.171)\n\n\n_lspline(X2, =0, 1\n\n-0.14**\n(0.05)\n-0.14**\n(0.05)\n\n\nfe\n\n\nf1\nx\nx\nx\n\n\nf2\nx\nx\nx\n\n\nstats\n\n\nObservations\n998\n998\n998\n\n\nR2\n0.56\n0.56\n0.56\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nAfter fitting a model via feols(), you can use the predict() method to get the predicted values:\n\nfit = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data)\nfit.predict()[0:5]\n\narray([ 3.06336634, -0.69574135, -0.91240433, -0.46370256, -1.67331152])\n\n\nThe predict() method also supports a newdata argument to predict on new data, which returns a numpy array of the predicted values:\n\nfit = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data)\nfit.predict(newdata=data)[0:5]\n\narray([ 2.14598761,         nan,         nan,  3.06336415, -0.69574276])\n\n\nLast, you can plot the results of a model via the coefplot() method:\n\nfit = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data)\nfit.coefplot()\n\n   \n   \n\n\nWe can conduct a regression decomposition via the decompose() method, which implements a regression decomposition following the method developed in Gelbach (2016):\n\nimport re\nimport pyfixest as pf\nfrom pyfixest.utils.dgps import gelbach_data\n\ndata_gelbach = gelbach_data(nobs = 1000)\nfit = pf.feols(\"y ~ x1 + x21 + x22 + x23\", data=data_gelbach)\n\n# simple decomposition\nres = fit.decompose(param = \"x1\")\nres.etable()\n\n# group covariates via \"combine_covariates\" argument\nres = fit.decompose(param = \"x1\", combine_covariates={\"g1\": [\"x21\", \"x22\"], \"g2\": [\"x23\"]})\nres.etable()\n\n# group covariates via regex\nres = fit.decompose(param=\"x1\", combine_covariates={\"g1\": re.compile(\"x2[1-2]\"), \"g2\": re.compile(\"x23\")})\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1841: UserWarning: The 'param' argument is deprecated. Please use 'decomp_var' instead.\n  warnings.warn(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  1%|          | 8/1000 [00:02&lt;05:35,  2.95it/s]  1%|          | 12/1000 [00:02&lt;03:27,  4.75it/s]  8%|▊         | 76/1000 [00:02&lt;00:19, 46.32it/s] 20%|██        | 204/1000 [00:03&lt;00:05, 150.43it/s] 40%|███▉      | 396/1000 [00:03&lt;00:01, 315.88it/s] 52%|█████▏    | 524/1000 [00:03&lt;00:01, 402.67it/s] 65%|██████▌   | 652/1000 [00:03&lt;00:00, 480.14it/s] 78%|███████▊  | 780/1000 [00:03&lt;00:00, 547.38it/s] 91%|█████████ | 908/1000 [00:03&lt;00:00, 602.73it/s]100%|██████████| 1000/1000 [00:03&lt;00:00, 255.90it/s]\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1841: UserWarning: The 'param' argument is deprecated. Please use 'decomp_var' instead.\n  warnings.warn(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s] 12%|█▏        | 120/1000 [00:00&lt;00:00, 1075.76it/s] 25%|██▍       | 248/1000 [00:00&lt;00:00, 1115.13it/s] 50%|█████     | 504/1000 [00:00&lt;00:00, 1152.85it/s] 63%|██████▎   | 632/1000 [00:00&lt;00:00, 1056.23it/s] 76%|███████▌  | 760/1000 [00:00&lt;00:00, 993.34it/s]  89%|████████▉ | 888/1000 [00:00&lt;00:00, 957.22it/s]100%|██████████| 1000/1000 [00:00&lt;00:00, 1136.64it/s]\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1841: UserWarning: The 'param' argument is deprecated. Please use 'decomp_var' instead.\n  warnings.warn(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s] 12%|█▏        | 120/1000 [00:00&lt;00:00, 1050.91it/s] 25%|██▍       | 248/1000 [00:00&lt;00:00, 1110.24it/s] 50%|█████     | 504/1000 [00:00&lt;00:00, 1160.15it/s] 63%|██████▎   | 632/1000 [00:00&lt;00:00, 1060.86it/s] 76%|███████▌  | 760/1000 [00:00&lt;00:00, 997.27it/s]  89%|████████▉ | 888/1000 [00:00&lt;00:00, 965.45it/s]100%|██████████| 1000/1000 [00:00&lt;00:00, 1140.50it/s]\n\n\nObjects of type Feols support a range of other methods to conduct inference. For example, you can run a wild (cluster) bootstrap via the wildboottest() method:\n\nfit = pf.feols(\"Y ~ X1 + X2\", data)\nfit.wildboottest(param = \"X1\", reps=1000)\n\nparam                    X1\nt value          -12.444998\nPr(&gt;|t|)                0.0\nbootstrap_type           11\ninference                HC\nimpose_null            True\nssc                 1.00201\ndtype: object\n\n\nwould run a wild bootstrap test for the coefficient of X1 with 1000 bootstrap repetitions.\nFor a wild cluster bootstrap, you can specify the cluster variable via the cluster argument:\n\nfit.wildboottest(param = \"X1\", reps=1000, cluster=\"group_id\")\n\nparam                             X1\nt value           -9.502246183519802\nPr(&gt;|t|)                         0.0\nbootstrap_type                    11\ninference              CRV(group_id)\nimpose_null                     True\nssc                         1.057677\ndtype: object\n\n\nThe ritest() method can be used to conduct randomization inference:\n\nfit.ritest(resampvar = \"X1\", reps=1000)\n\nH0                                      X1=0\nri-type                      randomization-c\nEstimate                 -0.9929357698186859\nPr(&gt;|t|)                                 0.0\nStd. Error (Pr(&gt;|t|))                    0.0\n2.5% (Pr(&gt;|t|))                          0.0\n97.5% (Pr(&gt;|t|))                         0.0\ndtype: object\n\n\nLast, you can compute the cluster causal variance estimator by Athey et al by using the ccv() method:\n\nimport numpy as np\nrng = np.random.default_rng(1234)\ndata[\"D\"] = rng.choice([0, 1], size = data.shape[0])\nfit_D = pf.feols(\"Y ~ D\", data = data)\nfit_D.ccv(treatment = \"D\", cluster = \"group_id\")\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1588: UserWarning: The initial model was not clustered. CRV1 inference is computed and stored in the model object.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\nCCV\n0.016087657906364183\n0.231904\n0.069372\n0.945458\n-0.471124\n0.503299\n\n\nCRV1\n0.016088\n0.13378\n0.120254\n0.905614\n-0.264974\n0.29715",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.api.feols.feols"
    ]
  },
  {
    "objectID": "reference/estimation.felogit_.Felogit.html",
    "href": "reference/estimation.felogit_.Felogit.html",
    "title": "estimation.felogit_.Felogit",
    "section": "",
    "text": "estimation.felogit_.Felogit\nestimation.felogit_.Felogit(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    tol,\n    maxiter,\n    solver,\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    context=0,\n    sample_split_var=None,\n    sample_split_value=None,\n    separation_check=None,\n    accelerate=True,\n)\nClass for the estimation of a fixed-effects logit model.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.felogit_.Felogit"
    ]
  },
  {
    "objectID": "reference/estimation.model_matrix_fixest.html",
    "href": "reference/estimation.model_matrix_fixest.html",
    "title": "estimation.model_matrix_fixest",
    "section": "",
    "text": "estimation.model_matrix_fixest(\n    FixestFormula,\n    data,\n    drop_singletons=False,\n    weights=None,\n    drop_intercept=False,\n    context=0,\n)\nCreate model matrices for fixed effects estimation.\nThis function processes the data and then calls formulaic.Formula.get_model_matrix() to create the model matrices.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.model_matrix_fixest"
    ]
  },
  {
    "objectID": "reference/estimation.model_matrix_fixest.html#parameters",
    "href": "reference/estimation.model_matrix_fixest.html#parameters",
    "title": "estimation.model_matrix_fixest",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nFixestFormula\nA pyfixest.estimation.FormulaParser.FixestFormula object\nthat contains information on the model formula, the formula of the first and second stage, dependent variable, covariates, fixed effects, endogenous variables (if any), and instruments (if any).\nrequired\n\n\ndata\npd.DataFrame\nThe input DataFrame containing the data.\nrequired\n\n\ndrop_singletons\nbool\nWhether to drop singleton fixed effects. Default is False.\nFalse\n\n\nweights\nstr or None\nA string specifying the name of the weights column in data. Default is None.\nNone\n\n\ndata\npd.DataFrame\nThe input DataFrame containing the data.\nrequired\n\n\ndrop_intercept\nbool\nWhether to drop the intercept from the model matrix. Default is False. If True, the intercept is dropped ex post from the model matrix created by formulaic.\nFalse\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n0",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.model_matrix_fixest"
    ]
  },
  {
    "objectID": "reference/estimation.model_matrix_fixest.html#returns",
    "href": "reference/estimation.model_matrix_fixest.html#returns",
    "title": "estimation.model_matrix_fixest",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA dictionary with the following keys and value types: - ‘Y’ : pd.DataFrame The dependent variable. - ‘X’ : pd.DataFrame The Design Matrix. - ‘fe’ : Optional[pd.DataFrame] The model’s fixed effects. None if not applicable. - ‘endogvar’ : Optional[pd.DataFrame] The model’s endogenous variable(s), None if not applicable. - ‘Z’ : np.ndarray The model’s set of instruments (exogenous covariates plus instruments). None if not applicable. - ‘weights_df’ : Optional[pd.DataFrame] DataFrame containing weights, None if weights are not used. - ‘na_index’ : np.ndarray Array indicating rows droppled beause of NA values or singleton fixed effects. - ‘na_index_str’ : str String representation of ‘na_index’. - ’_icovars’ : Optional[list[str]] List of variables interacted with i() syntax, None if not applicable. - ‘X_is_empty’ : bool Flag indicating whether X is empty. - ‘model_spec’ : formulaic ModelSpec The model specification used to create the model matrices.",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.model_matrix_fixest"
    ]
  },
  {
    "objectID": "reference/estimation.model_matrix_fixest.html#examples",
    "href": "reference/estimation.model_matrix_fixest.html#examples",
    "title": "estimation.model_matrix_fixest",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\nfrom pyfixest.estimation.model_matrix_fixest_ import model_matrix_fixest\n\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1 + f1 + f2\", data=data)\nFixestFormula = fit.FixestFormula\n\nmm = model_matrix_fixest(FixestFormula, data)\nmm\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n{'Y':             Y\n 3    3.319513\n 4    0.134420\n 5   -0.278350\n 6   -1.519790\n 7   -2.072451\n ..        ...\n 995 -2.876714\n 996  1.430674\n 997 -0.494217\n 998 -1.047594\n 999  0.105551\n \n [997 rows x 1 columns],\n 'X':      Intercept   X1    f1    f2\n 3          1.0  1.0   1.0  10.0\n 4          1.0  2.0  19.0  20.0\n 5          1.0  2.0  13.0   3.0\n 6          1.0  1.0   2.0  16.0\n 7          1.0  0.0   2.0  23.0\n ..         ...  ...   ...   ...\n 995        1.0  2.0  14.0  23.0\n 996        1.0  0.0  19.0  17.0\n 997        1.0  1.0   3.0   5.0\n 998        1.0  0.0  18.0  20.0\n 999        1.0  2.0   4.0  19.0\n \n [997 rows x 4 columns],\n 'fe': None,\n 'endogvar': None,\n 'Z': None,\n 'weights_df': None,\n 'na_index': array([0, 1, 2]),\n 'na_index_str': '0,1,2',\n 'icovars': None,\n 'X_is_empty': False,\n 'model_spec': .fml_second_stage:\n     .lhs:\n         ModelSpec(formula=Y, materializer='pandas', materializer_params={}, ensure_full_rank=True, na_action=&lt;NAAction.DROP: 'drop'&gt;, output='pandas', cluster_by=&lt;ClusterBy.NONE: 'none'&gt;, structure=[EncodedTermStructure(term=Y, scoped_terms=[Y], columns=['Y'])], transform_state={}, encoder_state={'Y': (&lt;Kind.NUMERICAL: 'numerical'&gt;, {})})\n     .rhs:\n         ModelSpec(formula=1 + X1 + f1 + f2, materializer='pandas', materializer_params={}, ensure_full_rank=True, na_action=&lt;NAAction.DROP: 'drop'&gt;, output='pandas', cluster_by=&lt;ClusterBy.NONE: 'none'&gt;, structure=[EncodedTermStructure(term=1, scoped_terms=[1], columns=['Intercept']), EncodedTermStructure(term=X1, scoped_terms=[X1], columns=['X1']), EncodedTermStructure(term=f1, scoped_terms=[f1], columns=['f1']), EncodedTermStructure(term=f2, scoped_terms=[f2], columns=['f2'])], transform_state={}, encoder_state={'X1': (&lt;Kind.NUMERICAL: 'numerical'&gt;, {}), 'f1': (&lt;Kind.NUMERICAL: 'numerical'&gt;, {}), 'f2': (&lt;Kind.NUMERICAL: 'numerical'&gt;, {})})}",
    "crumbs": [
      "Function Reference",
      "Misc / Utilities",
      "estimation.model_matrix_fixest"
    ]
  },
  {
    "objectID": "reference/estimation.feols_.Feols.html",
    "href": "reference/estimation.feols_.Feols.html",
    "title": "estimation.feols_.Feols",
    "section": "",
    "text": "estimation.feols_.Feols(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    solver='np.linalg.solve',\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    context=0,\n    sample_split_var=None,\n    sample_split_value=None,\n)\nNon user-facing class to estimate a linear regression via OLS.\nUsers should not directly instantiate this class, but rather use the feols() function. Note that no demeaning is performed in this class: demeaning is performed in the FixestMulti class (to allow for caching of demeaned variables for multiple estimation).",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_.Feols"
    ]
  },
  {
    "objectID": "reference/estimation.feols_.Feols.html#parameters",
    "href": "reference/estimation.feols_.Feols.html#parameters",
    "title": "estimation.feols_.Feols",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nY\nnp.ndarray\nDependent variable, a two-dimensional numpy array.\nrequired\n\n\nX\nnp.ndarray\nIndependent variables, a two-dimensional numpy array.\nrequired\n\n\nweights\nnp.ndarray\nWeights, a one-dimensional numpy array.\nrequired\n\n\ncollin_tol\nfloat\nTolerance level for collinearity checks.\nrequired\n\n\ncoefnames\nlist[str]\nNames of the coefficients (of the design matrix X).\nrequired\n\n\nweights_name\nOptional[str]\nName of the weights variable.\nrequired\n\n\nweights_type\nOptional[str]\nType of the weights variable. Either “aweights” for analytic weights or “fweights” for frequency weights.\nrequired\n\n\nsolver\nstr, optional.\nThe solver to use for the regression. Can be “np.linalg.lstsq”, “np.linalg.solve”, “scipy.linalg.solve”, “scipy.sparse.linalg.lsqr” and “jax”. Defaults to “scipy.linalg.solve”.\n'np.linalg.solve'\n\n\ncontext\nint or Mapping[str, Any]\nA dictionary containing additional context variables to be used by formulaic during the creation of the model matrix. This can include custom factorization functions, transformations, or any other variables that need to be available in the formula environment.\n0",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_.Feols"
    ]
  },
  {
    "objectID": "reference/estimation.feols_.Feols.html#attributes",
    "href": "reference/estimation.feols_.Feols.html#attributes",
    "title": "estimation.feols_.Feols",
    "section": "Attributes",
    "text": "Attributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_method\nstr\nSpecifies the method used for regression, set to “feols”.\n\n\n_is_iv\nbool\nIndicates whether instrumental variables are used, initialized as False.\n\n\n_Y\nnp.ndarray\nThe demeaned dependent variable, a two-dimensional numpy array.\n\n\n_X\nnp.ndarray\nThe demeaned independent variables, a two-dimensional numpy array.\n\n\n_X_is_empty\nbool\nIndicates whether the X array is empty.\n\n\n_collin_tol\nfloat\nTolerance level for collinearity checks.\n\n\n_coefnames\nlist\nNames of the coefficients (of the design matrix X).\n\n\n_collin_vars\nlist\nVariables identified as collinear.\n\n\n_collin_index\nlist\nIndices of collinear variables.\n\n\n_Z\nnp.ndarray\nAlias for the _X array, used for calculations.\n\n\n_solver\nstr\nThe solver used for the regression.\n\n\n_weights\nnp.ndarray\nArray of weights for each observation.\n\n\n_N\nint\nNumber of observations.\n\n\n_k\nint\nNumber of independent variables (or features).\n\n\n_support_crv3_inference\nbool\nIndicates support for CRV3 inference.\n\n\n_data\nAny\nData used in the regression, to be enriched outside of the class.\n\n\n_fml\nAny\nFormula used in the regression, to be enriched outside of the class.\n\n\n_has_fixef\nbool\nIndicates whether fixed effects are used.\n\n\n_fixef\nAny\nFixed effects used in the regression.\n\n\n_icovars\nAny\nInternal covariates, to be enriched outside of the class.\n\n\n_ssc_dict\ndict\ndictionary for sum of squares and cross products matrices.\n\n\n_tZX\nnp.ndarray\nTranspose of Z multiplied by X, set in get_fit().\n\n\n_tXZ\nnp.ndarray\nTranspose of X multiplied by Z, set in get_fit().\n\n\n_tZy\nnp.ndarray\nTranspose of Z multiplied by Y, set in get_fit().\n\n\n_tZZinv\nnp.ndarray\nInverse of the transpose of Z multiplied by Z, set in get_fit().\n\n\n_beta_hat\nnp.ndarray\nEstimated regression coefficients.\n\n\n_Y_hat_link\nnp.ndarray\nPrediction at the level of the explanatory variable, i.e., the linear predictor X @ beta.\n\n\n_Y_hat_response\nnp.ndarray\nPrediction at the level of the response variable, i.e., the expected predictor E(Y|X).\n\n\n_u_hat\nnp.ndarray\nResiduals of the regression model.\n\n\n_scores\nnp.ndarray\nScores used in the regression analysis.\n\n\n_hessian\nnp.ndarray\nHessian matrix used in the regression.\n\n\n_bread\nnp.ndarray\nBread matrix, used in calculating the variance-covariance matrix.\n\n\n_vcov_type\nAny\nType of variance-covariance matrix used.\n\n\n_vcov_type_detail\nAny\nDetailed specification of the variance-covariance matrix type.\n\n\n_is_clustered\nbool\nIndicates if clustering is used in the variance-covariance calculation.\n\n\n_clustervar\nAny\nVariable used for clustering in the variance-covariance calculation.\n\n\n_G\nAny\nGroup information used in clustering.\n\n\n_ssc\nAny\nSum of squares and cross products matrix.\n\n\n_vcov\nnp.ndarray\nVariance-covariance matrix of the estimated coefficients.\n\n\n_se\nnp.ndarray\nStandard errors of the estimated coefficients.\n\n\n_tstat\nnp.ndarray\nT-statistics of the estimated coefficients.\n\n\n_pvalue\nnp.ndarray\nP-values associated with the t-statistics.\n\n\n_conf_int\nnp.ndarray\nConfidence intervals for the estimated coefficients.\n\n\n_F_stat\nAny\nF-statistic for the model, set in get_Ftest().\n\n\n_fixef_dict\ndict\ndictionary containing fixed effects estimates.\n\n\n_alpha\npd.DataFrame\nA DataFrame with the estimated fixed effects.\n\n\n_sumFE\nnp.ndarray\nSum of all fixed effects for each observation.\n\n\n_rmse\nfloat\nRoot mean squared error of the model.\n\n\n_r2\nfloat\nR-squared value of the model.\n\n\n_r2_within\nfloat\nR-squared value computed on demeaned dependent variable.\n\n\n_adj_r2\nfloat\nAdjusted R-squared value of the model.\n\n\n_adj_r2_within\nfloat\nAdjusted R-squared value computed on demeaned dependent variable.\n\n\n_solver\nLiteral[\"np.linalg.lstsq\", \"np.linalg.solve\", \"scipy.linalg.solve\",\n“scipy.sparse.linalg.lsqr”, “jax”], default is “scipy.linalg.solve”. Solver to use for the estimation.\n\n\n_demeaner_backend\nDemeanerBackendOptions\n\n\n\n_data\npd.DataFrame\nThe data frame used in the estimation. None if arguments lean = True or store_data = False.\n\n\n_model_name\nstr\nThe name of the model. Usually just the formula string. If split estimation is used, the model name will include the split variable and value.\n\n\n_model_name_plot\nstr\nThe name of the model used when plotting and summarizing models. Usually identical to _model_name. This might be different when pf.summary() or pf.coefplot() are called and models with identical _model_name attributes are passed. In this case, the _model_name_plot attribute will be modified.\n\n\n_quantile\nOptional[float]\nThe quantile used for quantile regression. None if not a quantile regression.\n\n\n# special for did\n\n\n\n\n_res_cohort_eventtime_dict\nOptional[dict[str, Any]]\n\n\n\n_yname\nOptional[str]\n\n\n\n_gname\nOptional[str]\n\n\n\n_tname\nOptional[str]\n\n\n\n_idname\nOptional[str]\n\n\n\n_att\nOptional[Any]\n\n\n\ntest_treatment_heterogeneity\nCallable[…, Any]\n\n\n\naggregate\nCallable[…, Any]\n\n\n\niplot_aggregate\nCallable[…, Any]",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_.Feols"
    ]
  },
  {
    "objectID": "reference/estimation.feols_.Feols.html#methods",
    "href": "reference/estimation.feols_.Feols.html#methods",
    "title": "estimation.feols_.Feols",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nadd_fixest_multi_context\nEnrich Feols object.\n\n\nccv\nCompute the Causal Cluster Variance following Abadie et al (QJE 2023).\n\n\ncoef\nFitted model coefficents.\n\n\nconfint\nFitted model confidence intervals.\n\n\ndecompose\nImplement the Gelbach (2016) decomposition method for mediation analysis.\n\n\ndemean\nDemean the dependent variable and covariates by the fixed effect(s).\n\n\ndrop_multicol_vars\nDetect and drop multicollinear variables.\n\n\nfixef\nCompute the coefficients of (swept out) fixed effects for a regression model.\n\n\nget_fit\nFit an OLS model.\n\n\nget_inference\nCompute standard errors, t-statistics, and p-values for the regression model.\n\n\nget_performance\nGet Goodness-of-Fit measures.\n\n\nplot_ritest\nPlot the distribution of the Randomization Inference Statistics.\n\n\npredict\nPredict values of the model on new data.\n\n\nprepare_model_matrix\nPrepare model matrices for estimation.\n\n\npvalue\nFitted model p-values.\n\n\nresid\nFitted model residuals.\n\n\nritest\nConduct Randomization Inference (RI) test against a null hypothesis of\n\n\nse\nFitted model standard errors.\n\n\ntidy\nTidy model outputs.\n\n\nto_array\nConvert estimation data frames to np arrays.\n\n\ntstat\nFitted model t-statistics.\n\n\nupdate\nUpdate coefficients for new observations using Sherman-Morrison formula.\n\n\nvcov\nCompute covariance matrices for an estimated regression model.\n\n\nwald_test\nConduct Wald test.\n\n\nwildboottest\nRun a wild cluster bootstrap based on an object of type “Feols”.\n\n\nwls_transform\nTransform model matrices for WLS Estimation.\n\n\n\n\nadd_fixest_multi_context\nestimation.feols_.Feols.add_fixest_multi_context(\n    depvar,\n    Y,\n    _data,\n    _ssc_dict,\n    _k_fe,\n    fval,\n    store_data,\n)\nEnrich Feols object.\nEnrich an instance of Feols Class with additional attributes set in the FixestMulti class.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nFixestFormula\nFixestFormula\nThe formula(s) used for estimation encoded in a FixestFormula object.\nrequired\n\n\ndepvar\nstr\nThe dependent variable of the regression model.\nrequired\n\n\nY\npd.Series\nThe dependent variable of the regression model.\nrequired\n\n\n_data\npd.DataFrame\nThe data used for estimation.\nrequired\n\n\n_ssc_dict\ndict\nA dictionary with the sum of squares and cross products matrices.\nrequired\n\n\n_k_fe\nint\nThe number of fixed effects.\nrequired\n\n\nfval\nstr\nThe fixed effects formula.\nrequired\n\n\nstore_data\nbool\nIndicates whether to save the data used for estimation in the object\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\nccv\nestimation.feols_.Feols.ccv(\n    treatment,\n    cluster=None,\n    seed=None,\n    n_splits=8,\n    pk=1,\n    qk=1,\n)\nCompute the Causal Cluster Variance following Abadie et al (QJE 2023).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntreatment\n\nThe name of the treatment variable.\nrequired\n\n\ncluster\nstr\nThe name of the cluster variable. None by default. If None, uses the cluster variable from the model fit.\nNone\n\n\nseed\nint\nAn integer to set the random seed. Defaults to None.\nNone\n\n\nn_splits\nint\nThe number of splits to use in the cross-fitting procedure. Defaults to 8.\n8\n\n\npk\nfloat\nThe proportion of sampled clusters. Defaults to 1, which corresponds to all clusters of the population being sampled.\n1\n\n\nqk\nfloat\nThe proportion of sampled observations within each cluster. Defaults to 1, which corresponds to all observations within each cluster being sampled.\n1\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA DataFrame with inference based on the “Causal Cluster Variance” and “regular” CRV1 inference.\n\n\n\n\n\nExamples\n\nimport pyfixest as pf\nimport numpy as np\n\ndata = pf.get_data()\ndata[\"D\"] = np.random.choice([0, 1], size=data.shape[0])\n\nfit = pf.feols(\"Y ~ D\", data=data, vcov={\"CRV1\": \"group_id\"})\nfit.ccv(treatment=\"D\", pk=0.05, qk=0.5, n_splits=8, seed=123).head()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\nCCV\n0.12208789190744884\n0.241679\n0.505165\n0.619572\n-0.385662\n0.629838\n\n\nCRV1\n0.122088\n0.174217\n0.700782\n0.4924\n-0.243928\n0.488104\n\n\n\n\n\n\n\n\n\n\ncoef\nestimation.feols_.Feols.coef()\nFitted model coefficents.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA pd.Series with the estimated coefficients of the regression model.\n\n\n\n\n\n\nconfint\nestimation.feols_.Feols.confint(\n    alpha=0.05,\n    keep=None,\n    drop=None,\n    exact_match=False,\n    joint=False,\n    seed=None,\n    reps=10000,\n)\nFitted model confidence intervals.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nThe significance level for confidence intervals. Defaults to 0.05. keep: str or list of str, optional\n0.05\n\n\njoint\nbool\nWhether to compute simultaneous confidence interval for joint null of parameters selected by keep and drop. Defaults to False. See https://www.causalml-book.org/assets/chapters/CausalML_chap_4.pdf, Remark 4.4.1 for details.\nFalse\n\n\nkeep\nOptional[Union[list, str]]\nThe pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients. “age”, # would keep all coefficients containing age r”^tr”, # would keep all coefficients starting with tr r”\\d$“, # would keep all coefficients ending with number Output will be in the order of the patterns.\nNone\n\n\ndrop\nOptional[Union[list, str]]\nThe pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for keep. Default is keeping all coefficients. Parameter keep and drop can be used simultaneously.\nNone\n\n\nexact_match\nOptional[bool]\nWhether to use exact match for keep and drop. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.\nFalse\n\n\nreps\nint\nThe number of bootstrap iterations to run for joint confidence intervals. Defaults to 10_000. Only used if joint is True.\n10000\n\n\nseed\nint\nThe seed for the random number generator. Defaults to None. Only used if joint is True.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA pd.DataFrame with confidence intervals of the estimated regression model for the selected coefficients.\n\n\n\n\n\nExamples\n\nfrom pyfixest.utils import get_data\nfrom pyfixest.estimation import feols\n\ndata = get_data()\nfit = feols(\"Y ~ C(f1)\", data=data)\nfit.confint(alpha=0.10).head()\nfit.confint(alpha=0.10, joint=True, reps=9999).head()\n\n\n\n\n\n\n\n\n5.0%\n95.0%\n\n\n\n\nIntercept\n-1.370571\n0.396853\n\n\nC(f1)[T.1.0]\n1.352691\n3.735885\n\n\nC(f1)[T.2.0]\n-2.884438\n-0.384924\n\n\nC(f1)[T.3.0]\n-1.752201\n0.824240\n\n\nC(f1)[T.4.0]\n-3.021364\n-0.504060\n\n\n\n\n\n\n\n\n\n\ndecompose\nestimation.feols_.Feols.decompose(\n    param=None,\n    x1_vars=None,\n    decomp_var=None,\n    type='gelbach',\n    cluster=None,\n    combine_covariates=None,\n    reps=1000,\n    seed=None,\n    nthreads=None,\n    agg_first=None,\n    only_coef=False,\n    digits=4,\n)\nImplement the Gelbach (2016) decomposition method for mediation analysis.\nCompares a short model depvar on param with the long model specified in the original feols() call.\nFor details, take a look at “When do covariates matter?” by Gelbach (2016, JoLe). You can find an ungated version of the paper on SSRN under the following link: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737 .\nWhen the initial regression is weighted, weights are interpreted as frequency weights. Inference is not yet supported for weighted models.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nparam\nstr\nThe name of the focal covariate whose effect is to be decomposed into direct and indirect components with respect to the rest of the right-hand side.\nNone\n\n\nx1_vars\nlist[str]\nA list of covariates that are included in both the baseline and the full regressions.\nNone\n\n\ndecomp_var\nstr\nThe name of the focal covariate whose effect is to be decomposed into direct and indirect components with respect to the rest of the right-hand side.\nNone\n\n\ntype\nstr\nThe type of decomposition method to use. Defaults to “gelbach”, which currently is the only supported option.\n'gelbach'\n\n\ncluster\nOptional[str]\nThe name of the cluster variable. If None, uses the cluster variable from the model fit. Defaults to None.\nNone\n\n\ncombine_covariates\nOptional[dict[str, list[str]]]\nA dictionary that specifies which covariates to combine into groups. See the example for how to use this argument. Defaults to None.\nNone\n\n\nreps\nint\nThe number of bootstrap iterations to run. Defaults to 1000.\n1000\n\n\nseed\nint\nAn integer to set the random seed. Defaults to None.\nNone\n\n\nnthreads\nint\nThe number of threads to use for the bootstrap. Defaults to None. If None, uses all available threads minus one.\nNone\n\n\nagg_first\nbool\nIf True, use the ‘aggregate first’ algorithm described in Gelbach (2016). False by default, unless combine_covariates is provided. Recommended to set to True if combine_covariates is argument is provided. As a rule of thumb, the more covariates are combined, the larger the performance improvement.\nNone\n\n\nonly_coef\nbool\nIndicates whether to compute inference for the decomposition. Defaults to False. If True, skips the inference step and only returns the decomposition results.\nFalse\n\n\ndigits\nint\nThe number of digits to round the results to. Defaults to 4.\n4\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGelbachDecomposition\nA GelbachDecomposition object with the decomposition results. Use tidy() and etable() to access the estimation results.\n\n\n\n\n\nExamples\n\nimport re\nimport pyfixest as pf\nfrom pyfixest.utils.dgps import gelbach_data\n\ndata = gelbach_data(nobs = 1000)\nfit = pf.feols(\"y ~ x1 + x21 + x22 + x23\", data=data)\n\n# simple decomposition\ngb = fit.decompose(decomp_var = \"x1\", reps = 10, nthreads = 1)\ntype(gb)\n\ngb.tidy()\ngb = fit.decompose(decomp_var = \"x1\", reps = 10, nthreads = 1, x1_vars = [\"x21\"])\n# combine covariates\ngb = fit.decompose(decomp_var = \"x1\", reps = 10, nthreads = 1, combine_covariates = {\"g1\": [\"x21\", \"x22\"], \"g2\": [\"x23\"]})\n# supress inference\ngb = fit.decompose(decomp_var = \"x1\", reps = 10, nthreads = 1, combine_covariates = {\"g1\": [\"x21\", \"x22\"], \"g2\": [\"x23\"]}, only_coef = True)\n# print results\ngb.etable()\n\n# group covariates via regex\nres = fit.decompose(decomp_var=\"x1\", combine_covariates={\"g1\": re.compile(\"x2[1-2]\"), \"g2\": re.compile(\"x23\")})\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]100%|██████████| 10/10 [00:00&lt;00:00, 340.75it/s]\n  0%|          | 0/10 [00:00&lt;?, ?it/s]100%|██████████| 10/10 [00:00&lt;00:00, 351.46it/s]\n  0%|          | 0/10 [00:00&lt;?, ?it/s]100%|██████████| 10/10 [00:00&lt;00:00, 398.13it/s]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  1%|          | 8/1000 [00:02&lt;05:32,  2.98it/s]  1%|          | 12/1000 [00:02&lt;03:26,  4.80it/s]  8%|▊         | 76/1000 [00:02&lt;00:19, 46.89it/s] 27%|██▋       | 268/1000 [00:03&lt;00:03, 197.61it/s] 52%|█████▏    | 524/1000 [00:03&lt;00:01, 394.17it/s] 65%|██████▌   | 652/1000 [00:03&lt;00:00, 477.36it/s] 78%|███████▊  | 780/1000 [00:03&lt;00:00, 556.53it/s] 91%|█████████ | 908/1000 [00:03&lt;00:00, 624.07it/s]100%|██████████| 1000/1000 [00:03&lt;00:00, 268.19it/s]\n\n\n\n\n\ndemean\nestimation.feols_.Feols.demean()\nDemean the dependent variable and covariates by the fixed effect(s).\n\n\ndrop_multicol_vars\nestimation.feols_.Feols.drop_multicol_vars()\nDetect and drop multicollinear variables.\n\n\nfixef\nestimation.feols_.Feols.fixef(atol=1e-06, btol=1e-06)\nCompute the coefficients of (swept out) fixed effects for a regression model.\nThis method creates the following attributes: - _alpha (pd.DataFrame): A DataFrame with the estimated fixed effects. - _sumFE (np.array): An array with the sum of fixed effects for each observation (i = 1, …, N).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, dict[str, float]]\nA dictionary with the estimated fixed effects.\n\n\n\n\n\n\nget_fit\nestimation.feols_.Feols.get_fit()\nFit an OLS model.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\nget_inference\nestimation.feols_.Feols.get_inference(alpha=0.05)\nCompute standard errors, t-statistics, and p-values for the regression model.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nThe significance level for confidence intervals. Defaults to 0.05, which produces a 95% confidence interval.\n0.05\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\nDetails\nrelevant fixest functions:\n\n\n- fixest_CI_factor: https://github.com/lrberge/fixest/blob/5523d48ef4a430fa2e82815ca589fc8a47168fe7/R/miscfuns.R#L5614\n\n\n\nget_performance\nestimation.feols_.Feols.get_performance()\nGet Goodness-of-Fit measures.\nCompute multiple additional measures commonly reported with linear regression output, including R-squared and adjusted R-squared. Note that variables with the suffix _within use demeaned dependent variables Y, while variables without do not or are invariant to demeaning.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\nCreates the following instances:\n\n\n\n\n- r2 (float): R-squared of the regression model.\n\n\n\n\n- adj_r2 (float): Adjusted R-squared of the regression model.\n\n\n\n\n- r2_within (float): R-squared of the regression model, computed on\n\n\n\n\ndemeaned dependent variable.\n\n\n\n\n- adj_r2_within (float): Adjusted R-squared of the regression model,\n\n\n\n\ncomputed on demeaned dependent variable.\n\n\n\n\n\n\n\nplot_ritest\nestimation.feols_.Feols.plot_ritest(plot_backend='lets_plot')\nPlot the distribution of the Randomization Inference Statistics.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nplot_backend\nstr\nThe plotting backend to use. Defaults to “lets_plot”. Alternatively, “matplotlib” is available.\n'lets_plot'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA lets_plot or matplotlib figure with the distribution of the Randomization\n\n\n\n\nInference Statistics.\n\n\n\n\n\n\n\npredict\nestimation.feols_.Feols.predict(\n    newdata=None,\n    atol=1e-06,\n    btol=1e-06,\n    type='link',\n    se_fit=False,\n    interval=None,\n    alpha=0.05,\n)\nPredict values of the model on new data.\nReturn a flat np.array with predicted values of the regression model. If new fixed effect levels are introduced in newdata, predicted values for such observations will be set to NaN.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnewdata\nDataFrameType\nA narwhals compatible DataFrame (polars, pandas, duckdb, etc). If None (default), the data used for fitting the model is used.\nNone\n\n\ntype\nstr\nThe type of prediction to be computed. Can be either “response” (default) or “link”. For linear models, both are identical.\n'link'\n\n\natol\nFloat\nStopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\nbtol\nFloat\nAnother stopping tolerance for scipy.sparse.linalg.lsqr(). See https://docs.scipy.org/doc/ scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n1e-6\n\n\ntype\nPredictionType\nThe type of prediction to be made. Can be either ‘link’ or ‘response’. Defaults to ‘link’. ‘link’ and ‘response’ lead to identical results for linear models.\n'link'\n\n\nse_fit\nOptional[bool]\nIf True, the standard error of the prediction is computed. Only feasible for models without fixed effects. GLMs are not supported. Defaults to False.\nFalse\n\n\ninterval\nOptional[PredictionErrorOptions]\nThe type of interval to compute. Can be either ‘prediction’ or None.\nNone\n\n\nalpha\nfloat\nThe alpha level for the confidence interval. Defaults to 0.05. Only used if interval = “prediction” is not None.\n0.05\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[np.ndarray, pd.DataFrame]\nReturns a pd.Dataframe with columns “fit”, “se_fit” and CIs if argument “interval=prediction”. Otherwise, returns a np.ndarray with the predicted values of the model or the prediction standard errors if argument “se_fit=True”.\n\n\n\n\n\n\nprepare_model_matrix\nestimation.feols_.Feols.prepare_model_matrix()\nPrepare model matrices for estimation.\n\n\npvalue\nestimation.feols_.Feols.pvalue()\nFitted model p-values.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA pd.Series with p-values of the estimated regression model.\n\n\n\n\n\n\nresid\nestimation.feols_.Feols.resid()\nFitted model residuals.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nA np.ndarray with the residuals of the estimated regression model.\n\n\n\n\n\n\nritest\nestimation.feols_.Feols.ritest(\n    resampvar,\n    cluster=None,\n    reps=100,\n    type='randomization-c',\n    rng=None,\n    choose_algorithm='auto',\n    store_ritest_statistics=False,\n    level=0.95,\n)\nConduct Randomization Inference (RI) test against a null hypothesis of resampvar = 0.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nresampvar\nstr\nThe name of the variable to be resampled.\nrequired\n\n\ncluster\nstr\nThe name of the cluster variable in case of cluster random assignment. If provided, resampvar is held constant within each cluster. Defaults to None.\nNone\n\n\nreps\nint\nThe number of randomization iterations. Defaults to 100.\n100\n\n\ntype\nstr\nThe type of the randomization inference test. Can be “randomization-c” or “randomization-t”. Note that the “randomization-c” is much faster, while the “randomization-t” is recommended by Wu & Ding (JASA, 2021).\n'randomization-c'\n\n\nrng\nnp.random.Generator\nA random number generator. Defaults to None.\nNone\n\n\nchoose_algorithm\nstr\nThe algorithm to use for the computation. Defaults to “auto”. The alternative is “fast” and “slow”, and should only be used for running CI tests. Ironically, this argument is not tested for any input errors from the user! So please don’t use it =)\n'auto'\n\n\ninclude_plot\n\nWhether to include a plot of the distribution p-values. Defaults to False.\nrequired\n\n\nstore_ritest_statistics\nbool\nWhether to store the simulated statistics of the RI procedure. Defaults to False. If True, stores the simulated statistics in the model object via the ritest_statistics attribute as a numpy array.\nFalse\n\n\nlevel\nfloat\nThe level for the confidence interval of the randomization inference p-value. Defaults to 0.95.\n0.95\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nA pd.Series with the regression coefficient of resampvar and the p-value\n\n\n\n\nof the RI test. Additionally, reports the standard error and the confidence\n\n\n\n\ninterval of the p-value.\n\n\n\n\n\n\nExamples\n\nimport pyfixest as pf\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1 + X2\", data=data)\n\n# Conduct a randomization inference test for the coefficient of X1\nfit.ritest(\"X1\", reps=1000)\n\n# use randomization-t instead of randomization-c\nfit.ritest(\"X1\", reps=1000, type=\"randomization-t\")\n\n# store statistics for plotting\nfit.ritest(\"X1\", reps=1000, store_ritest_statistics=True)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]  0%|          | 1/1000 [00:00&lt;09:24,  1.77it/s]  0%|          | 2/1000 [00:00&lt;05:13,  3.19it/s]  0%|          | 3/1000 [00:00&lt;03:45,  4.42it/s]  0%|          | 4/1000 [00:00&lt;03:01,  5.50it/s]  0%|          | 5/1000 [00:01&lt;02:37,  6.31it/s]  1%|          | 6/1000 [00:01&lt;02:23,  6.94it/s]  1%|          | 7/1000 [00:01&lt;02:16,  7.26it/s]  1%|          | 8/1000 [00:01&lt;02:12,  7.48it/s]  1%|          | 9/1000 [00:01&lt;02:09,  7.68it/s]  1%|          | 10/1000 [00:01&lt;02:09,  7.62it/s]  1%|          | 11/1000 [00:01&lt;02:05,  7.87it/s]  1%|          | 12/1000 [00:01&lt;02:02,  8.08it/s]  1%|▏         | 13/1000 [00:02&lt;01:58,  8.31it/s]  1%|▏         | 14/1000 [00:02&lt;01:56,  8.46it/s]  2%|▏         | 15/1000 [00:02&lt;01:58,  8.30it/s]  2%|▏         | 16/1000 [00:02&lt;01:59,  8.25it/s]  2%|▏         | 17/1000 [00:02&lt;02:00,  8.16it/s]  2%|▏         | 18/1000 [00:02&lt;01:58,  8.32it/s]  2%|▏         | 19/1000 [00:02&lt;01:56,  8.45it/s]  2%|▏         | 20/1000 [00:02&lt;01:55,  8.49it/s]  2%|▏         | 21/1000 [00:02&lt;01:54,  8.52it/s]  2%|▏         | 22/1000 [00:03&lt;01:54,  8.58it/s]  2%|▏         | 23/1000 [00:03&lt;01:52,  8.66it/s]  2%|▏         | 24/1000 [00:03&lt;01:53,  8.58it/s]  2%|▎         | 25/1000 [00:03&lt;01:53,  8.61it/s]  3%|▎         | 26/1000 [00:03&lt;01:55,  8.45it/s]  3%|▎         | 27/1000 [00:03&lt;01:59,  8.13it/s]  3%|▎         | 28/1000 [00:03&lt;01:59,  8.12it/s]  3%|▎         | 29/1000 [00:03&lt;01:57,  8.29it/s]  3%|▎         | 30/1000 [00:04&lt;01:56,  8.29it/s]  3%|▎         | 31/1000 [00:04&lt;01:56,  8.29it/s]  3%|▎         | 32/1000 [00:04&lt;01:56,  8.31it/s]  3%|▎         | 33/1000 [00:04&lt;01:55,  8.37it/s]  3%|▎         | 34/1000 [00:04&lt;01:56,  8.32it/s]  4%|▎         | 35/1000 [00:04&lt;01:56,  8.26it/s]  4%|▎         | 36/1000 [00:04&lt;01:57,  8.17it/s]  4%|▎         | 37/1000 [00:04&lt;01:56,  8.25it/s]  4%|▍         | 38/1000 [00:05&lt;01:58,  8.13it/s]  4%|▍         | 39/1000 [00:05&lt;02:02,  7.82it/s]  4%|▍         | 40/1000 [00:05&lt;02:05,  7.63it/s]  4%|▍         | 41/1000 [00:05&lt;02:04,  7.67it/s]  4%|▍         | 42/1000 [00:05&lt;02:01,  7.91it/s]  4%|▍         | 43/1000 [00:05&lt;01:58,  8.05it/s]  4%|▍         | 44/1000 [00:05&lt;01:56,  8.22it/s]  4%|▍         | 45/1000 [00:05&lt;01:55,  8.28it/s]  5%|▍         | 46/1000 [00:06&lt;01:54,  8.34it/s]  5%|▍         | 47/1000 [00:06&lt;01:53,  8.39it/s]  5%|▍         | 48/1000 [00:06&lt;01:56,  8.14it/s]  5%|▍         | 49/1000 [00:06&lt;01:57,  8.09it/s]  5%|▌         | 50/1000 [00:06&lt;01:57,  8.10it/s]  5%|▌         | 51/1000 [00:06&lt;02:02,  7.77it/s]  5%|▌         | 52/1000 [00:06&lt;02:01,  7.83it/s]  5%|▌         | 53/1000 [00:06&lt;01:58,  8.02it/s]  5%|▌         | 54/1000 [00:07&lt;01:55,  8.18it/s]  6%|▌         | 55/1000 [00:07&lt;01:53,  8.29it/s]  6%|▌         | 56/1000 [00:07&lt;01:55,  8.20it/s]  6%|▌         | 57/1000 [00:07&lt;01:53,  8.29it/s]  6%|▌         | 58/1000 [00:07&lt;02:00,  7.82it/s]  6%|▌         | 59/1000 [00:07&lt;01:59,  7.86it/s]  6%|▌         | 60/1000 [00:07&lt;01:59,  7.89it/s]  6%|▌         | 61/1000 [00:07&lt;01:56,  8.09it/s]  6%|▌         | 62/1000 [00:08&lt;01:53,  8.25it/s]  6%|▋         | 63/1000 [00:08&lt;01:52,  8.34it/s]  6%|▋         | 64/1000 [00:08&lt;01:53,  8.22it/s]  6%|▋         | 65/1000 [00:08&lt;01:53,  8.24it/s]  7%|▋         | 66/1000 [00:08&lt;01:51,  8.34it/s]  7%|▋         | 67/1000 [00:08&lt;01:50,  8.45it/s]  7%|▋         | 68/1000 [00:08&lt;01:50,  8.44it/s]  7%|▋         | 69/1000 [00:08&lt;01:50,  8.46it/s]  7%|▋         | 70/1000 [00:08&lt;01:50,  8.43it/s]  7%|▋         | 71/1000 [00:09&lt;01:49,  8.49it/s]  7%|▋         | 72/1000 [00:09&lt;01:48,  8.56it/s]  7%|▋         | 73/1000 [00:09&lt;01:51,  8.28it/s]  7%|▋         | 74/1000 [00:09&lt;01:53,  8.18it/s]  8%|▊         | 75/1000 [00:09&lt;01:51,  8.28it/s]  8%|▊         | 76/1000 [00:09&lt;01:56,  7.91it/s]  8%|▊         | 77/1000 [00:09&lt;01:56,  7.90it/s]  8%|▊         | 78/1000 [00:09&lt;01:54,  8.07it/s]  8%|▊         | 79/1000 [00:10&lt;01:58,  7.77it/s]  8%|▊         | 80/1000 [00:10&lt;02:02,  7.53it/s]  8%|▊         | 81/1000 [00:10&lt;01:59,  7.68it/s]  8%|▊         | 82/1000 [00:10&lt;01:55,  7.95it/s]  8%|▊         | 83/1000 [00:10&lt;01:56,  7.88it/s]  8%|▊         | 84/1000 [00:10&lt;01:54,  8.01it/s]  8%|▊         | 85/1000 [00:10&lt;01:52,  8.14it/s]  9%|▊         | 86/1000 [00:10&lt;01:50,  8.25it/s]  9%|▊         | 87/1000 [00:11&lt;01:49,  8.34it/s]  9%|▉         | 88/1000 [00:11&lt;01:48,  8.38it/s]  9%|▉         | 89/1000 [00:11&lt;01:50,  8.26it/s]  9%|▉         | 90/1000 [00:11&lt;01:49,  8.31it/s]  9%|▉         | 91/1000 [00:11&lt;01:49,  8.32it/s]  9%|▉         | 92/1000 [00:11&lt;01:51,  8.17it/s]  9%|▉         | 93/1000 [00:11&lt;01:50,  8.24it/s]  9%|▉         | 94/1000 [00:11&lt;01:48,  8.33it/s] 10%|▉         | 95/1000 [00:12&lt;01:47,  8.42it/s] 10%|▉         | 96/1000 [00:12&lt;01:46,  8.51it/s] 10%|▉         | 97/1000 [00:12&lt;01:47,  8.39it/s] 10%|▉         | 98/1000 [00:12&lt;01:47,  8.38it/s] 10%|▉         | 99/1000 [00:12&lt;01:49,  8.25it/s] 10%|█         | 100/1000 [00:12&lt;01:51,  8.08it/s] 10%|█         | 101/1000 [00:12&lt;01:50,  8.14it/s] 10%|█         | 102/1000 [00:12&lt;01:49,  8.19it/s] 10%|█         | 103/1000 [00:12&lt;01:47,  8.31it/s] 10%|█         | 104/1000 [00:13&lt;01:46,  8.44it/s] 10%|█         | 105/1000 [00:13&lt;01:48,  8.25it/s] 11%|█         | 106/1000 [00:13&lt;01:50,  8.11it/s] 11%|█         | 107/1000 [00:13&lt;01:50,  8.08it/s] 11%|█         | 108/1000 [00:13&lt;01:49,  8.11it/s] 11%|█         | 109/1000 [00:13&lt;01:50,  8.06it/s] 11%|█         | 110/1000 [00:13&lt;01:49,  8.14it/s] 11%|█         | 111/1000 [00:13&lt;01:46,  8.33it/s] 11%|█         | 112/1000 [00:14&lt;01:45,  8.45it/s] 11%|█▏        | 113/1000 [00:14&lt;01:43,  8.53it/s] 11%|█▏        | 114/1000 [00:14&lt;01:43,  8.58it/s] 12%|█▏        | 115/1000 [00:14&lt;01:43,  8.56it/s] 12%|█▏        | 116/1000 [00:14&lt;01:44,  8.48it/s] 12%|█▏        | 117/1000 [00:14&lt;01:44,  8.45it/s] 12%|█▏        | 118/1000 [00:14&lt;01:47,  8.20it/s] 12%|█▏        | 119/1000 [00:14&lt;01:45,  8.32it/s] 12%|█▏        | 120/1000 [00:15&lt;01:48,  8.13it/s] 12%|█▏        | 121/1000 [00:15&lt;01:47,  8.18it/s] 12%|█▏        | 122/1000 [00:15&lt;01:45,  8.32it/s] 12%|█▏        | 123/1000 [00:15&lt;01:43,  8.46it/s] 12%|█▏        | 124/1000 [00:15&lt;01:45,  8.30it/s] 12%|█▎        | 125/1000 [00:15&lt;01:48,  8.09it/s] 13%|█▎        | 126/1000 [00:15&lt;01:50,  7.94it/s] 13%|█▎        | 127/1000 [00:15&lt;01:48,  8.03it/s] 13%|█▎        | 128/1000 [00:16&lt;01:46,  8.16it/s] 13%|█▎        | 129/1000 [00:16&lt;01:45,  8.26it/s] 13%|█▎        | 130/1000 [00:16&lt;01:45,  8.25it/s] 13%|█▎        | 131/1000 [00:16&lt;01:47,  8.09it/s] 13%|█▎        | 132/1000 [00:16&lt;01:48,  8.03it/s] 13%|█▎        | 133/1000 [00:16&lt;01:49,  7.94it/s] 13%|█▎        | 134/1000 [00:16&lt;01:47,  8.02it/s] 14%|█▎        | 135/1000 [00:16&lt;01:46,  8.13it/s] 14%|█▎        | 136/1000 [00:17&lt;01:44,  8.29it/s] 14%|█▎        | 137/1000 [00:17&lt;01:42,  8.41it/s] 14%|█▍        | 138/1000 [00:17&lt;01:43,  8.34it/s] 14%|█▍        | 139/1000 [00:17&lt;01:43,  8.33it/s] 14%|█▍        | 140/1000 [00:17&lt;01:44,  8.26it/s] 14%|█▍        | 141/1000 [00:17&lt;01:42,  8.38it/s] 14%|█▍        | 142/1000 [00:17&lt;01:40,  8.52it/s] 14%|█▍        | 143/1000 [00:17&lt;01:40,  8.56it/s] 14%|█▍        | 144/1000 [00:17&lt;01:38,  8.65it/s] 14%|█▍        | 145/1000 [00:18&lt;01:38,  8.70it/s] 15%|█▍        | 146/1000 [00:18&lt;01:40,  8.48it/s] 15%|█▍        | 147/1000 [00:18&lt;01:41,  8.39it/s] 15%|█▍        | 148/1000 [00:18&lt;01:40,  8.46it/s] 15%|█▍        | 149/1000 [00:18&lt;01:40,  8.51it/s] 15%|█▌        | 150/1000 [00:18&lt;01:44,  8.15it/s] 15%|█▌        | 151/1000 [00:18&lt;01:45,  8.07it/s] 15%|█▌        | 152/1000 [00:18&lt;01:47,  7.92it/s] 15%|█▌        | 153/1000 [00:19&lt;01:45,  8.03it/s] 15%|█▌        | 154/1000 [00:19&lt;01:44,  8.06it/s] 16%|█▌        | 155/1000 [00:19&lt;01:43,  8.16it/s] 16%|█▌        | 156/1000 [00:19&lt;01:41,  8.28it/s] 16%|█▌        | 157/1000 [00:19&lt;01:42,  8.23it/s] 16%|█▌        | 158/1000 [00:19&lt;01:46,  7.93it/s] 16%|█▌        | 159/1000 [00:19&lt;01:46,  7.91it/s] 16%|█▌        | 160/1000 [00:19&lt;01:44,  8.01it/s] 16%|█▌        | 161/1000 [00:20&lt;01:44,  8.02it/s] 16%|█▌        | 162/1000 [00:20&lt;01:50,  7.61it/s] 16%|█▋        | 163/1000 [00:20&lt;01:49,  7.62it/s] 16%|█▋        | 164/1000 [00:20&lt;01:45,  7.89it/s] 16%|█▋        | 165/1000 [00:20&lt;01:42,  8.11it/s] 17%|█▋        | 166/1000 [00:20&lt;01:43,  8.05it/s] 17%|█▋        | 167/1000 [00:20&lt;01:41,  8.20it/s] 17%|█▋        | 168/1000 [00:20&lt;01:39,  8.33it/s] 17%|█▋        | 169/1000 [00:21&lt;01:38,  8.42it/s] 17%|█▋        | 170/1000 [00:21&lt;01:37,  8.49it/s] 17%|█▋        | 171/1000 [00:21&lt;01:38,  8.42it/s] 17%|█▋        | 172/1000 [00:21&lt;01:38,  8.43it/s] 17%|█▋        | 173/1000 [00:21&lt;01:37,  8.49it/s] 17%|█▋        | 174/1000 [00:21&lt;01:39,  8.28it/s] 18%|█▊        | 175/1000 [00:21&lt;01:42,  8.03it/s] 18%|█▊        | 176/1000 [00:21&lt;01:41,  8.13it/s] 18%|█▊        | 177/1000 [00:21&lt;01:39,  8.25it/s] 18%|█▊        | 178/1000 [00:22&lt;01:38,  8.38it/s] 18%|█▊        | 179/1000 [00:22&lt;01:38,  8.38it/s] 18%|█▊        | 180/1000 [00:22&lt;01:38,  8.35it/s] 18%|█▊        | 181/1000 [00:22&lt;01:40,  8.19it/s] 18%|█▊        | 182/1000 [00:22&lt;01:39,  8.19it/s] 18%|█▊        | 183/1000 [00:22&lt;01:38,  8.26it/s] 18%|█▊        | 184/1000 [00:22&lt;01:37,  8.34it/s] 18%|█▊        | 185/1000 [00:22&lt;01:38,  8.25it/s] 19%|█▊        | 186/1000 [00:23&lt;01:39,  8.17it/s] 19%|█▊        | 187/1000 [00:23&lt;01:37,  8.30it/s] 19%|█▉        | 188/1000 [00:23&lt;01:38,  8.26it/s] 19%|█▉        | 189/1000 [00:23&lt;01:36,  8.37it/s] 19%|█▉        | 190/1000 [00:23&lt;01:35,  8.47it/s] 19%|█▉        | 191/1000 [00:23&lt;01:35,  8.44it/s] 19%|█▉        | 192/1000 [00:23&lt;01:36,  8.36it/s] 19%|█▉        | 193/1000 [00:23&lt;01:35,  8.47it/s] 19%|█▉        | 194/1000 [00:24&lt;01:35,  8.47it/s] 20%|█▉        | 195/1000 [00:24&lt;01:34,  8.52it/s] 20%|█▉        | 196/1000 [00:24&lt;01:34,  8.55it/s] 20%|█▉        | 197/1000 [00:24&lt;01:34,  8.48it/s] 20%|█▉        | 198/1000 [00:24&lt;01:34,  8.53it/s] 20%|█▉        | 199/1000 [00:24&lt;01:37,  8.25it/s] 20%|██        | 200/1000 [00:24&lt;01:41,  7.86it/s] 20%|██        | 201/1000 [00:24&lt;01:40,  7.98it/s] 20%|██        | 202/1000 [00:25&lt;01:42,  7.81it/s] 20%|██        | 203/1000 [00:25&lt;01:40,  7.92it/s] 20%|██        | 204/1000 [00:25&lt;01:38,  8.11it/s] 20%|██        | 205/1000 [00:25&lt;01:40,  7.90it/s] 21%|██        | 206/1000 [00:25&lt;01:44,  7.58it/s] 21%|██        | 207/1000 [00:25&lt;01:43,  7.69it/s] 21%|██        | 208/1000 [00:25&lt;01:41,  7.81it/s] 21%|██        | 209/1000 [00:25&lt;01:46,  7.46it/s] 21%|██        | 210/1000 [00:26&lt;01:44,  7.56it/s] 21%|██        | 211/1000 [00:26&lt;01:41,  7.78it/s] 21%|██        | 212/1000 [00:26&lt;01:43,  7.61it/s] 21%|██▏       | 213/1000 [00:26&lt;01:44,  7.56it/s] 21%|██▏       | 214/1000 [00:26&lt;01:46,  7.38it/s] 22%|██▏       | 215/1000 [00:26&lt;01:49,  7.18it/s] 22%|██▏       | 216/1000 [00:26&lt;01:46,  7.33it/s] 22%|██▏       | 217/1000 [00:26&lt;01:41,  7.69it/s] 22%|██▏       | 218/1000 [00:27&lt;01:41,  7.73it/s] 22%|██▏       | 219/1000 [00:27&lt;01:41,  7.67it/s] 22%|██▏       | 220/1000 [00:27&lt;01:41,  7.67it/s] 22%|██▏       | 221/1000 [00:27&lt;01:44,  7.48it/s] 22%|██▏       | 222/1000 [00:27&lt;01:44,  7.46it/s] 22%|██▏       | 223/1000 [00:27&lt;01:46,  7.30it/s] 22%|██▏       | 224/1000 [00:27&lt;01:47,  7.24it/s] 22%|██▎       | 225/1000 [00:28&lt;01:46,  7.27it/s] 23%|██▎       | 226/1000 [00:28&lt;01:47,  7.19it/s] 23%|██▎       | 227/1000 [00:28&lt;01:49,  7.09it/s] 23%|██▎       | 228/1000 [00:28&lt;01:50,  7.01it/s] 23%|██▎       | 229/1000 [00:28&lt;01:50,  6.96it/s] 23%|██▎       | 230/1000 [00:28&lt;01:50,  6.97it/s] 23%|██▎       | 231/1000 [00:28&lt;01:45,  7.27it/s] 23%|██▎       | 232/1000 [00:29&lt;01:42,  7.51it/s] 23%|██▎       | 233/1000 [00:29&lt;01:39,  7.72it/s] 23%|██▎       | 234/1000 [00:29&lt;01:40,  7.65it/s] 24%|██▎       | 235/1000 [00:29&lt;01:39,  7.67it/s] 24%|██▎       | 236/1000 [00:29&lt;01:40,  7.59it/s] 24%|██▎       | 237/1000 [00:29&lt;01:44,  7.33it/s] 24%|██▍       | 238/1000 [00:29&lt;01:47,  7.11it/s] 24%|██▍       | 239/1000 [00:29&lt;01:43,  7.36it/s] 24%|██▍       | 240/1000 [00:30&lt;01:44,  7.26it/s] 24%|██▍       | 241/1000 [00:30&lt;01:43,  7.37it/s] 24%|██▍       | 242/1000 [00:30&lt;01:43,  7.31it/s] 24%|██▍       | 243/1000 [00:30&lt;01:40,  7.55it/s] 24%|██▍       | 244/1000 [00:30&lt;01:38,  7.68it/s] 24%|██▍       | 245/1000 [00:30&lt;01:39,  7.55it/s] 25%|██▍       | 246/1000 [00:30&lt;01:40,  7.50it/s] 25%|██▍       | 247/1000 [00:31&lt;01:39,  7.58it/s] 25%|██▍       | 248/1000 [00:31&lt;01:35,  7.85it/s] 25%|██▍       | 249/1000 [00:31&lt;01:35,  7.89it/s] 25%|██▌       | 250/1000 [00:31&lt;01:33,  8.05it/s] 25%|██▌       | 251/1000 [00:31&lt;01:37,  7.66it/s] 25%|██▌       | 252/1000 [00:31&lt;01:38,  7.63it/s] 25%|██▌       | 253/1000 [00:31&lt;01:34,  7.88it/s] 25%|██▌       | 254/1000 [00:31&lt;01:33,  8.00it/s] 26%|██▌       | 255/1000 [00:32&lt;01:31,  8.18it/s] 26%|██▌       | 256/1000 [00:32&lt;01:35,  7.80it/s] 26%|██▌       | 257/1000 [00:32&lt;01:34,  7.83it/s] 26%|██▌       | 258/1000 [00:32&lt;01:32,  8.06it/s] 26%|██▌       | 259/1000 [00:32&lt;01:33,  7.92it/s] 26%|██▌       | 260/1000 [00:32&lt;01:32,  8.02it/s] 26%|██▌       | 261/1000 [00:32&lt;01:34,  7.83it/s] 26%|██▌       | 262/1000 [00:32&lt;01:33,  7.91it/s] 26%|██▋       | 263/1000 [00:33&lt;01:31,  8.10it/s] 26%|██▋       | 264/1000 [00:33&lt;01:29,  8.24it/s] 26%|██▋       | 265/1000 [00:33&lt;01:31,  8.03it/s] 27%|██▋       | 266/1000 [00:33&lt;01:35,  7.73it/s] 27%|██▋       | 267/1000 [00:33&lt;01:33,  7.86it/s] 27%|██▋       | 268/1000 [00:33&lt;01:33,  7.84it/s] 27%|██▋       | 269/1000 [00:33&lt;01:33,  7.82it/s] 27%|██▋       | 270/1000 [00:33&lt;01:31,  7.95it/s] 27%|██▋       | 271/1000 [00:34&lt;01:29,  8.16it/s] 27%|██▋       | 272/1000 [00:34&lt;01:29,  8.17it/s] 27%|██▋       | 273/1000 [00:34&lt;01:28,  8.19it/s] 27%|██▋       | 274/1000 [00:34&lt;01:28,  8.24it/s] 28%|██▊       | 275/1000 [00:34&lt;01:31,  7.90it/s] 28%|██▊       | 276/1000 [00:34&lt;01:31,  7.87it/s] 28%|██▊       | 277/1000 [00:34&lt;01:30,  7.98it/s] 28%|██▊       | 278/1000 [00:34&lt;01:28,  8.14it/s] 28%|██▊       | 279/1000 [00:35&lt;01:31,  7.92it/s] 28%|██▊       | 280/1000 [00:35&lt;01:32,  7.82it/s] 28%|██▊       | 281/1000 [00:35&lt;01:28,  8.08it/s] 28%|██▊       | 282/1000 [00:35&lt;01:26,  8.27it/s] 28%|██▊       | 283/1000 [00:35&lt;01:25,  8.40it/s] 28%|██▊       | 284/1000 [00:35&lt;01:26,  8.24it/s] 28%|██▊       | 285/1000 [00:35&lt;01:30,  7.87it/s] 29%|██▊       | 286/1000 [00:35&lt;01:28,  8.02it/s] 29%|██▊       | 287/1000 [00:36&lt;01:27,  8.12it/s] 29%|██▉       | 288/1000 [00:36&lt;01:26,  8.24it/s] 29%|██▉       | 289/1000 [00:36&lt;01:29,  7.97it/s] 29%|██▉       | 290/1000 [00:36&lt;01:31,  7.80it/s] 29%|██▉       | 291/1000 [00:36&lt;01:28,  8.04it/s] 29%|██▉       | 292/1000 [00:36&lt;01:31,  7.71it/s] 29%|██▉       | 293/1000 [00:36&lt;01:30,  7.83it/s] 29%|██▉       | 294/1000 [00:36&lt;01:31,  7.68it/s] 30%|██▉       | 295/1000 [00:37&lt;01:33,  7.57it/s] 30%|██▉       | 296/1000 [00:37&lt;01:30,  7.75it/s] 30%|██▉       | 297/1000 [00:37&lt;01:29,  7.86it/s] 30%|██▉       | 298/1000 [00:37&lt;01:30,  7.73it/s] 30%|██▉       | 299/1000 [00:37&lt;01:32,  7.59it/s] 30%|███       | 300/1000 [00:37&lt;01:30,  7.76it/s] 30%|███       | 301/1000 [00:37&lt;01:27,  7.97it/s] 30%|███       | 302/1000 [00:37&lt;01:25,  8.20it/s] 30%|███       | 303/1000 [00:38&lt;01:27,  7.96it/s] 30%|███       | 304/1000 [00:38&lt;01:31,  7.57it/s] 30%|███       | 305/1000 [00:38&lt;01:34,  7.38it/s] 31%|███       | 306/1000 [00:38&lt;01:33,  7.39it/s] 31%|███       | 307/1000 [00:38&lt;01:30,  7.63it/s] 31%|███       | 308/1000 [00:38&lt;01:29,  7.74it/s] 31%|███       | 309/1000 [00:38&lt;01:27,  7.93it/s] 31%|███       | 310/1000 [00:38&lt;01:24,  8.20it/s] 31%|███       | 311/1000 [00:39&lt;01:22,  8.30it/s] 31%|███       | 312/1000 [00:39&lt;01:22,  8.31it/s] 31%|███▏      | 313/1000 [00:39&lt;01:26,  7.91it/s] 31%|███▏      | 314/1000 [00:39&lt;01:30,  7.56it/s] 32%|███▏      | 315/1000 [00:39&lt;01:28,  7.71it/s] 32%|███▏      | 316/1000 [00:39&lt;01:30,  7.60it/s] 32%|███▏      | 317/1000 [00:39&lt;01:31,  7.45it/s] 32%|███▏      | 318/1000 [00:40&lt;01:34,  7.21it/s] 32%|███▏      | 319/1000 [00:40&lt;01:36,  7.08it/s] 32%|███▏      | 320/1000 [00:40&lt;01:32,  7.33it/s] 32%|███▏      | 321/1000 [00:40&lt;01:30,  7.51it/s] 32%|███▏      | 322/1000 [00:40&lt;01:31,  7.44it/s] 32%|███▏      | 323/1000 [00:40&lt;01:28,  7.68it/s] 32%|███▏      | 324/1000 [00:40&lt;01:25,  7.88it/s] 32%|███▎      | 325/1000 [00:40&lt;01:24,  7.95it/s] 33%|███▎      | 326/1000 [00:41&lt;01:23,  8.06it/s] 33%|███▎      | 327/1000 [00:41&lt;01:21,  8.22it/s] 33%|███▎      | 328/1000 [00:41&lt;01:22,  8.13it/s] 33%|███▎      | 329/1000 [00:41&lt;01:21,  8.25it/s] 33%|███▎      | 330/1000 [00:41&lt;01:22,  8.14it/s] 33%|███▎      | 331/1000 [00:41&lt;01:22,  8.10it/s] 33%|███▎      | 332/1000 [00:41&lt;01:22,  8.09it/s] 33%|███▎      | 333/1000 [00:41&lt;01:21,  8.19it/s] 33%|███▎      | 334/1000 [00:42&lt;01:20,  8.30it/s] 34%|███▎      | 335/1000 [00:42&lt;01:19,  8.40it/s] 34%|███▎      | 336/1000 [00:42&lt;01:20,  8.28it/s] 34%|███▎      | 337/1000 [00:42&lt;01:20,  8.28it/s] 34%|███▍      | 338/1000 [00:42&lt;01:19,  8.29it/s] 34%|███▍      | 339/1000 [00:42&lt;01:20,  8.26it/s] 34%|███▍      | 340/1000 [00:42&lt;01:22,  7.97it/s] 34%|███▍      | 341/1000 [00:42&lt;01:21,  8.08it/s] 34%|███▍      | 342/1000 [00:43&lt;01:19,  8.25it/s] 34%|███▍      | 343/1000 [00:43&lt;01:19,  8.31it/s] 34%|███▍      | 344/1000 [00:43&lt;01:20,  8.13it/s] 34%|███▍      | 345/1000 [00:43&lt;01:19,  8.24it/s] 35%|███▍      | 346/1000 [00:43&lt;01:18,  8.36it/s] 35%|███▍      | 347/1000 [00:43&lt;01:18,  8.28it/s] 35%|███▍      | 348/1000 [00:43&lt;01:17,  8.39it/s] 35%|███▍      | 349/1000 [00:43&lt;01:18,  8.28it/s] 35%|███▌      | 350/1000 [00:43&lt;01:17,  8.38it/s] 35%|███▌      | 351/1000 [00:44&lt;01:16,  8.48it/s] 35%|███▌      | 352/1000 [00:44&lt;01:15,  8.55it/s] 35%|███▌      | 353/1000 [00:44&lt;01:15,  8.53it/s] 35%|███▌      | 354/1000 [00:44&lt;01:17,  8.35it/s] 36%|███▌      | 355/1000 [00:44&lt;01:16,  8.39it/s] 36%|███▌      | 356/1000 [00:44&lt;01:15,  8.53it/s] 36%|███▌      | 357/1000 [00:44&lt;01:18,  8.21it/s] 36%|███▌      | 358/1000 [00:44&lt;01:18,  8.16it/s] 36%|███▌      | 359/1000 [00:45&lt;01:18,  8.13it/s] 36%|███▌      | 360/1000 [00:45&lt;01:17,  8.27it/s] 36%|███▌      | 361/1000 [00:45&lt;01:16,  8.33it/s] 36%|███▌      | 362/1000 [00:45&lt;01:15,  8.44it/s] 36%|███▋      | 363/1000 [00:45&lt;01:14,  8.54it/s] 36%|███▋      | 364/1000 [00:45&lt;01:14,  8.50it/s] 36%|███▋      | 365/1000 [00:45&lt;01:17,  8.15it/s] 37%|███▋      | 366/1000 [00:45&lt;01:16,  8.25it/s] 37%|███▋      | 367/1000 [00:46&lt;01:15,  8.38it/s] 37%|███▋      | 368/1000 [00:46&lt;01:14,  8.47it/s] 37%|███▋      | 369/1000 [00:46&lt;01:15,  8.36it/s] 37%|███▋      | 370/1000 [00:46&lt;01:15,  8.39it/s] 37%|███▋      | 371/1000 [00:46&lt;01:14,  8.46it/s] 37%|███▋      | 372/1000 [00:46&lt;01:14,  8.38it/s] 37%|███▋      | 373/1000 [00:46&lt;01:17,  8.12it/s] 37%|███▋      | 374/1000 [00:46&lt;01:18,  8.00it/s] 38%|███▊      | 375/1000 [00:46&lt;01:16,  8.12it/s] 38%|███▊      | 376/1000 [00:47&lt;01:15,  8.26it/s] 38%|███▊      | 377/1000 [00:47&lt;01:14,  8.36it/s] 38%|███▊      | 378/1000 [00:47&lt;01:13,  8.42it/s] 38%|███▊      | 379/1000 [00:47&lt;01:13,  8.47it/s] 38%|███▊      | 380/1000 [00:47&lt;01:14,  8.37it/s] 38%|███▊      | 381/1000 [00:47&lt;01:14,  8.36it/s] 38%|███▊      | 382/1000 [00:47&lt;01:12,  8.52it/s] 38%|███▊      | 383/1000 [00:47&lt;01:11,  8.60it/s] 38%|███▊      | 384/1000 [00:48&lt;01:10,  8.71it/s] 38%|███▊      | 385/1000 [00:48&lt;01:10,  8.76it/s] 39%|███▊      | 386/1000 [00:48&lt;01:12,  8.51it/s] 39%|███▊      | 387/1000 [00:48&lt;01:11,  8.52it/s] 39%|███▉      | 388/1000 [00:48&lt;01:10,  8.62it/s] 39%|███▉      | 389/1000 [00:48&lt;01:11,  8.53it/s] 39%|███▉      | 390/1000 [00:48&lt;01:13,  8.29it/s] 39%|███▉      | 391/1000 [00:48&lt;01:17,  7.89it/s] 39%|███▉      | 392/1000 [00:49&lt;01:16,  7.98it/s] 39%|███▉      | 393/1000 [00:49&lt;01:15,  8.00it/s] 39%|███▉      | 394/1000 [00:49&lt;01:14,  8.14it/s] 40%|███▉      | 395/1000 [00:49&lt;01:12,  8.31it/s] 40%|███▉      | 396/1000 [00:49&lt;01:12,  8.35it/s] 40%|███▉      | 397/1000 [00:49&lt;01:12,  8.33it/s] 40%|███▉      | 398/1000 [00:49&lt;01:12,  8.29it/s] 40%|███▉      | 399/1000 [00:49&lt;01:12,  8.33it/s] 40%|████      | 400/1000 [00:49&lt;01:12,  8.23it/s] 40%|████      | 401/1000 [00:50&lt;01:16,  7.85it/s] 40%|████      | 402/1000 [00:50&lt;01:16,  7.78it/s] 40%|████      | 403/1000 [00:50&lt;01:14,  8.01it/s] 40%|████      | 404/1000 [00:50&lt;01:12,  8.26it/s] 40%|████      | 405/1000 [00:50&lt;01:13,  8.07it/s] 41%|████      | 406/1000 [00:50&lt;01:14,  7.96it/s] 41%|████      | 407/1000 [00:50&lt;01:14,  7.97it/s] 41%|████      | 408/1000 [00:50&lt;01:13,  8.03it/s] 41%|████      | 409/1000 [00:51&lt;01:12,  8.11it/s] 41%|████      | 410/1000 [00:51&lt;01:11,  8.24it/s] 41%|████      | 411/1000 [00:51&lt;01:11,  8.28it/s] 41%|████      | 412/1000 [00:51&lt;01:09,  8.41it/s] 41%|████▏     | 413/1000 [00:51&lt;01:09,  8.49it/s] 41%|████▏     | 414/1000 [00:51&lt;01:11,  8.21it/s] 42%|████▏     | 415/1000 [00:51&lt;01:12,  8.06it/s] 42%|████▏     | 416/1000 [00:51&lt;01:11,  8.17it/s] 42%|████▏     | 417/1000 [00:52&lt;01:11,  8.18it/s] 42%|████▏     | 418/1000 [00:52&lt;01:11,  8.13it/s] 42%|████▏     | 419/1000 [00:52&lt;01:12,  8.06it/s] 42%|████▏     | 420/1000 [00:52&lt;01:10,  8.19it/s] 42%|████▏     | 421/1000 [00:52&lt;01:11,  8.08it/s] 42%|████▏     | 422/1000 [00:52&lt;01:11,  8.13it/s] 42%|████▏     | 423/1000 [00:52&lt;01:09,  8.34it/s] 42%|████▏     | 424/1000 [00:52&lt;01:08,  8.43it/s] 42%|████▎     | 425/1000 [00:53&lt;01:08,  8.45it/s] 43%|████▎     | 426/1000 [00:53&lt;01:06,  8.59it/s] 43%|████▎     | 427/1000 [00:53&lt;01:07,  8.51it/s] 43%|████▎     | 428/1000 [00:53&lt;01:10,  8.16it/s] 43%|████▎     | 429/1000 [00:53&lt;01:09,  8.23it/s] 43%|████▎     | 430/1000 [00:53&lt;01:07,  8.40it/s] 43%|████▎     | 431/1000 [00:53&lt;01:06,  8.55it/s] 43%|████▎     | 432/1000 [00:53&lt;01:06,  8.54it/s] 43%|████▎     | 433/1000 [00:53&lt;01:05,  8.66it/s] 43%|████▎     | 434/1000 [00:54&lt;01:05,  8.66it/s] 44%|████▎     | 435/1000 [00:54&lt;01:04,  8.77it/s] 44%|████▎     | 436/1000 [00:54&lt;01:04,  8.79it/s] 44%|████▎     | 437/1000 [00:54&lt;01:03,  8.86it/s] 44%|████▍     | 438/1000 [00:54&lt;01:03,  8.80it/s] 44%|████▍     | 439/1000 [00:54&lt;01:04,  8.76it/s] 44%|████▍     | 440/1000 [00:54&lt;01:06,  8.40it/s] 44%|████▍     | 441/1000 [00:54&lt;01:07,  8.31it/s] 44%|████▍     | 442/1000 [00:55&lt;01:08,  8.17it/s] 44%|████▍     | 443/1000 [00:55&lt;01:06,  8.33it/s] 44%|████▍     | 444/1000 [00:55&lt;01:06,  8.32it/s] 44%|████▍     | 445/1000 [00:55&lt;01:05,  8.52it/s] 45%|████▍     | 446/1000 [00:55&lt;01:04,  8.60it/s] 45%|████▍     | 447/1000 [00:55&lt;01:04,  8.56it/s] 45%|████▍     | 448/1000 [00:55&lt;01:04,  8.56it/s] 45%|████▍     | 449/1000 [00:55&lt;01:03,  8.63it/s] 45%|████▌     | 450/1000 [00:55&lt;01:05,  8.42it/s] 45%|████▌     | 451/1000 [00:56&lt;01:06,  8.21it/s] 45%|████▌     | 452/1000 [00:56&lt;01:05,  8.36it/s] 45%|████▌     | 453/1000 [00:56&lt;01:04,  8.44it/s] 45%|████▌     | 454/1000 [00:56&lt;01:03,  8.61it/s] 46%|████▌     | 455/1000 [00:56&lt;01:02,  8.69it/s] 46%|████▌     | 456/1000 [00:56&lt;01:03,  8.56it/s] 46%|████▌     | 457/1000 [00:56&lt;01:02,  8.64it/s] 46%|████▌     | 458/1000 [00:56&lt;01:04,  8.43it/s] 46%|████▌     | 459/1000 [00:57&lt;01:05,  8.26it/s] 46%|████▌     | 460/1000 [00:57&lt;01:04,  8.39it/s] 46%|████▌     | 461/1000 [00:57&lt;01:04,  8.38it/s] 46%|████▌     | 462/1000 [00:57&lt;01:03,  8.45it/s] 46%|████▋     | 463/1000 [00:57&lt;01:02,  8.52it/s] 46%|████▋     | 464/1000 [00:57&lt;01:02,  8.59it/s] 46%|████▋     | 465/1000 [00:57&lt;01:04,  8.32it/s] 47%|████▋     | 466/1000 [00:57&lt;01:05,  8.20it/s] 47%|████▋     | 467/1000 [00:58&lt;01:05,  8.18it/s] 47%|████▋     | 468/1000 [00:58&lt;01:03,  8.32it/s] 47%|████▋     | 469/1000 [00:58&lt;01:05,  8.17it/s] 47%|████▋     | 470/1000 [00:58&lt;01:05,  8.10it/s] 47%|████▋     | 471/1000 [00:58&lt;01:04,  8.20it/s] 47%|████▋     | 472/1000 [00:58&lt;01:04,  8.17it/s] 47%|████▋     | 473/1000 [00:58&lt;01:02,  8.37it/s] 47%|████▋     | 474/1000 [00:58&lt;01:02,  8.43it/s] 48%|████▊     | 475/1000 [00:58&lt;01:02,  8.43it/s] 48%|████▊     | 476/1000 [00:59&lt;01:02,  8.39it/s] 48%|████▊     | 477/1000 [00:59&lt;01:01,  8.55it/s] 48%|████▊     | 478/1000 [00:59&lt;01:00,  8.59it/s] 48%|████▊     | 479/1000 [00:59&lt;01:00,  8.58it/s] 48%|████▊     | 480/1000 [00:59&lt;00:59,  8.71it/s] 48%|████▊     | 481/1000 [00:59&lt;00:59,  8.67it/s] 48%|████▊     | 482/1000 [00:59&lt;00:59,  8.76it/s] 48%|████▊     | 483/1000 [00:59&lt;00:59,  8.66it/s] 48%|████▊     | 484/1000 [00:59&lt;00:59,  8.74it/s] 48%|████▊     | 485/1000 [01:00&lt;01:02,  8.27it/s] 49%|████▊     | 486/1000 [01:00&lt;01:03,  8.14it/s] 49%|████▊     | 487/1000 [01:00&lt;01:01,  8.31it/s] 49%|████▉     | 488/1000 [01:00&lt;01:00,  8.49it/s] 49%|████▉     | 489/1000 [01:00&lt;00:59,  8.56it/s] 49%|████▉     | 490/1000 [01:00&lt;00:59,  8.53it/s] 49%|████▉     | 491/1000 [01:00&lt;01:03,  8.06it/s] 49%|████▉     | 492/1000 [01:00&lt;01:02,  8.15it/s] 49%|████▉     | 493/1000 [01:01&lt;01:01,  8.24it/s] 49%|████▉     | 494/1000 [01:01&lt;01:00,  8.42it/s] 50%|████▉     | 495/1000 [01:01&lt;01:00,  8.32it/s] 50%|████▉     | 496/1000 [01:01&lt;00:59,  8.44it/s] 50%|████▉     | 497/1000 [01:01&lt;00:58,  8.53it/s] 50%|████▉     | 498/1000 [01:01&lt;00:59,  8.46it/s] 50%|████▉     | 499/1000 [01:01&lt;00:58,  8.57it/s] 50%|█████     | 500/1000 [01:01&lt;00:58,  8.60it/s] 50%|█████     | 501/1000 [01:02&lt;00:57,  8.66it/s] 50%|█████     | 502/1000 [01:02&lt;00:56,  8.78it/s] 50%|█████     | 503/1000 [01:02&lt;00:58,  8.56it/s] 50%|█████     | 504/1000 [01:02&lt;00:58,  8.41it/s] 50%|█████     | 505/1000 [01:02&lt;00:58,  8.53it/s] 51%|█████     | 506/1000 [01:02&lt;00:58,  8.44it/s] 51%|█████     | 507/1000 [01:02&lt;01:01,  8.06it/s] 51%|█████     | 508/1000 [01:02&lt;01:00,  8.07it/s] 51%|█████     | 509/1000 [01:02&lt;01:00,  8.16it/s] 51%|█████     | 510/1000 [01:03&lt;00:58,  8.33it/s] 51%|█████     | 511/1000 [01:03&lt;00:58,  8.40it/s] 51%|█████     | 512/1000 [01:03&lt;00:58,  8.40it/s] 51%|█████▏    | 513/1000 [01:03&lt;00:57,  8.44it/s] 51%|█████▏    | 514/1000 [01:03&lt;00:57,  8.45it/s] 52%|█████▏    | 515/1000 [01:03&lt;00:57,  8.51it/s] 52%|█████▏    | 516/1000 [01:03&lt;00:59,  8.14it/s] 52%|█████▏    | 517/1000 [01:03&lt;00:58,  8.23it/s] 52%|█████▏    | 518/1000 [01:04&lt;00:58,  8.27it/s] 52%|█████▏    | 519/1000 [01:04&lt;00:58,  8.26it/s] 52%|█████▏    | 520/1000 [01:04&lt;00:58,  8.23it/s] 52%|█████▏    | 521/1000 [01:04&lt;00:58,  8.26it/s] 52%|█████▏    | 522/1000 [01:04&lt;00:57,  8.33it/s] 52%|█████▏    | 523/1000 [01:04&lt;00:57,  8.32it/s] 52%|█████▏    | 524/1000 [01:04&lt;00:57,  8.33it/s] 52%|█████▎    | 525/1000 [01:04&lt;00:56,  8.38it/s] 53%|█████▎    | 526/1000 [01:05&lt;00:57,  8.19it/s] 53%|█████▎    | 527/1000 [01:05&lt;00:58,  8.03it/s] 53%|█████▎    | 528/1000 [01:05&lt;00:59,  7.91it/s] 53%|█████▎    | 529/1000 [01:05&lt;01:00,  7.85it/s] 53%|█████▎    | 530/1000 [01:05&lt;01:00,  7.79it/s] 53%|█████▎    | 531/1000 [01:05&lt;00:59,  7.89it/s] 53%|█████▎    | 532/1000 [01:05&lt;00:58,  8.01it/s] 53%|█████▎    | 533/1000 [01:05&lt;00:57,  8.12it/s] 53%|█████▎    | 534/1000 [01:06&lt;00:56,  8.21it/s] 54%|█████▎    | 535/1000 [01:06&lt;00:55,  8.34it/s] 54%|█████▎    | 536/1000 [01:06&lt;00:56,  8.29it/s] 54%|█████▎    | 537/1000 [01:06&lt;00:55,  8.34it/s] 54%|█████▍    | 538/1000 [01:06&lt;00:55,  8.33it/s] 54%|█████▍    | 539/1000 [01:06&lt;00:56,  8.14it/s] 54%|█████▍    | 540/1000 [01:06&lt;00:57,  7.98it/s] 54%|█████▍    | 541/1000 [01:06&lt;00:57,  7.95it/s] 54%|█████▍    | 542/1000 [01:07&lt;00:57,  7.93it/s] 54%|█████▍    | 543/1000 [01:07&lt;00:57,  7.99it/s] 54%|█████▍    | 544/1000 [01:07&lt;00:56,  8.03it/s] 55%|█████▍    | 545/1000 [01:07&lt;00:56,  8.03it/s] 55%|█████▍    | 546/1000 [01:07&lt;00:58,  7.78it/s] 55%|█████▍    | 547/1000 [01:07&lt;00:57,  7.90it/s] 55%|█████▍    | 548/1000 [01:07&lt;00:57,  7.88it/s] 55%|█████▍    | 549/1000 [01:07&lt;00:56,  8.00it/s] 55%|█████▌    | 550/1000 [01:08&lt;00:55,  8.13it/s] 55%|█████▌    | 551/1000 [01:08&lt;00:54,  8.17it/s] 55%|█████▌    | 552/1000 [01:08&lt;00:55,  8.14it/s] 55%|█████▌    | 553/1000 [01:08&lt;00:54,  8.25it/s] 55%|█████▌    | 554/1000 [01:08&lt;00:53,  8.37it/s] 56%|█████▌    | 555/1000 [01:08&lt;00:53,  8.36it/s] 56%|█████▌    | 556/1000 [01:08&lt;00:52,  8.47it/s] 56%|█████▌    | 557/1000 [01:08&lt;00:52,  8.51it/s] 56%|█████▌    | 558/1000 [01:08&lt;00:51,  8.57it/s] 56%|█████▌    | 559/1000 [01:09&lt;00:51,  8.61it/s] 56%|█████▌    | 560/1000 [01:09&lt;00:50,  8.65it/s] 56%|█████▌    | 561/1000 [01:09&lt;00:50,  8.68it/s] 56%|█████▌    | 562/1000 [01:09&lt;00:51,  8.55it/s] 56%|█████▋    | 563/1000 [01:09&lt;00:50,  8.61it/s] 56%|█████▋    | 564/1000 [01:09&lt;00:50,  8.59it/s] 56%|█████▋    | 565/1000 [01:09&lt;00:51,  8.37it/s] 57%|█████▋    | 566/1000 [01:09&lt;00:52,  8.20it/s] 57%|█████▋    | 567/1000 [01:10&lt;00:53,  8.11it/s] 57%|█████▋    | 568/1000 [01:10&lt;00:56,  7.64it/s] 57%|█████▋    | 569/1000 [01:10&lt;00:55,  7.72it/s] 57%|█████▋    | 570/1000 [01:10&lt;00:54,  7.93it/s] 57%|█████▋    | 571/1000 [01:10&lt;00:52,  8.10it/s] 57%|█████▋    | 572/1000 [01:10&lt;00:52,  8.09it/s] 57%|█████▋    | 573/1000 [01:10&lt;00:52,  8.20it/s] 57%|█████▋    | 574/1000 [01:10&lt;00:51,  8.26it/s] 57%|█████▊    | 575/1000 [01:11&lt;00:50,  8.34it/s] 58%|█████▊    | 576/1000 [01:11&lt;00:50,  8.33it/s] 58%|█████▊    | 577/1000 [01:11&lt;00:50,  8.30it/s] 58%|█████▊    | 578/1000 [01:11&lt;00:50,  8.36it/s] 58%|█████▊    | 579/1000 [01:11&lt;00:50,  8.35it/s] 58%|█████▊    | 580/1000 [01:11&lt;00:51,  8.15it/s] 58%|█████▊    | 581/1000 [01:11&lt;00:51,  8.18it/s] 58%|█████▊    | 582/1000 [01:11&lt;00:49,  8.38it/s] 58%|█████▊    | 583/1000 [01:11&lt;00:49,  8.46it/s] 58%|█████▊    | 584/1000 [01:12&lt;00:48,  8.53it/s] 58%|█████▊    | 585/1000 [01:12&lt;00:48,  8.59it/s] 59%|█████▊    | 586/1000 [01:12&lt;00:49,  8.43it/s] 59%|█████▊    | 587/1000 [01:12&lt;00:49,  8.42it/s] 59%|█████▉    | 588/1000 [01:12&lt;00:49,  8.37it/s] 59%|█████▉    | 589/1000 [01:12&lt;00:48,  8.47it/s] 59%|█████▉    | 590/1000 [01:12&lt;00:50,  8.13it/s] 59%|█████▉    | 591/1000 [01:12&lt;00:51,  8.00it/s] 59%|█████▉    | 592/1000 [01:13&lt;00:49,  8.16it/s] 59%|█████▉    | 593/1000 [01:13&lt;00:49,  8.21it/s] 59%|█████▉    | 594/1000 [01:13&lt;00:49,  8.14it/s] 60%|█████▉    | 595/1000 [01:13&lt;00:49,  8.24it/s] 60%|█████▉    | 596/1000 [01:13&lt;00:48,  8.37it/s] 60%|█████▉    | 597/1000 [01:13&lt;00:48,  8.38it/s] 60%|█████▉    | 598/1000 [01:13&lt;00:47,  8.51it/s] 60%|█████▉    | 599/1000 [01:13&lt;00:47,  8.47it/s] 60%|██████    | 600/1000 [01:14&lt;00:47,  8.47it/s] 60%|██████    | 601/1000 [01:14&lt;00:47,  8.36it/s] 60%|██████    | 602/1000 [01:14&lt;00:47,  8.36it/s] 60%|██████    | 603/1000 [01:14&lt;00:47,  8.41it/s] 60%|██████    | 604/1000 [01:14&lt;00:47,  8.33it/s] 60%|██████    | 605/1000 [01:14&lt;00:46,  8.43it/s] 61%|██████    | 606/1000 [01:14&lt;00:46,  8.56it/s] 61%|██████    | 607/1000 [01:14&lt;00:46,  8.48it/s] 61%|██████    | 608/1000 [01:14&lt;00:46,  8.51it/s] 61%|██████    | 609/1000 [01:15&lt;00:46,  8.38it/s] 61%|██████    | 610/1000 [01:15&lt;00:46,  8.47it/s] 61%|██████    | 611/1000 [01:15&lt;00:45,  8.53it/s] 61%|██████    | 612/1000 [01:15&lt;00:44,  8.62it/s] 61%|██████▏   | 613/1000 [01:15&lt;00:45,  8.55it/s] 61%|██████▏   | 614/1000 [01:15&lt;00:45,  8.50it/s] 62%|██████▏   | 615/1000 [01:15&lt;00:46,  8.20it/s] 62%|██████▏   | 616/1000 [01:15&lt;00:47,  8.14it/s] 62%|██████▏   | 617/1000 [01:16&lt;00:46,  8.27it/s] 62%|██████▏   | 618/1000 [01:16&lt;00:45,  8.36it/s] 62%|██████▏   | 619/1000 [01:16&lt;00:45,  8.29it/s] 62%|██████▏   | 620/1000 [01:16&lt;00:45,  8.40it/s] 62%|██████▏   | 621/1000 [01:16&lt;00:45,  8.39it/s] 62%|██████▏   | 622/1000 [01:16&lt;00:46,  8.13it/s] 62%|██████▏   | 623/1000 [01:16&lt;00:45,  8.26it/s] 62%|██████▏   | 624/1000 [01:16&lt;00:45,  8.36it/s] 62%|██████▎   | 625/1000 [01:17&lt;00:44,  8.38it/s] 63%|██████▎   | 626/1000 [01:17&lt;00:45,  8.22it/s] 63%|██████▎   | 627/1000 [01:17&lt;00:46,  8.02it/s] 63%|██████▎   | 628/1000 [01:17&lt;00:45,  8.17it/s] 63%|██████▎   | 629/1000 [01:17&lt;00:44,  8.25it/s] 63%|██████▎   | 630/1000 [01:17&lt;00:44,  8.37it/s] 63%|██████▎   | 631/1000 [01:17&lt;00:43,  8.51it/s] 63%|██████▎   | 632/1000 [01:17&lt;00:43,  8.43it/s] 63%|██████▎   | 633/1000 [01:17&lt;00:43,  8.46it/s] 63%|██████▎   | 634/1000 [01:18&lt;00:42,  8.57it/s] 64%|██████▎   | 635/1000 [01:18&lt;00:42,  8.63it/s] 64%|██████▎   | 636/1000 [01:18&lt;00:43,  8.45it/s] 64%|██████▎   | 637/1000 [01:18&lt;00:42,  8.52it/s] 64%|██████▍   | 638/1000 [01:18&lt;00:42,  8.56it/s] 64%|██████▍   | 639/1000 [01:18&lt;00:41,  8.64it/s] 64%|██████▍   | 640/1000 [01:18&lt;00:43,  8.30it/s] 64%|██████▍   | 641/1000 [01:18&lt;00:44,  8.06it/s] 64%|██████▍   | 642/1000 [01:19&lt;00:43,  8.18it/s] 64%|██████▍   | 643/1000 [01:19&lt;00:44,  7.97it/s] 64%|██████▍   | 644/1000 [01:19&lt;00:44,  8.05it/s] 64%|██████▍   | 645/1000 [01:19&lt;00:43,  8.21it/s] 65%|██████▍   | 646/1000 [01:19&lt;00:43,  8.16it/s] 65%|██████▍   | 647/1000 [01:19&lt;00:43,  8.19it/s] 65%|██████▍   | 648/1000 [01:19&lt;00:42,  8.26it/s] 65%|██████▍   | 649/1000 [01:19&lt;00:42,  8.29it/s] 65%|██████▌   | 650/1000 [01:20&lt;00:42,  8.17it/s] 65%|██████▌   | 651/1000 [01:20&lt;00:45,  7.69it/s] 65%|██████▌   | 652/1000 [01:20&lt;00:45,  7.73it/s] 65%|██████▌   | 653/1000 [01:20&lt;00:43,  8.03it/s] 65%|██████▌   | 654/1000 [01:20&lt;00:42,  8.12it/s] 66%|██████▌   | 655/1000 [01:20&lt;00:42,  8.17it/s] 66%|██████▌   | 656/1000 [01:20&lt;00:41,  8.30it/s] 66%|██████▌   | 657/1000 [01:20&lt;00:40,  8.43it/s] 66%|██████▌   | 658/1000 [01:21&lt;00:39,  8.57it/s] 66%|██████▌   | 659/1000 [01:21&lt;00:39,  8.58it/s] 66%|██████▌   | 660/1000 [01:21&lt;00:39,  8.60it/s] 66%|██████▌   | 661/1000 [01:21&lt;00:39,  8.52it/s] 66%|██████▌   | 662/1000 [01:21&lt;00:39,  8.62it/s] 66%|██████▋   | 663/1000 [01:21&lt;00:39,  8.54it/s] 66%|██████▋   | 664/1000 [01:21&lt;00:39,  8.42it/s] 66%|██████▋   | 665/1000 [01:21&lt;00:40,  8.21it/s] 67%|██████▋   | 666/1000 [01:21&lt;00:40,  8.16it/s] 67%|██████▋   | 667/1000 [01:22&lt;00:40,  8.16it/s] 67%|██████▋   | 668/1000 [01:22&lt;00:40,  8.18it/s] 67%|██████▋   | 669/1000 [01:22&lt;00:41,  8.00it/s] 67%|██████▋   | 670/1000 [01:22&lt;00:40,  8.11it/s] 67%|██████▋   | 671/1000 [01:22&lt;00:40,  8.09it/s] 67%|██████▋   | 672/1000 [01:22&lt;00:40,  8.18it/s] 67%|██████▋   | 673/1000 [01:22&lt;00:39,  8.28it/s] 67%|██████▋   | 674/1000 [01:22&lt;00:39,  8.15it/s] 68%|██████▊   | 675/1000 [01:23&lt;00:40,  8.01it/s] 68%|██████▊   | 676/1000 [01:23&lt;00:39,  8.17it/s] 68%|██████▊   | 677/1000 [01:23&lt;00:38,  8.30it/s] 68%|██████▊   | 678/1000 [01:23&lt;00:38,  8.30it/s] 68%|██████▊   | 679/1000 [01:23&lt;00:38,  8.39it/s] 68%|██████▊   | 680/1000 [01:23&lt;00:38,  8.41it/s] 68%|██████▊   | 681/1000 [01:23&lt;00:37,  8.42it/s] 68%|██████▊   | 682/1000 [01:23&lt;00:37,  8.48it/s] 68%|██████▊   | 683/1000 [01:24&lt;00:37,  8.47it/s] 68%|██████▊   | 684/1000 [01:24&lt;00:37,  8.43it/s] 68%|██████▊   | 685/1000 [01:24&lt;00:37,  8.40it/s] 69%|██████▊   | 686/1000 [01:24&lt;00:36,  8.50it/s] 69%|██████▊   | 687/1000 [01:24&lt;00:36,  8.53it/s] 69%|██████▉   | 688/1000 [01:24&lt;00:36,  8.47it/s] 69%|██████▉   | 689/1000 [01:24&lt;00:36,  8.47it/s] 69%|██████▉   | 690/1000 [01:24&lt;00:38,  8.09it/s] 69%|██████▉   | 691/1000 [01:25&lt;00:39,  7.79it/s] 69%|██████▉   | 692/1000 [01:25&lt;00:39,  7.87it/s] 69%|██████▉   | 693/1000 [01:25&lt;00:38,  8.02it/s] 69%|██████▉   | 694/1000 [01:25&lt;00:37,  8.27it/s] 70%|██████▉   | 695/1000 [01:25&lt;00:36,  8.34it/s] 70%|██████▉   | 696/1000 [01:25&lt;00:36,  8.39it/s] 70%|██████▉   | 697/1000 [01:25&lt;00:36,  8.32it/s] 70%|██████▉   | 698/1000 [01:25&lt;00:35,  8.44it/s] 70%|██████▉   | 699/1000 [01:25&lt;00:35,  8.44it/s] 70%|███████   | 700/1000 [01:26&lt;00:35,  8.49it/s] 70%|███████   | 701/1000 [01:26&lt;00:35,  8.35it/s] 70%|███████   | 702/1000 [01:26&lt;00:36,  8.20it/s] 70%|███████   | 703/1000 [01:26&lt;00:36,  8.10it/s] 70%|███████   | 704/1000 [01:26&lt;00:35,  8.23it/s] 70%|███████   | 705/1000 [01:26&lt;00:36,  8.12it/s] 71%|███████   | 706/1000 [01:26&lt;00:35,  8.22it/s] 71%|███████   | 707/1000 [01:26&lt;00:35,  8.36it/s] 71%|███████   | 708/1000 [01:27&lt;00:34,  8.49it/s] 71%|███████   | 709/1000 [01:27&lt;00:33,  8.60it/s] 71%|███████   | 710/1000 [01:27&lt;00:34,  8.31it/s] 71%|███████   | 711/1000 [01:27&lt;00:35,  8.11it/s] 71%|███████   | 712/1000 [01:27&lt;00:35,  8.07it/s] 71%|███████▏  | 713/1000 [01:27&lt;00:35,  8.09it/s] 71%|███████▏  | 714/1000 [01:27&lt;00:34,  8.21it/s] 72%|███████▏  | 715/1000 [01:27&lt;00:35,  7.96it/s] 72%|███████▏  | 716/1000 [01:28&lt;00:34,  8.13it/s] 72%|███████▏  | 717/1000 [01:28&lt;00:34,  8.28it/s] 72%|███████▏  | 718/1000 [01:28&lt;00:35,  7.97it/s] 72%|███████▏  | 719/1000 [01:28&lt;00:34,  8.09it/s] 72%|███████▏  | 720/1000 [01:28&lt;00:33,  8.25it/s] 72%|███████▏  | 721/1000 [01:28&lt;00:33,  8.32it/s] 72%|███████▏  | 722/1000 [01:28&lt;00:34,  8.16it/s] 72%|███████▏  | 723/1000 [01:28&lt;00:33,  8.28it/s] 72%|███████▏  | 724/1000 [01:28&lt;00:32,  8.37it/s] 72%|███████▎  | 725/1000 [01:29&lt;00:32,  8.39it/s] 73%|███████▎  | 726/1000 [01:29&lt;00:33,  8.27it/s] 73%|███████▎  | 727/1000 [01:29&lt;00:32,  8.28it/s] 73%|███████▎  | 728/1000 [01:29&lt;00:32,  8.34it/s] 73%|███████▎  | 729/1000 [01:29&lt;00:32,  8.44it/s] 73%|███████▎  | 730/1000 [01:29&lt;00:31,  8.48it/s] 73%|███████▎  | 731/1000 [01:29&lt;00:31,  8.57it/s] 73%|███████▎  | 732/1000 [01:29&lt;00:31,  8.42it/s] 73%|███████▎  | 733/1000 [01:30&lt;00:32,  8.09it/s] 73%|███████▎  | 734/1000 [01:30&lt;00:34,  7.71it/s] 74%|███████▎  | 735/1000 [01:30&lt;00:33,  7.86it/s] 74%|███████▎  | 736/1000 [01:30&lt;00:32,  8.12it/s] 74%|███████▎  | 737/1000 [01:30&lt;00:31,  8.32it/s] 74%|███████▍  | 738/1000 [01:30&lt;00:31,  8.37it/s] 74%|███████▍  | 739/1000 [01:30&lt;00:31,  8.24it/s] 74%|███████▍  | 740/1000 [01:30&lt;00:33,  7.85it/s] 74%|███████▍  | 741/1000 [01:31&lt;00:32,  7.94it/s] 74%|███████▍  | 742/1000 [01:31&lt;00:31,  8.14it/s] 74%|███████▍  | 743/1000 [01:31&lt;00:31,  8.17it/s] 74%|███████▍  | 744/1000 [01:31&lt;00:30,  8.36it/s] 74%|███████▍  | 745/1000 [01:31&lt;00:32,  7.95it/s] 75%|███████▍  | 746/1000 [01:31&lt;00:32,  7.87it/s] 75%|███████▍  | 747/1000 [01:31&lt;00:31,  7.97it/s] 75%|███████▍  | 748/1000 [01:31&lt;00:30,  8.18it/s] 75%|███████▍  | 749/1000 [01:32&lt;00:30,  8.27it/s] 75%|███████▌  | 750/1000 [01:32&lt;00:30,  8.31it/s] 75%|███████▌  | 751/1000 [01:32&lt;00:30,  8.28it/s] 75%|███████▌  | 752/1000 [01:32&lt;00:29,  8.37it/s] 75%|███████▌  | 753/1000 [01:32&lt;00:29,  8.49it/s] 75%|███████▌  | 754/1000 [01:32&lt;00:29,  8.48it/s] 76%|███████▌  | 755/1000 [01:32&lt;00:28,  8.51it/s] 76%|███████▌  | 756/1000 [01:32&lt;00:28,  8.63it/s] 76%|███████▌  | 757/1000 [01:32&lt;00:28,  8.52it/s] 76%|███████▌  | 758/1000 [01:33&lt;00:28,  8.53it/s] 76%|███████▌  | 759/1000 [01:33&lt;00:28,  8.59it/s] 76%|███████▌  | 760/1000 [01:33&lt;00:27,  8.59it/s] 76%|███████▌  | 761/1000 [01:33&lt;00:27,  8.64it/s] 76%|███████▌  | 762/1000 [01:33&lt;00:27,  8.76it/s] 76%|███████▋  | 763/1000 [01:33&lt;00:26,  8.78it/s] 76%|███████▋  | 764/1000 [01:33&lt;00:27,  8.59it/s] 76%|███████▋  | 765/1000 [01:33&lt;00:28,  8.29it/s] 77%|███████▋  | 766/1000 [01:34&lt;00:28,  8.31it/s] 77%|███████▋  | 767/1000 [01:34&lt;00:28,  8.16it/s] 77%|███████▋  | 768/1000 [01:34&lt;00:28,  8.10it/s] 77%|███████▋  | 769/1000 [01:34&lt;00:27,  8.27it/s] 77%|███████▋  | 770/1000 [01:34&lt;00:27,  8.37it/s] 77%|███████▋  | 771/1000 [01:34&lt;00:27,  8.47it/s] 77%|███████▋  | 772/1000 [01:34&lt;00:27,  8.28it/s] 77%|███████▋  | 773/1000 [01:34&lt;00:27,  8.36it/s] 77%|███████▋  | 774/1000 [01:35&lt;00:27,  8.16it/s] 78%|███████▊  | 775/1000 [01:35&lt;00:28,  7.93it/s] 78%|███████▊  | 776/1000 [01:35&lt;00:27,  8.02it/s] 78%|███████▊  | 777/1000 [01:35&lt;00:26,  8.27it/s] 78%|███████▊  | 778/1000 [01:35&lt;00:26,  8.42it/s] 78%|███████▊  | 779/1000 [01:35&lt;00:27,  7.95it/s] 78%|███████▊  | 780/1000 [01:35&lt;00:27,  7.90it/s] 78%|███████▊  | 781/1000 [01:35&lt;00:27,  8.04it/s] 78%|███████▊  | 782/1000 [01:36&lt;00:26,  8.20it/s] 78%|███████▊  | 783/1000 [01:36&lt;00:26,  8.24it/s] 78%|███████▊  | 784/1000 [01:36&lt;00:27,  7.95it/s] 78%|███████▊  | 785/1000 [01:36&lt;00:28,  7.59it/s] 79%|███████▊  | 786/1000 [01:36&lt;00:27,  7.68it/s] 79%|███████▊  | 787/1000 [01:36&lt;00:27,  7.64it/s] 79%|███████▉  | 788/1000 [01:36&lt;00:27,  7.70it/s] 79%|███████▉  | 789/1000 [01:36&lt;00:28,  7.46it/s] 79%|███████▉  | 790/1000 [01:37&lt;00:28,  7.35it/s] 79%|███████▉  | 791/1000 [01:37&lt;00:27,  7.60it/s] 79%|███████▉  | 792/1000 [01:37&lt;00:26,  7.75it/s] 79%|███████▉  | 793/1000 [01:37&lt;00:27,  7.52it/s] 79%|███████▉  | 794/1000 [01:37&lt;00:26,  7.71it/s] 80%|███████▉  | 795/1000 [01:37&lt;00:25,  7.90it/s] 80%|███████▉  | 796/1000 [01:37&lt;00:25,  8.07it/s] 80%|███████▉  | 797/1000 [01:37&lt;00:25,  8.11it/s] 80%|███████▉  | 798/1000 [01:38&lt;00:26,  7.72it/s] 80%|███████▉  | 799/1000 [01:38&lt;00:25,  7.75it/s] 80%|████████  | 800/1000 [01:38&lt;00:25,  7.94it/s] 80%|████████  | 801/1000 [01:38&lt;00:24,  8.09it/s] 80%|████████  | 802/1000 [01:38&lt;00:24,  8.22it/s] 80%|████████  | 803/1000 [01:38&lt;00:24,  7.92it/s] 80%|████████  | 804/1000 [01:38&lt;00:24,  8.04it/s] 80%|████████  | 805/1000 [01:38&lt;00:23,  8.18it/s] 81%|████████  | 806/1000 [01:39&lt;00:23,  8.35it/s] 81%|████████  | 807/1000 [01:39&lt;00:22,  8.43it/s] 81%|████████  | 808/1000 [01:39&lt;00:23,  8.04it/s] 81%|████████  | 809/1000 [01:39&lt;00:24,  7.90it/s] 81%|████████  | 810/1000 [01:39&lt;00:23,  8.12it/s] 81%|████████  | 811/1000 [01:39&lt;00:22,  8.23it/s] 81%|████████  | 812/1000 [01:39&lt;00:23,  8.08it/s] 81%|████████▏ | 813/1000 [01:39&lt;00:24,  7.74it/s] 81%|████████▏ | 814/1000 [01:40&lt;00:24,  7.57it/s] 82%|████████▏ | 815/1000 [01:40&lt;00:24,  7.43it/s] 82%|████████▏ | 816/1000 [01:40&lt;00:23,  7.72it/s] 82%|████████▏ | 817/1000 [01:40&lt;00:23,  7.72it/s] 82%|████████▏ | 818/1000 [01:40&lt;00:22,  7.97it/s] 82%|████████▏ | 819/1000 [01:40&lt;00:22,  8.03it/s] 82%|████████▏ | 820/1000 [01:40&lt;00:22,  8.10it/s] 82%|████████▏ | 821/1000 [01:40&lt;00:21,  8.16it/s] 82%|████████▏ | 822/1000 [01:41&lt;00:22,  7.81it/s] 82%|████████▏ | 823/1000 [01:41&lt;00:23,  7.64it/s] 82%|████████▏ | 824/1000 [01:41&lt;00:22,  7.78it/s] 82%|████████▎ | 825/1000 [01:41&lt;00:21,  8.01it/s] 83%|████████▎ | 826/1000 [01:41&lt;00:22,  7.73it/s] 83%|████████▎ | 827/1000 [01:41&lt;00:22,  7.57it/s] 83%|████████▎ | 828/1000 [01:41&lt;00:22,  7.81it/s] 83%|████████▎ | 829/1000 [01:41&lt;00:21,  8.05it/s] 83%|████████▎ | 830/1000 [01:42&lt;00:20,  8.24it/s] 83%|████████▎ | 831/1000 [01:42&lt;00:20,  8.25it/s] 83%|████████▎ | 832/1000 [01:42&lt;00:20,  8.25it/s] 83%|████████▎ | 833/1000 [01:42&lt;00:20,  8.23it/s] 83%|████████▎ | 834/1000 [01:42&lt;00:20,  8.17it/s] 84%|████████▎ | 835/1000 [01:42&lt;00:20,  8.15it/s] 84%|████████▎ | 836/1000 [01:42&lt;00:21,  7.79it/s] 84%|████████▎ | 837/1000 [01:42&lt;00:21,  7.64it/s] 84%|████████▍ | 838/1000 [01:43&lt;00:20,  7.88it/s] 84%|████████▍ | 839/1000 [01:43&lt;00:20,  7.93it/s] 84%|████████▍ | 840/1000 [01:43&lt;00:20,  7.69it/s] 84%|████████▍ | 841/1000 [01:43&lt;00:20,  7.62it/s] 84%|████████▍ | 842/1000 [01:43&lt;00:19,  7.93it/s] 84%|████████▍ | 843/1000 [01:43&lt;00:19,  8.11it/s] 84%|████████▍ | 844/1000 [01:43&lt;00:19,  8.17it/s] 84%|████████▍ | 845/1000 [01:43&lt;00:19,  8.04it/s] 85%|████████▍ | 846/1000 [01:44&lt;00:19,  8.01it/s] 85%|████████▍ | 847/1000 [01:44&lt;00:18,  8.09it/s] 85%|████████▍ | 848/1000 [01:44&lt;00:18,  8.15it/s] 85%|████████▍ | 849/1000 [01:44&lt;00:18,  8.07it/s] 85%|████████▌ | 850/1000 [01:44&lt;00:19,  7.82it/s] 85%|████████▌ | 851/1000 [01:44&lt;00:19,  7.64it/s] 85%|████████▌ | 852/1000 [01:44&lt;00:19,  7.69it/s] 85%|████████▌ | 853/1000 [01:45&lt;00:18,  7.79it/s] 85%|████████▌ | 854/1000 [01:45&lt;00:18,  7.78it/s] 86%|████████▌ | 855/1000 [01:45&lt;00:18,  7.72it/s] 86%|████████▌ | 856/1000 [01:45&lt;00:18,  7.93it/s] 86%|████████▌ | 857/1000 [01:45&lt;00:17,  8.13it/s] 86%|████████▌ | 858/1000 [01:45&lt;00:17,  8.26it/s] 86%|████████▌ | 859/1000 [01:45&lt;00:16,  8.33it/s] 86%|████████▌ | 860/1000 [01:45&lt;00:17,  8.16it/s] 86%|████████▌ | 861/1000 [01:45&lt;00:17,  8.12it/s] 86%|████████▌ | 862/1000 [01:46&lt;00:16,  8.27it/s] 86%|████████▋ | 863/1000 [01:46&lt;00:16,  8.34it/s] 86%|████████▋ | 864/1000 [01:46&lt;00:17,  7.83it/s] 86%|████████▋ | 865/1000 [01:46&lt;00:17,  7.78it/s] 87%|████████▋ | 866/1000 [01:46&lt;00:17,  7.88it/s] 87%|████████▋ | 867/1000 [01:46&lt;00:16,  7.91it/s] 87%|████████▋ | 868/1000 [01:46&lt;00:16,  8.07it/s] 87%|████████▋ | 869/1000 [01:47&lt;00:16,  7.96it/s] 87%|████████▋ | 870/1000 [01:47&lt;00:16,  7.77it/s] 87%|████████▋ | 871/1000 [01:47&lt;00:16,  7.94it/s] 87%|████████▋ | 872/1000 [01:47&lt;00:15,  8.10it/s] 87%|████████▋ | 873/1000 [01:47&lt;00:15,  8.14it/s] 87%|████████▋ | 874/1000 [01:47&lt;00:15,  7.90it/s] 88%|████████▊ | 875/1000 [01:47&lt;00:15,  8.06it/s] 88%|████████▊ | 876/1000 [01:47&lt;00:15,  8.24it/s] 88%|████████▊ | 877/1000 [01:47&lt;00:15,  8.14it/s] 88%|████████▊ | 878/1000 [01:48&lt;00:15,  7.80it/s] 88%|████████▊ | 879/1000 [01:48&lt;00:15,  7.74it/s] 88%|████████▊ | 880/1000 [01:48&lt;00:15,  7.89it/s] 88%|████████▊ | 881/1000 [01:48&lt;00:14,  8.06it/s] 88%|████████▊ | 882/1000 [01:48&lt;00:14,  8.23it/s] 88%|████████▊ | 883/1000 [01:48&lt;00:13,  8.36it/s] 88%|████████▊ | 884/1000 [01:48&lt;00:13,  8.35it/s] 88%|████████▊ | 885/1000 [01:48&lt;00:14,  7.96it/s] 89%|████████▊ | 886/1000 [01:49&lt;00:14,  8.10it/s] 89%|████████▊ | 887/1000 [01:49&lt;00:14,  8.04it/s] 89%|████████▉ | 888/1000 [01:49&lt;00:13,  8.13it/s] 89%|████████▉ | 889/1000 [01:49&lt;00:13,  8.15it/s] 89%|████████▉ | 890/1000 [01:49&lt;00:13,  8.10it/s] 89%|████████▉ | 891/1000 [01:49&lt;00:13,  8.06it/s] 89%|████████▉ | 892/1000 [01:49&lt;00:13,  8.09it/s] 89%|████████▉ | 893/1000 [01:49&lt;00:13,  7.99it/s] 89%|████████▉ | 894/1000 [01:50&lt;00:13,  7.65it/s] 90%|████████▉ | 895/1000 [01:50&lt;00:13,  7.58it/s] 90%|████████▉ | 896/1000 [01:50&lt;00:13,  7.80it/s] 90%|████████▉ | 897/1000 [01:50&lt;00:12,  8.01it/s] 90%|████████▉ | 898/1000 [01:50&lt;00:12,  8.13it/s] 90%|████████▉ | 899/1000 [01:50&lt;00:12,  8.21it/s] 90%|█████████ | 900/1000 [01:50&lt;00:12,  8.32it/s] 90%|█████████ | 901/1000 [01:50&lt;00:11,  8.38it/s] 90%|█████████ | 902/1000 [01:51&lt;00:11,  8.38it/s] 90%|█████████ | 903/1000 [01:51&lt;00:11,  8.40it/s] 90%|█████████ | 904/1000 [01:51&lt;00:11,  8.41it/s] 90%|█████████ | 905/1000 [01:51&lt;00:11,  8.53it/s] 91%|█████████ | 906/1000 [01:51&lt;00:11,  8.02it/s] 91%|█████████ | 907/1000 [01:51&lt;00:12,  7.74it/s] 91%|█████████ | 908/1000 [01:51&lt;00:11,  7.87it/s] 91%|█████████ | 909/1000 [01:51&lt;00:11,  7.67it/s] 91%|█████████ | 910/1000 [01:52&lt;00:11,  7.87it/s] 91%|█████████ | 911/1000 [01:52&lt;00:11,  8.07it/s] 91%|█████████ | 912/1000 [01:52&lt;00:10,  8.07it/s] 91%|█████████▏| 913/1000 [01:52&lt;00:10,  8.21it/s] 91%|█████████▏| 914/1000 [01:52&lt;00:10,  8.13it/s] 92%|█████████▏| 915/1000 [01:52&lt;00:10,  8.17it/s] 92%|█████████▏| 916/1000 [01:52&lt;00:10,  8.25it/s] 92%|█████████▏| 917/1000 [01:52&lt;00:10,  8.24it/s] 92%|█████████▏| 918/1000 [01:53&lt;00:10,  8.13it/s] 92%|█████████▏| 919/1000 [01:53&lt;00:09,  8.22it/s] 92%|█████████▏| 920/1000 [01:53&lt;00:09,  8.33it/s] 92%|█████████▏| 921/1000 [01:53&lt;00:09,  8.05it/s] 92%|█████████▏| 922/1000 [01:53&lt;00:09,  8.17it/s] 92%|█████████▏| 923/1000 [01:53&lt;00:09,  8.25it/s] 92%|█████████▏| 924/1000 [01:53&lt;00:09,  8.26it/s] 92%|█████████▎| 925/1000 [01:53&lt;00:09,  8.30it/s] 93%|█████████▎| 926/1000 [01:54&lt;00:08,  8.34it/s] 93%|█████████▎| 927/1000 [01:54&lt;00:08,  8.33it/s] 93%|█████████▎| 928/1000 [01:54&lt;00:08,  8.36it/s] 93%|█████████▎| 929/1000 [01:54&lt;00:08,  8.41it/s] 93%|█████████▎| 930/1000 [01:54&lt;00:08,  8.42it/s] 93%|█████████▎| 931/1000 [01:54&lt;00:08,  8.39it/s] 93%|█████████▎| 932/1000 [01:54&lt;00:08,  8.38it/s] 93%|█████████▎| 933/1000 [01:54&lt;00:07,  8.41it/s] 93%|█████████▎| 934/1000 [01:55&lt;00:08,  8.08it/s] 94%|█████████▎| 935/1000 [01:55&lt;00:08,  8.06it/s] 94%|█████████▎| 936/1000 [01:55&lt;00:07,  8.12it/s] 94%|█████████▎| 937/1000 [01:55&lt;00:07,  8.27it/s] 94%|█████████▍| 938/1000 [01:55&lt;00:07,  8.41it/s] 94%|█████████▍| 939/1000 [01:55&lt;00:07,  8.47it/s] 94%|█████████▍| 940/1000 [01:55&lt;00:07,  8.32it/s] 94%|█████████▍| 941/1000 [01:55&lt;00:07,  8.39it/s] 94%|█████████▍| 942/1000 [01:55&lt;00:06,  8.33it/s] 94%|█████████▍| 943/1000 [01:56&lt;00:06,  8.34it/s] 94%|█████████▍| 944/1000 [01:56&lt;00:06,  8.38it/s] 94%|█████████▍| 945/1000 [01:56&lt;00:06,  8.33it/s] 95%|█████████▍| 946/1000 [01:56&lt;00:06,  8.06it/s] 95%|█████████▍| 947/1000 [01:56&lt;00:06,  8.09it/s] 95%|█████████▍| 948/1000 [01:56&lt;00:06,  8.10it/s] 95%|█████████▍| 949/1000 [01:56&lt;00:06,  8.28it/s] 95%|█████████▌| 950/1000 [01:56&lt;00:05,  8.40it/s] 95%|█████████▌| 951/1000 [01:57&lt;00:05,  8.46it/s] 95%|█████████▌| 952/1000 [01:57&lt;00:05,  8.54it/s] 95%|█████████▌| 953/1000 [01:57&lt;00:05,  8.48it/s] 95%|█████████▌| 954/1000 [01:57&lt;00:05,  8.42it/s] 96%|█████████▌| 955/1000 [01:57&lt;00:05,  8.41it/s] 96%|█████████▌| 956/1000 [01:57&lt;00:05,  8.27it/s] 96%|█████████▌| 957/1000 [01:57&lt;00:05,  8.11it/s] 96%|█████████▌| 958/1000 [01:57&lt;00:05,  8.14it/s] 96%|█████████▌| 959/1000 [01:58&lt;00:05,  7.91it/s] 96%|█████████▌| 960/1000 [01:58&lt;00:05,  7.89it/s] 96%|█████████▌| 961/1000 [01:58&lt;00:05,  7.73it/s] 96%|█████████▌| 962/1000 [01:58&lt;00:04,  7.89it/s] 96%|█████████▋| 963/1000 [01:58&lt;00:04,  7.96it/s] 96%|█████████▋| 964/1000 [01:58&lt;00:04,  8.11it/s] 96%|█████████▋| 965/1000 [01:58&lt;00:04,  8.16it/s] 97%|█████████▋| 966/1000 [01:58&lt;00:04,  8.18it/s] 97%|█████████▋| 967/1000 [01:59&lt;00:04,  8.21it/s] 97%|█████████▋| 968/1000 [01:59&lt;00:03,  8.25it/s] 97%|█████████▋| 969/1000 [01:59&lt;00:03,  8.22it/s] 97%|█████████▋| 970/1000 [01:59&lt;00:03,  8.14it/s] 97%|█████████▋| 971/1000 [01:59&lt;00:03,  8.18it/s] 97%|█████████▋| 972/1000 [01:59&lt;00:03,  8.12it/s] 97%|█████████▋| 973/1000 [01:59&lt;00:03,  8.18it/s] 97%|█████████▋| 974/1000 [01:59&lt;00:03,  8.25it/s] 98%|█████████▊| 975/1000 [01:59&lt;00:03,  8.23it/s] 98%|█████████▊| 976/1000 [02:00&lt;00:03,  7.83it/s] 98%|█████████▊| 977/1000 [02:00&lt;00:02,  7.77it/s] 98%|█████████▊| 978/1000 [02:00&lt;00:02,  7.93it/s] 98%|█████████▊| 979/1000 [02:00&lt;00:02,  8.03it/s] 98%|█████████▊| 980/1000 [02:00&lt;00:02,  8.10it/s] 98%|█████████▊| 981/1000 [02:00&lt;00:02,  8.25it/s] 98%|█████████▊| 982/1000 [02:00&lt;00:02,  8.33it/s] 98%|█████████▊| 983/1000 [02:00&lt;00:02,  8.08it/s] 98%|█████████▊| 984/1000 [02:01&lt;00:02,  8.00it/s] 98%|█████████▊| 985/1000 [02:01&lt;00:01,  8.16it/s] 99%|█████████▊| 986/1000 [02:01&lt;00:01,  8.23it/s] 99%|█████████▊| 987/1000 [02:01&lt;00:01,  8.38it/s] 99%|█████████▉| 988/1000 [02:01&lt;00:01,  8.45it/s] 99%|█████████▉| 989/1000 [02:01&lt;00:01,  8.44it/s] 99%|█████████▉| 990/1000 [02:01&lt;00:01,  8.45it/s] 99%|█████████▉| 991/1000 [02:01&lt;00:01,  8.51it/s] 99%|█████████▉| 992/1000 [02:02&lt;00:00,  8.49it/s] 99%|█████████▉| 993/1000 [02:02&lt;00:00,  8.50it/s] 99%|█████████▉| 994/1000 [02:02&lt;00:00,  8.31it/s]100%|█████████▉| 995/1000 [02:02&lt;00:00,  8.39it/s]100%|█████████▉| 996/1000 [02:02&lt;00:00,  8.39it/s]100%|█████████▉| 997/1000 [02:02&lt;00:00,  8.37it/s]100%|█████████▉| 998/1000 [02:02&lt;00:00,  8.47it/s]100%|█████████▉| 999/1000 [02:02&lt;00:00,  8.56it/s]100%|██████████| 1000/1000 [02:02&lt;00:00,  8.59it/s]100%|██████████| 1000/1000 [02:02&lt;00:00,  8.13it/s]\n\n\nH0                                      X1=0\nri-type                      randomization-c\nEstimate                 -0.9929357698186859\nPr(&gt;|t|)                                 0.0\nStd. Error (Pr(&gt;|t|))                    0.0\n2.5% (Pr(&gt;|t|))                          0.0\n97.5% (Pr(&gt;|t|))                         0.0\ndtype: object\n\n\n\n\n\nse\nestimation.feols_.Feols.se()\nFitted model standard errors.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA pd.Series with the standard errors of the estimated regression model.\n\n\n\n\n\n\ntidy\nestimation.feols_.Feols.tidy(alpha=0.05)\nTidy model outputs.\nReturn a tidy pd.DataFrame with the point estimates, standard errors, t-statistics, and p-values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nThe significance level for the confidence intervals. If None, computes a 95% confidence interval (alpha = 0.05).\n0.05\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntidy_df\npd.DataFrame\nA tidy pd.DataFrame containing the regression results, including point estimates, standard errors, t-statistics, and p-values.\n\n\n\n\n\n\nto_array\nestimation.feols_.Feols.to_array()\nConvert estimation data frames to np arrays.\n\n\ntstat\nestimation.feols_.Feols.tstat()\nFitted model t-statistics.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA pd.Series with t-statistics of the estimated regression model.\n\n\n\n\n\n\nupdate\nestimation.feols_.Feols.update(X_new, y_new, inplace=False)\nUpdate coefficients for new observations using Sherman-Morrison formula.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nUpdated coefficients\n\n\n\n\n\n\nvcov\nestimation.feols_.Feols.vcov(vcov, vcov_kwargs=None, data=None)\nCompute covariance matrices for an estimated regression model.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvcov\nUnion[str, dict[str, str]]\nA string or dictionary specifying the type of variance-covariance matrix to use for inference. If a string, it can be one of “iid”, “hetero”, “HC1”, “HC2”, “HC3”, “NW”, “DK”. If a dictionary, it should have the format {“CRV1”: “clustervar”} for CRV1 inference or {“CRV3”: “clustervar”} for CRV3 inference. Note that CRV3 inference is currently not supported for IV estimation.\nrequired\n\n\nvcov_kwargs\nOptional[dict[str, any]]\nAdditional keyword arguments for the variance-covariance matrix.\nNone\n\n\ndata\nOptional[DataFrameType]\nThe data used for estimation. If None, tries to fetch the data from the model object. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFeols\nAn instance of class Feols with updated inference.\n\n\n\n\n\n\nwald_test\nestimation.feols_.Feols.wald_test(R=None, q=None, distribution='F')\nConduct Wald test.\nCompute a Wald test for a linear hypothesis of the form R * β = q. where R is m x k matrix, β is a k x 1 vector of coefficients, and q is m x 1 vector. By default, tests the joint null hypothesis that all coefficients are zero.\nThis method producues the following attriutes\n_dfd : int degree of freedom in denominator _dfn : int degree of freedom in numerator _wald_statistic : scalar Wald-statistics computed for hypothesis testing _f_statistic : scalar Wald-statistics(when R is an indentity matrix, and q being zero vector) computed for hypothesis testing _p_value : scalar corresponding p-value for statistics\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nR\narray - like\nThe matrix R of the linear hypothesis. If None, defaults to an identity matrix.\nNone\n\n\nq\narray - like\nThe vector q of the linear hypothesis. If None, defaults to a vector of zeros.\nNone\n\n\ndistribution\nstr\nThe distribution to use for the p-value. Can be either “F” or “chi2”. Defaults to “F”.\n'F'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA pd.Series with the Wald statistic and p-value.\n\n\n\n\n\nExamples\n\nimport numpy as np\nimport pandas as pd\nimport pyfixest as pf\n\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1 + X2| f1\", data, vcov={\"CRV1\": \"f1\"}, ssc=pf.ssc(k_adj=False))\n\nR = np.array([[1,-1]] )\nq = np.array([0.0])\n\n# Wald test\nfit.wald_test(R=R, q=q, distribution = \"chi2\")\nf_stat = fit._f_statistic\np_stat = fit._p_value\n\nprint(f\"Python f_stat: {f_stat}\")\nprint(f\"Python p_stat: {p_stat}\")\n\nPython f_stat: 126.40650474043487\nPython p_stat: 2.5053092828140965e-29\n\n\n\n\n\nwildboottest\nestimation.feols_.Feols.wildboottest(\n    reps,\n    cluster=None,\n    param=None,\n    weights_type='rademacher',\n    impose_null=True,\n    bootstrap_type='11',\n    seed=None,\n    k_adj=True,\n    G_adj=True,\n    parallel=False,\n    return_bootstrapped_t_stats=False,\n)\nRun a wild cluster bootstrap based on an object of type “Feols”.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreps\nint\nThe number of bootstrap iterations to run.\nrequired\n\n\ncluster\nUnion[str, None]\nThe variable used for clustering. Defaults to None. If None, then uses the variable specified in the model’s clustervar attribute. If no _clustervar attribute is found, runs a heteroskedasticity- robust bootstrap.\nNone\n\n\nparam\nUnion[str, None]\nA string of length one, containing the test parameter of interest. Defaults to None.\nNone\n\n\nweights_type\nstr\nThe type of bootstrap weights. Options are ‘rademacher’, ‘mammen’, ‘webb’, or ‘normal’. Defaults to ‘rademacher’.\n'rademacher'\n\n\nimpose_null\nbool\nIndicates whether to impose the null hypothesis on the bootstrap DGP. Defaults to True.\nTrue\n\n\nbootstrap_type\nstr\nA string of length one to choose the bootstrap type. Options are ‘11’, ‘31’, ‘13’, or ‘33’. Defaults to ‘11’.\n'11'\n\n\nseed\nUnion[int, None]\nAn option to provide a random seed. Defaults to None.\nNone\n\n\nk_adj\nbool\nIndicates whether to apply a small sample adjustment for the number of observations and covariates. Defaults to True.\nTrue\n\n\nG_adj\nbool\nIndicates whether to apply a small sample adjustment for the number of clusters. Defaults to True.\nTrue\n\n\nparallel\nbool\nIndicates whether to run the bootstrap in parallel. Defaults to False.\nFalse\n\n\nseed\nUnion[str, None]\nAn option to provide a random seed. Defaults to None.\nNone\n\n\nreturn_bootstrapped_t_stats\nbool, optional:\nIf True, the method returns a tuple of the regular output and the bootstrapped t-stats. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA DataFrame with the original, non-bootstrapped t-statistic and bootstrapped p-value, along with the bootstrap type, inference type (HC vs CRV), and whether the null hypothesis was imposed on the bootstrap DGP. If return_bootstrapped_t_stats is True, the method returns a tuple of the regular output and the bootstrapped t-stats.\n\n\n\n\n\nExamples\n\nimport re\nimport pyfixest as pf\n\ndata = pf.get_data()\nfit = pf.feols(\"Y ~ X1 + X2 | f1\", data)\n\nfit.wildboottest(\n    param = \"X1\",\n    reps=1000,\n    seed = 822\n)\n\nfit.wildboottest(\n    param = \"X1\",\n    reps=1000,\n    seed = 822,\n    bootstrap_type = \"31\"\n)\n\nparam                    X1\nt value          -14.843741\nPr(&gt;|t|)                0.0\nbootstrap_type           31\ninference                HC\nimpose_null            True\nssc                       1\ndtype: object\n\n\n\n\n\nwls_transform\nestimation.feols_.Feols.wls_transform()\nTransform model matrices for WLS Estimation.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feols_.Feols"
    ]
  },
  {
    "objectID": "reference/estimation.fegaussian_.Fegaussian.html",
    "href": "reference/estimation.fegaussian_.Fegaussian.html",
    "title": "estimation.fegaussian_.Fegaussian",
    "section": "",
    "text": "estimation.fegaussian_.Fegaussian\nestimation.fegaussian_.Fegaussian(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    tol,\n    maxiter,\n    solver,\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    sample_split_var=None,\n    sample_split_value=None,\n    separation_check=None,\n    context=0,\n    demeaner_backend='numba',\n    accelerate=True,\n)\nClass for the estimation of a fixed-effects GLM with normal errors.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.fegaussian_.Fegaussian"
    ]
  },
  {
    "objectID": "reference/report.iplot.html",
    "href": "reference/report.iplot.html",
    "title": "report.iplot",
    "section": "",
    "text": "report.iplot(\n    models,\n    alpha=0.05,\n    figsize=None,\n    yintercept=None,\n    xintercept=None,\n    rotate_xticks=0,\n    title=None,\n    coord_flip=True,\n    keep=None,\n    drop=None,\n    exact_match=False,\n    plot_backend='lets_plot' if _HAS_LETS_PLOT else 'matplotlib',\n    labels=None,\n    cat_template=None,\n    rename_models=None,\n    ax=None,\n    joint=None,\n    seed=None,\n)\nPlot model coefficients for variables interacted via “i()” syntax, with confidence intervals.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.iplot"
    ]
  },
  {
    "objectID": "reference/report.iplot.html#parameters",
    "href": "reference/report.iplot.html#parameters",
    "title": "report.iplot",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nA supported model object (Feols, Fepois, Feiv, FixestMulti) or a list of\nFeols, Fepois & Feiv models.\nrequired\n\n\nfigsize\ntuple or None\nThe size of the figure. If None, the default size is used.\nNone\n\n\nalpha\nfloat\nThe significance level for the confidence intervals.\n0.05\n\n\nyintercept\nint or None\nThe value at which to draw a horizontal line on the plot.\nNone\n\n\nxintercept\nint or None\nThe value at which to draw a vertical line on the plot.\nNone\n\n\nrotate_xticks\nfloat\nThe angle in degrees to rotate the xticks labels. Default is 0 (no rotation).\n0\n\n\ntitle\nstr\nThe title of the plot.\nNone\n\n\ncoord_flip\nbool\nWhether to flip the coordinates of the plot. Default is True.\nTrue\n\n\nkeep\nOptional[Union[list, str]]\nThe pattern for retaining coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Default is keeping all coefficients. You should use regular expressions to select coefficients. “age”, # would keep all coefficients containing age r”^tr”, # would keep all coefficients starting with tr r”\\d$“, # would keep all coefficients ending with number Output will be in the order of the patterns.\nNone\n\n\ndrop\nOptional[Union[list, str]]\nThe pattern for excluding coefficient names. You can pass a string (one pattern) or a list (multiple patterns). Syntax is the same as for keep. Default is keeping all coefficients. Parameter keep and drop can be used simultaneously.\nNone\n\n\nexact_match\nbool\nWhether to use exact match for keep and drop. Default is False. If True, the pattern will be matched exactly to the coefficient name instead of using regular expressions.\nFalse\n\n\nplot_backend\nstr\nThe plotting backend to use. Options are “lets_plot” (default if installed) and “matplotlib”. If “lets_plot” is specified but not installed, an ImportError will be raised with instructions to install it or use “matplotlib” instead.\n'lets_plot' if _HAS_LETS_PLOT else 'matplotlib'\n\n\nrename_models\ndict\nA dictionary to rename the models. The keys are the original model names and the values the new names.\nNone\n\n\nlabels\nOptional[dict]\nA dictionary to relabel the variables. The keys in this dictionary are the original variable names, which correspond to the names stored in the _coefnames attribute of the model. The values in the dictionary are the new names you want to assign to these variables. Note that interaction terms will also be relabeled using the labels of the individual variables. The renaming is applied after the selection of the coefficients via keep and drop.\nNone\n\n\ncat_template\nOptional[str]\nTemplate to relabel categorical variables. None by default, which applies no relabeling. Other options include combinations of “{variable}” and “{value}”, e.g. “{variable}::{value}” to mimic fixest encoding. But “{variable}–{value}” or “{variable}{value}” or just “{value}” are also possible.\nNone\n\n\njoint\nOptional[Union[str, bool]]\nWhether to plot simultaneous confidence bands for the coefficients. If True, simultaneous confidence bands are plotted. If False, “standard” confidence intervals are plotted. If “both”, both are plotted in one figure. Default is None, which returns the standard confidence intervals. Note that this option is not available for objects of type FixestMulti, i.e. multiple estimation.\nNone\n\n\nseed\nOptional[int]\nThe seed for the random number generator. Default is None. Only required / used when joint is True.\nNone",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.iplot"
    ]
  },
  {
    "objectID": "reference/report.iplot.html#returns",
    "href": "reference/report.iplot.html#returns",
    "title": "report.iplot",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nA plot figure from the specified backend.",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.iplot"
    ]
  },
  {
    "objectID": "reference/report.iplot.html#examples",
    "href": "reference/report.iplot.html#examples",
    "title": "report.iplot",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\nfrom pyfixest.report.utils import rename_categoricals\n\ndf = pf.get_data()\nfit1 = pf.feols(\"Y ~ i(f1)\", data = df)\nfit2 = pf.feols(\"Y ~ i(f1) + X2\", data = df)\nfit3 = pf.feols(\"Y ~ i(f1) + X2 | f2\", data = df)\n\npf.iplot([fit1, fit2, fit3], labels = rename_categoricals(fit1._coefnames))\npf.iplot(\n    models = [fit1, fit2, fit3],\n    labels = rename_categoricals(fit1._coefnames)\n)\npf.iplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        fit1._model_name_plot: \"Model 1\",\n        fit2._model_name_plot: \"Model 2\",\n        fit3._model_name_plot: \"Model 3\"\n    },\n)\npf.iplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        \"Y~i(f1)\": \"Model 1\",\n        \"Y~i(f1)+X2\": \"Model 2\",\n        \"Y~i(f1)+X2|f2\": \"Model 3\"\n    },\n)\npf.iplot([fit1], joint = \"both\")\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/tmp/ipykernel_6660/3932325117.py:9: DeprecationWarning: The function `_relabel_expvar` is deprecated as we have adjusted the naming of variables interacted via the i() operator with pyfixest 0.50. For regression tables, please rely on the `labels` and `cat_template` arguments of `pf.etable()` instead. \n  pf.iplot([fit1, fit2, fit3], labels = rename_categoricals(fit1._coefnames))\n/home/runner/work/pyfixest/pyfixest/pyfixest/report/utils.py:20: UserWarning: The label key 'Intercept' is not in the covariate names.\n  warnings.warn(f\"The label key '{label_key}' is not in the covariate names.\")\n/tmp/ipykernel_6660/3932325117.py:12: DeprecationWarning: The function `_relabel_expvar` is deprecated as we have adjusted the naming of variables interacted via the i() operator with pyfixest 0.50. For regression tables, please rely on the `labels` and `cat_template` arguments of `pf.etable()` instead. \n  labels = rename_categoricals(fit1._coefnames)\n/home/runner/work/pyfixest/pyfixest/pyfixest/report/utils.py:20: UserWarning: The label key 'Intercept' is not in the covariate names.\n  warnings.warn(f\"The label key '{label_key}' is not in the covariate names.\")",
    "crumbs": [
      "Function Reference",
      "Summarize and Visualize",
      "report.iplot"
    ]
  },
  {
    "objectID": "reference/estimation.rwolf.html",
    "href": "reference/estimation.rwolf.html",
    "title": "estimation.rwolf",
    "section": "",
    "text": "estimation.rwolf(models, param, reps, seed, sampling_method='wild-bootstrap')\nCompute Romano-Wolf adjusted p-values for multiple hypothesis testing.\nFor each model, it is assumed that tests to adjust are of the form “param = 0”. This function uses the wildboottest() method for running the bootstrap, hence models of type Feiv or Fepois are not supported.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.rwolf"
    ]
  },
  {
    "objectID": "reference/estimation.rwolf.html#parameters",
    "href": "reference/estimation.rwolf.html#parameters",
    "title": "estimation.rwolf",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist[Feols] or FixestMulti\nA list of models for which the p-values should be computed, or a FixestMulti object. Models of type Feiv or Fepois are not supported.\nrequired\n\n\nparam\nstr\nThe parameter for which the p-values should be computed.\nrequired\n\n\nreps\nint\nThe number of bootstrap replications.\nrequired\n\n\nseed\nint\nThe seed for the random number generator.\nrequired\n\n\nsampling_method\nstr\nSampling method for computing resampled statistics. Users can choose either bootstrap(‘wild-bootstrap’) or randomization inference(‘ri’)\n'wild-bootstrap'",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.rwolf"
    ]
  },
  {
    "objectID": "reference/estimation.rwolf.html#returns",
    "href": "reference/estimation.rwolf.html#returns",
    "title": "estimation.rwolf",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nA DataFrame containing estimation statistics, including the Romano-Wolf adjusted p-values.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.rwolf"
    ]
  },
  {
    "objectID": "reference/estimation.rwolf.html#examples",
    "href": "reference/estimation.rwolf.html#examples",
    "title": "estimation.rwolf",
    "section": "Examples",
    "text": "Examples\n\nimport pyfixest as pf\nfrom pyfixest.utils import get_data\n\ndata = get_data().dropna()\nfit = pf.feols(\"Y ~ Y2 + X1 + X2\", data=data)\npf.rwolf(fit, \"X1\", reps=9999, seed=123)\n\nfit1 = pf.feols(\"Y ~ X1\", data=data)\nfit2 = pf.feols(\"Y ~ X1 + X2\", data=data)\nrwolf_df = pf.rwolf([fit1, fit2], \"X1\", reps=9999, seed=123)\n\n# use randomization inference - dontrun as too slow\n# rwolf_df = pf.rwolf([fit1, fit2], \"X1\", reps=9999, seed=123, sampling_method = \"ri\")\n\nrwolf_df\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\nest0\nest1\n\n\n\n\nEstimate\n-1.001930\n-0.995197\n\n\nStd. Error\n0.084823\n0.082194\n\n\nt value\n-11.811964\n-12.107957\n\n\nPr(&gt;|t|)\n0.000000\n0.000000\n\n\n2.5%\n-1.168383\n-1.156490\n\n\n97.5%\n-0.835476\n-0.833904\n\n\nRW Pr(&gt;|t|)\n0.000100\n0.000100",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "estimation.rwolf"
    ]
  },
  {
    "objectID": "reference/did.estimation.event_study.html",
    "href": "reference/did.estimation.event_study.html",
    "title": "did.estimation.event_study",
    "section": "",
    "text": "did.estimation.event_study(\n    data,\n    yname,\n    idname,\n    tname,\n    gname,\n    xfml=None,\n    cluster=None,\n    estimator='twfe',\n    att=True,\n)\nEstimate Event Study Model.\nThis function allows for the estimation of treatment effects using different estimators. Currently, it supports “twfe” for the two-way fixed effects estimator and “did2s” for Gardner’s two-step DID2S estimator. Other estimators are in development.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.event_study"
    ]
  },
  {
    "objectID": "reference/did.estimation.event_study.html#parameters",
    "href": "reference/did.estimation.event_study.html#parameters",
    "title": "did.estimation.event_study",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nDataFrame\nThe DataFrame containing all variables.\nrequired\n\n\nyname\nstr\nThe name of the dependent variable.\nrequired\n\n\nidname\nstr\nThe name of the id variable.\nrequired\n\n\ntname\nstr\nVariable name for calendar period.\nrequired\n\n\ngname\nstr\nUnit-specific time of initial treatment.\nrequired\n\n\ncluster\nOptional[str]\nThe name of the cluster variable. If None, defaults to idname.\nNone\n\n\nxfml\nstr\nThe formula for the covariates.\nNone\n\n\nestimator\nstr\nThe estimator to use. Options are “did2s”, “twfe”, and “saturated”.\n'twfe'\n\n\natt\nbool\nIf True, estimates the average treatment effect on the treated (ATT). If False, estimates the canonical event study design with all leads and lags. Default is True.\nTrue",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.event_study"
    ]
  },
  {
    "objectID": "reference/did.estimation.event_study.html#returns",
    "href": "reference/did.estimation.event_study.html#returns",
    "title": "did.estimation.event_study",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nA fitted model object of class Feols.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.event_study"
    ]
  },
  {
    "objectID": "reference/did.estimation.event_study.html#examples",
    "href": "reference/did.estimation.event_study.html#examples",
    "title": "did.estimation.event_study",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pyfixest as pf\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\n\nfit_twfe = pf.event_study(\n    df_het,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"twfe\",\n    att=True,\n)\n\nfit_twfe.tidy()\n\n# run saturated event study\nfit_twfe_saturated = pf.event_study(\n    df_het,\n    yname=\"dep_var\",\n    idname=\"unit\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"saturated\",\n)\n\nfit_twfe_saturated.aggregate()\nfit_twfe_saturated.iplot_aggregate()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/did/saturated_twfe.py:68: UserWarning: The SaturatedEventStudyClass is currently in beta. Please report any issues you may encounter.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:2761: UserWarning: \n            22 variables dropped due to multicollinearity.\n            The following variables are dropped: \n    C(rel_time, contr.treatment(base=-1.0))[-20.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-19.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-18.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-17.0]:cohort_dummy_2000\n    C(rel_time, contr.treatment(base=-1.0))[-16.0]:cohort_dummy_2000\n    ....\n            \n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/did/saturated_twfe.py:271: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.event_study"
    ]
  },
  {
    "objectID": "reference/did.estimation.did2s.html",
    "href": "reference/did.estimation.did2s.html",
    "title": "did.estimation.did2s",
    "section": "",
    "text": "did.estimation.did2s(\n    data,\n    yname,\n    first_stage,\n    second_stage,\n    treatment,\n    cluster,\n    weights=None,\n)\nEstimate a Difference-in-Differences model using Gardner’s two-step DID2S estimator.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.did2s"
    ]
  },
  {
    "objectID": "reference/did.estimation.did2s.html#parameters",
    "href": "reference/did.estimation.did2s.html#parameters",
    "title": "did.estimation.did2s",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe DataFrame containing all variables.\nrequired\n\n\nyname\nstr\nThe name of the dependent variable.\nrequired\n\n\nfirst_stage\nstr\nThe formula for the first stage, starting with ‘~’.\nrequired\n\n\nsecond_stage\nstr\nThe formula for the second stage, starting with ‘~’.\nrequired\n\n\ntreatment\nstr\nThe name of the treatment variable.\nrequired\n\n\ncluster\nstr\nThe name of the cluster variable.\nrequired",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.did2s"
    ]
  },
  {
    "objectID": "reference/did.estimation.did2s.html#returns",
    "href": "reference/did.estimation.did2s.html#returns",
    "title": "did.estimation.did2s",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nobject\nA fitted model object of class Feols.",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.did2s"
    ]
  },
  {
    "objectID": "reference/did.estimation.did2s.html#examples",
    "href": "reference/did.estimation.did2s.html#examples",
    "title": "did.estimation.did2s",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport numpy as np\nimport pyfixest as pf\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\ndf_het.head()\n\n\n\n\n\n\n\n\nunit\nstate\ngroup\nunit_fe\ng\nyear\nyear_fe\ntreat\nrel_year\nrel_year_binned\nerror\nte\nte_dynamic\ndep_var\n\n\n\n\n0\n1\n33\nGroup 2\n7.043016\n2010\n1990\n0.066159\nFalse\n-20.0\n-6\n-0.086466\n0\n0.0\n7.022709\n\n\n1\n1\n33\nGroup 2\n7.043016\n2010\n1991\n-0.030980\nFalse\n-19.0\n-6\n0.766593\n0\n0.0\n7.778628\n\n\n2\n1\n33\nGroup 2\n7.043016\n2010\n1992\n-0.119607\nFalse\n-18.0\n-6\n1.512968\n0\n0.0\n8.436377\n\n\n3\n1\n33\nGroup 2\n7.043016\n2010\n1993\n0.126321\nFalse\n-17.0\n-6\n0.021870\n0\n0.0\n7.191207\n\n\n4\n1\n33\nGroup 2\n7.043016\n2010\n1994\n-0.106921\nFalse\n-16.0\n-6\n-0.017603\n0\n0.0\n6.918492\n\n\n\n\n\n\n\nIn a first step, we estimate a classical event study model:\n\n# estimate the model\nfit = pf.did2s(\n    df_het,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | unit + year\",\n    second_stage=\"~i(rel_year, ref=-1.0)\",\n    treatment=\"treat\",\n    cluster=\"state\",\n)\n\nfit.tidy().head()\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nC(rel_year, contr.treatment(base=-1.0))[T.-inf]\n-3.551930e-08\n5.844125e-09\n-6.077778\n1.228051e-09\n-4.697387e-08\n-2.406472e-08\n\n\nC(rel_year, contr.treatment(base=-1.0))[T.-20.0]\n-5.822583e-02\n3.580900e-02\n-1.626011\n1.039541e-01\n-1.284120e-01\n1.196034e-02\n\n\nC(rel_year, contr.treatment(base=-1.0))[T.-19.0]\n-6.032212e-03\n3.034072e-02\n-0.198816\n8.424078e-01\n-6.550049e-02\n5.343606e-02\n\n\nC(rel_year, contr.treatment(base=-1.0))[T.-18.0]\n-6.152375e-03\n3.509400e-02\n-0.175311\n8.608358e-01\n-7.493715e-02\n6.263239e-02\n\n\nC(rel_year, contr.treatment(base=-1.0))[T.-17.0]\n-1.253327e-02\n2.483369e-02\n-0.504688\n6.137802e-01\n-6.120767e-02\n3.614113e-02\n\n\n\n\n\n\n\nWe can also inspect the model visually:\n\nfit.iplot(figsize= [1200, 400], coord_flip=False).show()\n\n   \n   \n\n\nTo estimate a pooled effect, we need to slightly update the second stage formula:\n\nfit = pf.did2s(\n    df_het,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | unit + year\",\n    second_stage=\"~i(treat)\",\n    treatment=\"treat\",\n    cluster=\"state\"\n)\nfit.tidy().head()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nC(treat)[T.True]\n2.230482\n0.024709\n90.271444\n0.0\n2.182052\n2.278911",
    "crumbs": [
      "Function Reference",
      "Estimation Functions",
      "did.estimation.did2s"
    ]
  },
  {
    "objectID": "reference/estimation.feprobit_.Feprobit.html",
    "href": "reference/estimation.feprobit_.Feprobit.html",
    "title": "estimation.feprobit_.Feprobit",
    "section": "",
    "text": "estimation.feprobit_.Feprobit\nestimation.feprobit_.Feprobit(\n    FixestFormula,\n    data,\n    ssc_dict,\n    drop_singletons,\n    drop_intercept,\n    weights,\n    weights_type,\n    collin_tol,\n    fixef_tol,\n    fixef_maxiter,\n    lookup_demeaned_data,\n    tol,\n    maxiter,\n    solver,\n    demeaner_backend='numba',\n    store_data=True,\n    copy_data=True,\n    lean=False,\n    sample_split_var=None,\n    sample_split_value=None,\n    separation_check=None,\n    context=0,\n    accelerate=True,\n)\nClass for the estimation of a fixed-effects probit model.",
    "crumbs": [
      "Function Reference",
      "Estimation Classes",
      "estimation.feprobit_.Feprobit"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "A fixed effect model is a statistical model that includes fixed effects, which are parameters that are estimated to be constant across different groups.\nExample [Panel Data]: In the context of panel data, fixed effects are parameters that are constant across different individuals or time. The typical model example is given by the following equation:\n\\[\nY_{it} = \\beta X_{it} + \\alpha_i + \\psi_t + \\varepsilon_{it}\n\\]\nwhere \\(Y_{it}\\) is the dependent variable for individual \\(i\\) at time \\(t\\), \\(X_{it}\\) is the independent variable, \\(\\beta\\) is the coefficient of the independent variable, \\(\\alpha_i\\) is the individual fixed effect, \\(\\psi_t\\) is the time fixed effect, and \\(\\varepsilon_{it}\\) is the error term. The individual fixed effect \\(\\alpha_i\\) is a parameter that is constant across time for each individual, while the time fixed effect \\(\\psi_t\\) is a parameter that is constant across individuals for each time period.\nNote however that, despite the fact that fixed effects are commonly used in panel setting, one does not need a panel data set to work with fixed effects. For example, cluster randomized trials with cluster fixed effects, or wage regressions with worker and firm fixed effects.\nIn this “quick start” guide, we will show you how to estimate a fixed effect model using the PyFixest package. We do not go into the details of the theory behind fixed effect models, but we focus on how to estimate them using PyFixest.\n\n\n\nIn a first step, we load the module and some synthetic example data:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ntry:\n    from lets_plot import LetsPlot\n    _HAS_LETS_PLOT = True\nexcept ImportError:\n    _HAS_LETS_PLOT = False\n\nfrom marginaleffects import slopes, avg_slopes\n\nimport pyfixest as pf\n\nif _HAS_LETS_PLOT:\n    LetsPlot.setup_html()\n\nplt.style.use(\"seaborn-v0_8\")\n\n%load_ext watermark\n%config InlineBackend.figure_format = \"retina\"\n%watermark --iversions\n\ndata = pf.get_data()\n\ndata.head()\n\n\n            \n            \n            \n\n\nnumpy     : 2.2.6\npyfixest  : 0.40.1\npandas    : 2.3.3\nmatplotlib: 3.10.7\n\n\n\n\n\n\n  \n    \n      \n      Y\n      Y2\n      X1\n      X2\n      f1\n      f2\n      f3\n      group_id\n      Z1\n      Z2\n      weights\n    \n  \n  \n    \n      0\n      NaN\n      2.357103\n      0.0\n      0.457858\n      15.0\n      0.0\n      7.0\n      9.0\n      -0.330607\n      1.054826\n      0.661478\n    \n    \n      1\n      -1.458643\n      5.163147\n      NaN\n      -4.998406\n      6.0\n      21.0\n      4.0\n      8.0\n      NaN\n      -4.113690\n      0.772732\n    \n    \n      2\n      0.169132\n      0.751140\n      2.0\n      1.558480\n      NaN\n      1.0\n      7.0\n      16.0\n      1.207778\n      0.465282\n      0.990929\n    \n    \n      3\n      3.319513\n      -2.656368\n      1.0\n      1.560402\n      1.0\n      10.0\n      11.0\n      3.0\n      2.869997\n      0.467570\n      0.021123\n    \n    \n      4\n      0.134420\n      -1.866416\n      2.0\n      -3.472232\n      19.0\n      20.0\n      6.0\n      14.0\n      0.835819\n      -3.115669\n      0.790815\n    \n  \n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Y         999 non-null    float64\n 1   Y2        1000 non-null   float64\n 2   X1        999 non-null    float64\n 3   X2        1000 non-null   float64\n 4   f1        999 non-null    float64\n 5   f2        1000 non-null   float64\n 6   f3        1000 non-null   float64\n 7   group_id  1000 non-null   float64\n 8   Z1        999 non-null    float64\n 9   Z2        1000 non-null   float64\n 10  weights   1000 non-null   float64\ndtypes: float64(11)\nmemory usage: 86.1 KB\n\n\nWe see that some of our columns have missing data.\n\n\n\nWe are interested in the relation between the dependent variable Y and the independent variables X1 using a fixed effect model for group_id. Let’s see how the data looks like:\n\nax = data.plot(kind=\"scatter\", x=\"X1\", y=\"Y\", c=\"group_id\", colormap=\"viridis\")\n\n\n\n\n\n\n\n\nWe can estimate a fixed effects regression via the feols() function. feols() has three arguments: a two-sided model formula, the data, and optionally, the type of inference.\n\nfit = pf.feols(fml=\"Y ~ X1 | group_id\", data=data, vcov=\"HC1\")\ntype(fit)\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\npyfixest.estimation.feols_.Feols\n\n\nThe first part of the formula contains the dependent variable and “regular” covariates, while the second part contains fixed effects.\nfeols() returns an instance of the Fixest class.\n\n\n\nTo inspect the results, we can use a summary function or method:\n\nfit.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: group_id\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n---\nRMSE: 2.141 R2: 0.137 R2 Within: 0.126 \n\n\nOr display a formatted regression table:\n\npf.etable(fit)\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -1.019***  (0.083)\n  \n  \n    fe\n  \n  \n    group_id\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    998\n  \n  \n    R2\n    0.137\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nAlternatively, the .summarize module contains a summary function, which can be applied on instances of regression model objects or lists of regression model objects. For details on how to customize etable(), please take a look at the dedicated vignette.\n\npf.summary(fit)\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: group_id\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n---\nRMSE: 2.141 R2: 0.137 R2 Within: 0.126 \n\n\nYou can access individual elements of the summary via dedicated methods: .tidy() returns a “tidy” pd.DataFrame, .coef() returns estimated parameters, and se() estimated standard errors. Other methods include pvalue(), confint() and tstat().\n\nfit.tidy()\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      X1\n      -1.019009\n      0.083296\n      -12.233634\n      0.0\n      -1.182467\n      -0.85555\n    \n  \n\n\n\n\n\nfit.coef()\n\nCoefficient\nX1   -1.019009\nName: Estimate, dtype: float64\n\n\n\nfit.se()\n\nCoefficient\nX1    0.083296\nName: Std. Error, dtype: float64\n\n\n\nfit.tstat()\n\nCoefficient\nX1   -12.233634\nName: t value, dtype: float64\n\n\n\nfit.confint()\n\n\n\n\n  \n    \n      \n      2.5%\n      97.5%\n    \n  \n  \n    \n      X1\n      -1.182467\n      -0.85555\n    \n  \n\n\n\n\nLast, model results can be visualized via dedicated methods for plotting:\n\nfit.coefplot()\n# or pf.coefplot([fit])\n\n   \n   \n\n\n\n\n\nLet’s have a quick d-tour on the intuition behind fixed effects models using the example above. To do so, let us begin by comparing it with a simple OLS model.\n\nfit_simple = pf.feols(\"Y ~ X1\", data=data, vcov=\"HC1\")\n\nfit_simple.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.919 |        0.112 |     8.223 |      0.000 |  0.699 |   1.138 |\n| X1            |     -1.000 |        0.082 |   -12.134 |      0.000 | -1.162 |  -0.838 |\n---\nRMSE: 2.158 R2: 0.123 \n\n\nWe can compare both models side by side in a regression table:\n\npf.etable([fit, fit_simple])\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n  (2)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -1.019***  (0.083)\n    -1.000***  (0.082)\n  \n  \n    Intercept\n    \n    0.919***  (0.112)\n  \n  \n    fe\n  \n  \n    group_id\n    x\n    -\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n  \n  \n    R2\n    0.137\n    0.123\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nWe see that the X1 coefficient is -1.019, which is less than the value from the OLS model in column (2). Where is the difference coming from? Well, in the fixed effect model we are interested in controlling for the feature group_id. One possibility to do this is by adding a simple dummy variable for each level of group_id.\n\nfit_dummy = pf.feols(\"Y ~ X1 + C(group_id) \", data=data, vcov=\"HC1\")\n\nfit_dummy.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient         |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept           |      0.760 |        0.288 |     2.640 |      0.008 |  0.195 |   1.326 |\n| X1                  |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n| C(group_id)[T.1.0]  |      0.380 |        0.451 |     0.844 |      0.399 | -0.504 |   1.264 |\n| C(group_id)[T.2.0]  |      0.084 |        0.389 |     0.216 |      0.829 | -0.680 |   0.848 |\n| C(group_id)[T.3.0]  |      0.790 |        0.415 |     1.904 |      0.057 | -0.024 |   1.604 |\n| C(group_id)[T.4.0]  |     -0.189 |        0.388 |    -0.487 |      0.626 | -0.950 |   0.572 |\n| C(group_id)[T.5.0]  |      0.537 |        0.388 |     1.385 |      0.166 | -0.224 |   1.297 |\n| C(group_id)[T.6.0]  |      0.307 |        0.398 |     0.771 |      0.441 | -0.474 |   1.087 |\n| C(group_id)[T.7.0]  |      0.015 |        0.422 |     0.035 |      0.972 | -0.814 |   0.844 |\n| C(group_id)[T.8.0]  |      0.382 |        0.406 |     0.941 |      0.347 | -0.415 |   1.179 |\n| C(group_id)[T.9.0]  |      0.219 |        0.417 |     0.526 |      0.599 | -0.599 |   1.037 |\n| C(group_id)[T.10.0] |     -0.363 |        0.422 |    -0.861 |      0.390 | -1.191 |   0.465 |\n| C(group_id)[T.11.0] |      0.201 |        0.387 |     0.520 |      0.603 | -0.559 |   0.961 |\n| C(group_id)[T.12.0] |     -0.110 |        0.410 |    -0.268 |      0.788 | -0.915 |   0.694 |\n| C(group_id)[T.13.0] |      0.126 |        0.440 |     0.287 |      0.774 | -0.736 |   0.989 |\n| C(group_id)[T.14.0] |      0.353 |        0.416 |     0.848 |      0.397 | -0.464 |   1.170 |\n| C(group_id)[T.15.0] |      0.469 |        0.398 |     1.179 |      0.239 | -0.312 |   1.249 |\n| C(group_id)[T.16.0] |     -0.135 |        0.396 |    -0.340 |      0.734 | -0.913 |   0.643 |\n| C(group_id)[T.17.0] |     -0.005 |        0.401 |    -0.013 |      0.989 | -0.792 |   0.781 |\n| C(group_id)[T.18.0] |      0.283 |        0.403 |     0.702 |      0.483 | -0.508 |   1.074 |\n---\nRMSE: 2.141 R2: 0.137 \n\n\nThis is does not scale well! Imagine you have 1000 different levels of group_id. You would need to add 1000 dummy variables to your model. This is where fixed effect models come in handy. They allow you to control for these fixed effects without adding all these dummy variables. The way to do it is by a demeaning procedure. The idea is to subtract the average value of each level of group_id from the respective observations. This way, we control for the fixed effects without adding all these dummy variables. Let’s try to do this manually:\n\ndef _demean_column(df: pd.DataFrame, column: str, by: str) -&gt; pd.Series:\n    return df[column] - df.groupby(by)[column].transform(\"mean\")\n\n\nfit_demeaned = pf.feols(\n    fml=\"Y_demeaned ~ X1_demeaned\",\n    data=data.assign(\n        Y_demeaned=lambda df: _demean_column(df, \"Y\", \"group_id\"),\n        X1_demeaned=lambda df: _demean_column(df, \"X1\", \"group_id\"),\n    ),\n    vcov=\"HC1\",\n)\n\nfit_demeaned.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y_demeaned, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.003 |        0.068 |     0.041 |      0.968 | -0.130 |   0.136 |\n| X1_demeaned   |     -1.019 |        0.083 |   -12.345 |      0.000 | -1.181 |  -0.857 |\n---\nRMSE: 2.141 R2: 0.126 \n\n\nWe get the same results as the fixed effect model Y1 ~ X | group_id above. The PyFixest package uses a more efficient algorithm to estimate the fixed effect model, but the intuition is the same.\n\n\n\nYou can update the coefficients of a model object via the update() method, which may be useful in an online learning setting where data arrives sequentially.\nTo see this in action, let us first fit a model on a subset of the data:\n\ndata_subsample = data.sample(frac=0.5)\nm = pf.feols(\"Y ~ X1 + X2\", data=data_subsample)\n# current coefficient vector\nm._beta_hat\n\narray([ 0.99914168, -1.05983271, -0.17775322])\n\n\nThen sample 5 new observations and update the model with the new data. The update rule is\n\\[\n\\hat{\\beta}_{n+1} = \\hat{\\beta}_n + (X_{n+1}' X_{n+1})^{-1} x_{n+1} + (y_{n+1} - x_{n+1} \\hat{\\beta}_n)\n\\]\nfor a new observation \\((x_{n+1}, y_{n+1})\\).\n\nnew_points_id = np.random.choice(list(set(data.index) - set(data_subsample.index)), 5)\nX_new, y_new = (\n    np.c_[np.ones(len(new_points_id)), data.loc[new_points_id][[\"X1\", \"X2\"]].values],\n    data.loc[new_points_id][\"Y\"].values,\n)\nm.update(X_new, y_new)\n\narray([ 0.98247951, -1.04782148, -0.17523909])\n\n\nWe verify that we get the same results if we had estimated the model on the appended data.\n\npf.feols(\n    \"Y ~ X1 + X2\", data=data.loc[data_subsample.index.append(pd.Index(new_points_id))]\n).coef().values\n\narray([ 0.98247951, -1.04782148, -0.17523909])"
  },
  {
    "objectID": "quickstart.html#what-is-a-fixed-effect-model",
    "href": "quickstart.html#what-is-a-fixed-effect-model",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "A fixed effect model is a statistical model that includes fixed effects, which are parameters that are estimated to be constant across different groups.\nExample [Panel Data]: In the context of panel data, fixed effects are parameters that are constant across different individuals or time. The typical model example is given by the following equation:\n\\[\nY_{it} = \\beta X_{it} + \\alpha_i + \\psi_t + \\varepsilon_{it}\n\\]\nwhere \\(Y_{it}\\) is the dependent variable for individual \\(i\\) at time \\(t\\), \\(X_{it}\\) is the independent variable, \\(\\beta\\) is the coefficient of the independent variable, \\(\\alpha_i\\) is the individual fixed effect, \\(\\psi_t\\) is the time fixed effect, and \\(\\varepsilon_{it}\\) is the error term. The individual fixed effect \\(\\alpha_i\\) is a parameter that is constant across time for each individual, while the time fixed effect \\(\\psi_t\\) is a parameter that is constant across individuals for each time period.\nNote however that, despite the fact that fixed effects are commonly used in panel setting, one does not need a panel data set to work with fixed effects. For example, cluster randomized trials with cluster fixed effects, or wage regressions with worker and firm fixed effects.\nIn this “quick start” guide, we will show you how to estimate a fixed effect model using the PyFixest package. We do not go into the details of the theory behind fixed effect models, but we focus on how to estimate them using PyFixest."
  },
  {
    "objectID": "quickstart.html#read-sample-data",
    "href": "quickstart.html#read-sample-data",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "In a first step, we load the module and some synthetic example data:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ntry:\n    from lets_plot import LetsPlot\n    _HAS_LETS_PLOT = True\nexcept ImportError:\n    _HAS_LETS_PLOT = False\n\nfrom marginaleffects import slopes, avg_slopes\n\nimport pyfixest as pf\n\nif _HAS_LETS_PLOT:\n    LetsPlot.setup_html()\n\nplt.style.use(\"seaborn-v0_8\")\n\n%load_ext watermark\n%config InlineBackend.figure_format = \"retina\"\n%watermark --iversions\n\ndata = pf.get_data()\n\ndata.head()\n\n\n            \n            \n            \n\n\nnumpy     : 2.2.6\npyfixest  : 0.40.1\npandas    : 2.3.3\nmatplotlib: 3.10.7\n\n\n\n\n\n\n  \n    \n      \n      Y\n      Y2\n      X1\n      X2\n      f1\n      f2\n      f3\n      group_id\n      Z1\n      Z2\n      weights\n    \n  \n  \n    \n      0\n      NaN\n      2.357103\n      0.0\n      0.457858\n      15.0\n      0.0\n      7.0\n      9.0\n      -0.330607\n      1.054826\n      0.661478\n    \n    \n      1\n      -1.458643\n      5.163147\n      NaN\n      -4.998406\n      6.0\n      21.0\n      4.0\n      8.0\n      NaN\n      -4.113690\n      0.772732\n    \n    \n      2\n      0.169132\n      0.751140\n      2.0\n      1.558480\n      NaN\n      1.0\n      7.0\n      16.0\n      1.207778\n      0.465282\n      0.990929\n    \n    \n      3\n      3.319513\n      -2.656368\n      1.0\n      1.560402\n      1.0\n      10.0\n      11.0\n      3.0\n      2.869997\n      0.467570\n      0.021123\n    \n    \n      4\n      0.134420\n      -1.866416\n      2.0\n      -3.472232\n      19.0\n      20.0\n      6.0\n      14.0\n      0.835819\n      -3.115669\n      0.790815\n    \n  \n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Y         999 non-null    float64\n 1   Y2        1000 non-null   float64\n 2   X1        999 non-null    float64\n 3   X2        1000 non-null   float64\n 4   f1        999 non-null    float64\n 5   f2        1000 non-null   float64\n 6   f3        1000 non-null   float64\n 7   group_id  1000 non-null   float64\n 8   Z1        999 non-null    float64\n 9   Z2        1000 non-null   float64\n 10  weights   1000 non-null   float64\ndtypes: float64(11)\nmemory usage: 86.1 KB\n\n\nWe see that some of our columns have missing data."
  },
  {
    "objectID": "quickstart.html#ols-estimation",
    "href": "quickstart.html#ols-estimation",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "We are interested in the relation between the dependent variable Y and the independent variables X1 using a fixed effect model for group_id. Let’s see how the data looks like:\n\nax = data.plot(kind=\"scatter\", x=\"X1\", y=\"Y\", c=\"group_id\", colormap=\"viridis\")\n\n\n\n\n\n\n\n\nWe can estimate a fixed effects regression via the feols() function. feols() has three arguments: a two-sided model formula, the data, and optionally, the type of inference.\n\nfit = pf.feols(fml=\"Y ~ X1 | group_id\", data=data, vcov=\"HC1\")\ntype(fit)\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\npyfixest.estimation.feols_.Feols\n\n\nThe first part of the formula contains the dependent variable and “regular” covariates, while the second part contains fixed effects.\nfeols() returns an instance of the Fixest class."
  },
  {
    "objectID": "quickstart.html#inspecting-model-results",
    "href": "quickstart.html#inspecting-model-results",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "To inspect the results, we can use a summary function or method:\n\nfit.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: group_id\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n---\nRMSE: 2.141 R2: 0.137 R2 Within: 0.126 \n\n\nOr display a formatted regression table:\n\npf.etable(fit)\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -1.019***  (0.083)\n  \n  \n    fe\n  \n  \n    group_id\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    998\n  \n  \n    R2\n    0.137\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nAlternatively, the .summarize module contains a summary function, which can be applied on instances of regression model objects or lists of regression model objects. For details on how to customize etable(), please take a look at the dedicated vignette.\n\npf.summary(fit)\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: group_id\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n---\nRMSE: 2.141 R2: 0.137 R2 Within: 0.126 \n\n\nYou can access individual elements of the summary via dedicated methods: .tidy() returns a “tidy” pd.DataFrame, .coef() returns estimated parameters, and se() estimated standard errors. Other methods include pvalue(), confint() and tstat().\n\nfit.tidy()\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      X1\n      -1.019009\n      0.083296\n      -12.233634\n      0.0\n      -1.182467\n      -0.85555\n    \n  \n\n\n\n\n\nfit.coef()\n\nCoefficient\nX1   -1.019009\nName: Estimate, dtype: float64\n\n\n\nfit.se()\n\nCoefficient\nX1    0.083296\nName: Std. Error, dtype: float64\n\n\n\nfit.tstat()\n\nCoefficient\nX1   -12.233634\nName: t value, dtype: float64\n\n\n\nfit.confint()\n\n\n\n\n  \n    \n      \n      2.5%\n      97.5%\n    \n  \n  \n    \n      X1\n      -1.182467\n      -0.85555\n    \n  \n\n\n\n\nLast, model results can be visualized via dedicated methods for plotting:\n\nfit.coefplot()\n# or pf.coefplot([fit])"
  },
  {
    "objectID": "quickstart.html#how-to-interpret-the-results",
    "href": "quickstart.html#how-to-interpret-the-results",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "Let’s have a quick d-tour on the intuition behind fixed effects models using the example above. To do so, let us begin by comparing it with a simple OLS model.\n\nfit_simple = pf.feols(\"Y ~ X1\", data=data, vcov=\"HC1\")\n\nfit_simple.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.919 |        0.112 |     8.223 |      0.000 |  0.699 |   1.138 |\n| X1            |     -1.000 |        0.082 |   -12.134 |      0.000 | -1.162 |  -0.838 |\n---\nRMSE: 2.158 R2: 0.123 \n\n\nWe can compare both models side by side in a regression table:\n\npf.etable([fit, fit_simple])\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n\n\n  (1)\n  (2)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -1.019***  (0.083)\n    -1.000***  (0.082)\n  \n  \n    Intercept\n    \n    0.919***  (0.112)\n  \n  \n    fe\n  \n  \n    group_id\n    x\n    -\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n  \n  \n    R2\n    0.137\n    0.123\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nWe see that the X1 coefficient is -1.019, which is less than the value from the OLS model in column (2). Where is the difference coming from? Well, in the fixed effect model we are interested in controlling for the feature group_id. One possibility to do this is by adding a simple dummy variable for each level of group_id.\n\nfit_dummy = pf.feols(\"Y ~ X1 + C(group_id) \", data=data, vcov=\"HC1\")\n\nfit_dummy.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient         |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept           |      0.760 |        0.288 |     2.640 |      0.008 |  0.195 |   1.326 |\n| X1                  |     -1.019 |        0.083 |   -12.234 |      0.000 | -1.182 |  -0.856 |\n| C(group_id)[T.1.0]  |      0.380 |        0.451 |     0.844 |      0.399 | -0.504 |   1.264 |\n| C(group_id)[T.2.0]  |      0.084 |        0.389 |     0.216 |      0.829 | -0.680 |   0.848 |\n| C(group_id)[T.3.0]  |      0.790 |        0.415 |     1.904 |      0.057 | -0.024 |   1.604 |\n| C(group_id)[T.4.0]  |     -0.189 |        0.388 |    -0.487 |      0.626 | -0.950 |   0.572 |\n| C(group_id)[T.5.0]  |      0.537 |        0.388 |     1.385 |      0.166 | -0.224 |   1.297 |\n| C(group_id)[T.6.0]  |      0.307 |        0.398 |     0.771 |      0.441 | -0.474 |   1.087 |\n| C(group_id)[T.7.0]  |      0.015 |        0.422 |     0.035 |      0.972 | -0.814 |   0.844 |\n| C(group_id)[T.8.0]  |      0.382 |        0.406 |     0.941 |      0.347 | -0.415 |   1.179 |\n| C(group_id)[T.9.0]  |      0.219 |        0.417 |     0.526 |      0.599 | -0.599 |   1.037 |\n| C(group_id)[T.10.0] |     -0.363 |        0.422 |    -0.861 |      0.390 | -1.191 |   0.465 |\n| C(group_id)[T.11.0] |      0.201 |        0.387 |     0.520 |      0.603 | -0.559 |   0.961 |\n| C(group_id)[T.12.0] |     -0.110 |        0.410 |    -0.268 |      0.788 | -0.915 |   0.694 |\n| C(group_id)[T.13.0] |      0.126 |        0.440 |     0.287 |      0.774 | -0.736 |   0.989 |\n| C(group_id)[T.14.0] |      0.353 |        0.416 |     0.848 |      0.397 | -0.464 |   1.170 |\n| C(group_id)[T.15.0] |      0.469 |        0.398 |     1.179 |      0.239 | -0.312 |   1.249 |\n| C(group_id)[T.16.0] |     -0.135 |        0.396 |    -0.340 |      0.734 | -0.913 |   0.643 |\n| C(group_id)[T.17.0] |     -0.005 |        0.401 |    -0.013 |      0.989 | -0.792 |   0.781 |\n| C(group_id)[T.18.0] |      0.283 |        0.403 |     0.702 |      0.483 | -0.508 |   1.074 |\n---\nRMSE: 2.141 R2: 0.137 \n\n\nThis is does not scale well! Imagine you have 1000 different levels of group_id. You would need to add 1000 dummy variables to your model. This is where fixed effect models come in handy. They allow you to control for these fixed effects without adding all these dummy variables. The way to do it is by a demeaning procedure. The idea is to subtract the average value of each level of group_id from the respective observations. This way, we control for the fixed effects without adding all these dummy variables. Let’s try to do this manually:\n\ndef _demean_column(df: pd.DataFrame, column: str, by: str) -&gt; pd.Series:\n    return df[column] - df.groupby(by)[column].transform(\"mean\")\n\n\nfit_demeaned = pf.feols(\n    fml=\"Y_demeaned ~ X1_demeaned\",\n    data=data.assign(\n        Y_demeaned=lambda df: _demean_column(df, \"Y\", \"group_id\"),\n        X1_demeaned=lambda df: _demean_column(df, \"X1\", \"group_id\"),\n    ),\n    vcov=\"HC1\",\n)\n\nfit_demeaned.summary()\n\n###\n\nEstimation:  OLS\nDep. var.: Y_demeaned, Fixed effects: 0\nInference:  HC1\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| Intercept     |      0.003 |        0.068 |     0.041 |      0.968 | -0.130 |   0.136 |\n| X1_demeaned   |     -1.019 |        0.083 |   -12.345 |      0.000 | -1.181 |  -0.857 |\n---\nRMSE: 2.141 R2: 0.126 \n\n\nWe get the same results as the fixed effect model Y1 ~ X | group_id above. The PyFixest package uses a more efficient algorithm to estimate the fixed effect model, but the intuition is the same."
  },
  {
    "objectID": "quickstart.html#updating-regression-coefficients",
    "href": "quickstart.html#updating-regression-coefficients",
    "title": "Getting Started with PyFixest",
    "section": "",
    "text": "You can update the coefficients of a model object via the update() method, which may be useful in an online learning setting where data arrives sequentially.\nTo see this in action, let us first fit a model on a subset of the data:\n\ndata_subsample = data.sample(frac=0.5)\nm = pf.feols(\"Y ~ X1 + X2\", data=data_subsample)\n# current coefficient vector\nm._beta_hat\n\narray([ 0.99914168, -1.05983271, -0.17775322])\n\n\nThen sample 5 new observations and update the model with the new data. The update rule is\n\\[\n\\hat{\\beta}_{n+1} = \\hat{\\beta}_n + (X_{n+1}' X_{n+1})^{-1} x_{n+1} + (y_{n+1} - x_{n+1} \\hat{\\beta}_n)\n\\]\nfor a new observation \\((x_{n+1}, y_{n+1})\\).\n\nnew_points_id = np.random.choice(list(set(data.index) - set(data_subsample.index)), 5)\nX_new, y_new = (\n    np.c_[np.ones(len(new_points_id)), data.loc[new_points_id][[\"X1\", \"X2\"]].values],\n    data.loc[new_points_id][\"Y\"].values,\n)\nm.update(X_new, y_new)\n\narray([ 0.98247951, -1.04782148, -0.17523909])\n\n\nWe verify that we get the same results if we had estimated the model on the appended data.\n\npf.feols(\n    \"Y ~ X1 + X2\", data=data.loc[data_subsample.index.append(pd.Index(new_points_id))]\n).coef().values\n\narray([ 0.98247951, -1.04782148, -0.17523909])"
  },
  {
    "objectID": "quickstart.html#inference-via-the-wild-bootstrap",
    "href": "quickstart.html#inference-via-the-wild-bootstrap",
    "title": "Getting Started with PyFixest",
    "section": "Inference via the Wild Bootstrap",
    "text": "Inference via the Wild Bootstrap\nIt is also possible to run a wild (cluster) bootstrap after estimation (via the wildboottest module, see MacKinnon, J. G., Nielsen, M. Ø., & Webb, M. D. (2023). Fast and reliable jackknife and bootstrap methods for cluster-robust inference. Journal of Applied Econometrics, 38(5), 671–694.):\n\nfit2 = pf.feols(fml=\"Y ~ X1\", data=data, vcov={\"CRV1\": \"group_id\"})\nfit2.wildboottest(param=\"X1\", reps=999)\n\nparam                             X1\nt value           -8.567586579080423\nPr(&gt;|t|)                         0.0\nbootstrap_type                    11\ninference              CRV(group_id)\nimpose_null                     True\nssc                         1.056615\ndtype: object"
  },
  {
    "objectID": "quickstart.html#the-causal-cluster-variance-estimator",
    "href": "quickstart.html#the-causal-cluster-variance-estimator",
    "title": "Getting Started with PyFixest",
    "section": "The Causal Cluster Variance Estimator",
    "text": "The Causal Cluster Variance Estimator\nAdditionally, PyFixest supports the causal cluster variance estimator following Abadie et al. (2023). Let’s look into it with another data set:\n\ndf = pd.read_stata(\"http://www.damianclarke.net/stata/census2000_5pc.dta\")\n\ndf.head()\n\n\n\n\n  \n    \n      \n      ln_earnings\n      educ\n      hours\n      state\n      college\n    \n  \n  \n    \n      0\n      11.91839\n      18.0\n      50.0\n      44.0\n      1.0\n    \n    \n      1\n      11.48247\n      11.0\n      42.0\n      44.0\n      0.0\n    \n    \n      2\n      10.46310\n      12.0\n      42.0\n      44.0\n      0.0\n    \n    \n      3\n      10.22194\n      13.0\n      40.0\n      44.0\n      1.0\n    \n    \n      4\n      9.21034\n      13.0\n      8.0\n      44.0\n      1.0\n    \n  \n\n\n\n\n\naxes = df.plot.hist(column=[\"ln_earnings\"], by=[\"college\"])\n\n\n\n\n\n\n\n\nNow we can estimate the model ln_earnings ~ college where we cluster the standard errors at the state level:\n\nfit3 = pf.feols(\"ln_earnings ~ college\", vcov={\"CRV1\": \"state\"}, data=df)\nfit3.ccv(treatment=\"college\", pk=0.05, n_splits=2, seed=929)\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(&gt;|t|)\n      2.5%\n      97.5%\n    \n  \n  \n    \n      CCV\n      0.4656425903701481\n      0.00348\n      133.820079\n      0.0\n      0.458657\n      0.472628\n    \n    \n      CRV1\n      0.465643\n      0.027142\n      17.155606\n      0.0\n      0.411152\n      0.520133"
  },
  {
    "objectID": "quickstart.html#randomization-inference",
    "href": "quickstart.html#randomization-inference",
    "title": "Getting Started with PyFixest",
    "section": "Randomization Inference",
    "text": "Randomization Inference\nYou can also conduct inference via randomization inference (see Heß, Stata Journal 2017). PyFixest supports random and cluster random sampling.\n\nfit2.ritest(resampvar=\"X1=0\", reps=1000, cluster=\"group_id\")\n\nH0                                     X1=0\nri-type                     randomization-c\nEstimate                 -1.000085840074156\nPr(&gt;|t|)                                0.0\nStd. Error (Pr(&gt;|t|))                   0.0\n2.5% (Pr(&gt;|t|))                         0.0\n97.5% (Pr(&gt;|t|))                        0.0\nCluster                            group_id\ndtype: object"
  },
  {
    "objectID": "quickstart.html#multiple-testing-corrections-bonferroni-and-romano-wolf",
    "href": "quickstart.html#multiple-testing-corrections-bonferroni-and-romano-wolf",
    "title": "Getting Started with PyFixest",
    "section": "Multiple Testing Corrections: Bonferroni and Romano-Wolf",
    "text": "Multiple Testing Corrections: Bonferroni and Romano-Wolf\nTo correct for multiple testing, p-values can be adjusted via either the Bonferroni, the method by Romano and Wolf (2005), see for example The Romano-Wolf Multiple Hypothesis Correction in Stata, and the method by Westfall & Young (see here).\n\npf.bonferroni([fit, fit2], param=\"X1\").round(3)\n\n\n\n\n  \n    \n      \n      est0\n      est1\n    \n  \n  \n    \n      Estimate\n      -1.019\n      -1.000\n    \n    \n      Std. Error\n      0.125\n      0.117\n    \n    \n      t value\n      -8.174\n      -8.568\n    \n    \n      Pr(&gt;|t|)\n      0.000\n      0.000\n    \n    \n      2.5%\n      -1.281\n      -1.245\n    \n    \n      97.5%\n      -0.757\n      -0.755\n    \n    \n      Bonferroni Pr(&gt;|t|)\n      0.000\n      0.000\n    \n  \n\n\n\n\n\npf.rwolf([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n\n\n\n\n  \n    \n      \n      est0\n      est1\n    \n  \n  \n    \n      Estimate\n      -1.019\n      -1.000\n    \n    \n      Std. Error\n      0.125\n      0.117\n    \n    \n      t value\n      -8.174\n      -8.568\n    \n    \n      Pr(&gt;|t|)\n      0.000\n      0.000\n    \n    \n      2.5%\n      -1.281\n      -1.245\n    \n    \n      97.5%\n      -0.757\n      -0.755\n    \n    \n      RW Pr(&gt;|t|)\n      0.000\n      0.000\n    \n  \n\n\n\n\n\npf.wyoung([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n\n\n\n\n  \n    \n      \n      est0\n      est1\n    \n  \n  \n    \n      Estimate\n      -1.019\n      -1.000\n    \n    \n      Std. Error\n      0.125\n      0.117\n    \n    \n      t value\n      -8.174\n      -8.568\n    \n    \n      Pr(&gt;|t|)\n      0.000\n      0.000\n    \n    \n      2.5%\n      -1.281\n      -1.245\n    \n    \n      97.5%\n      -0.757\n      -0.755\n    \n    \n      WY Pr(&gt;|t|)\n      0.000\n      0.000"
  },
  {
    "objectID": "quickstart.html#joint-confidence-intervals",
    "href": "quickstart.html#joint-confidence-intervals",
    "title": "Getting Started with PyFixest",
    "section": "Joint Confidence Intervals",
    "text": "Joint Confidence Intervals\nSimultaneous confidence bands for a vector of parameters can be computed via the joint_confint() method. See Simultaneous confidence bands: Theory, implementation, and an application to SVARs for background.\n\nfit_ci = pf.feols(\"Y ~ X1+ C(f1)\", data=data)\nfit_ci.confint(joint=True).head()\n\n\n\n\n  \n    \n      \n      2.5%\n      97.5%\n    \n  \n  \n    \n      Intercept\n      -0.429101\n      1.407012\n    \n    \n      X1\n      -1.161461\n      -0.737421\n    \n    \n      C(f1)[T.1.0]\n      1.380088\n      3.785210\n    \n    \n      C(f1)[T.2.0]\n      -2.843214\n      -0.320654\n    \n    \n      C(f1)[T.3.0]\n      -1.612815\n      0.988148"
  },
  {
    "objectID": "pyfixest_gpu.html",
    "href": "pyfixest_gpu.html",
    "title": "PyFixest on professional-tier GPUs",
    "section": "",
    "text": "PyFixest allows to run the fixed effects demeaning on the GPU via the demeaner_backend argument. To do so, you will have to install jax and jaxblib, for example by typing pip install pyfixest[jax].\nWe test two back-ends for the iterative alternating-projections component of the fixed-effects regression on an Nvidia A100 GPU with 40 GB VRAM (a GPU that one typically wouldn’t have installed to play graphics-intensive videogames on consumer hardware). numba benchmarks are run on a 12-core xeon CPU.\nThe JAX backend exhibits major performance improvements on the GPU over numba in large problems.\n\nOn the CPU instead, we find that numba outperforms the JAX backend. You can find details in the benchmark section of the github repo."
  },
  {
    "objectID": "ssc.html",
    "href": "ssc.html",
    "title": "On Small Sample Corrections",
    "section": "",
    "text": "The fixest R package provides various options for small sample corrections. While it has an excellent vignette on the topic, reproducing its behavior in pyfixest took more time than expected. So that future developers (and my future self) can stay sane, I’ve compiled all of my hard-earned understanding of how small sample adjustments work in fixest and how they are implemented in pyfixest in this document.\nIn both fixest and pyfixest, small sample corrections are controlled by the ssc function. In pyfixest, ssc accepts four arguments: k_adj, G_adj, k_fixef and G_df.\nBased on these inputs, the adjusted variance-covariance matrix is computed as:\nWhere:\nOutside of this formula, we have df_t, which is the degrees of freedom used for p-values and confidence intervals:"
  },
  {
    "objectID": "ssc.html#k_adj-true",
    "href": "ssc.html#k_adj-true",
    "title": "On Small Sample Corrections",
    "section": "k_adj = True",
    "text": "k_adj = True\nIf k_adj = True, the adjustment factor is:\nadj_val = (N - 1) / (N - df_k)\nIf k_adj = False, no adjustment is applied.\nNote that for k_adj = True and heteroskedastic errors, the applied correction is N / (N-k)."
  },
  {
    "objectID": "ssc.html#k_fixef",
    "href": "ssc.html#k_fixef",
    "title": "On Small Sample Corrections",
    "section": "k_fixef",
    "text": "k_fixef\nThe k_fixef argument controls how fixed effects contribute to df_k, and thus to adj_val. It supports three options:\n\n\"none\"\n\"full\"\n\"nonnested\"\n\n\nk_fixef = \"none\"\nFixed effects are ignored when counting parameters:\n\nExample:\n\nY ~ X1 | f1 → k = 1\nY ~ X1 + X2 | f1 → k = 2\n\n\n\n\nk_fixef = \"full\"\nFixed effects are fully counted. For n_fe total fixed effects and each fixed effect f_i, we set df_k = k + k_fe,\n\nIf there is more than one fixed effect, we drop one level from each fixed effects except the first (to avoid multicollinearity) k_fe = sum_{i=1}^{n_fe} levels(f_i) - (n_fe - 1)\nIf there is only one fixed effect: k_fe = sum_{i=1}^{n_fe} levels(f_i) = levels(f_1)\n\n\n\nk_fixef = \"nonnested\"\nFixed effects may be nested within cluster variables (e.g., district FEs nested in state clusters). If k_fixef = \"nonnested\", nested fixed effects do not count toward k_fe:\nk_fe = sum_{i=1}^{n_fe} levels(f_i) - k_fe_nested - (n_fe - 1)\nwhere k_fe_nested is the count of nested fixed effects. For cluster fixed effects, k_fe_nested = G, the number of clusters.\n\n⚠️ Note: If you already subtracted a level from a nested FE, you may need to add it back."
  },
  {
    "objectID": "ssc.html#g_adj",
    "href": "ssc.html#g_adj",
    "title": "On Small Sample Corrections",
    "section": "G_adj",
    "text": "G_adj\nThis argument is only relevant for clustered errors.\nIf G_adj = True, we apply a second correction:\nG_df_val = G / (G - 1)\nWhere:\n\nG is the number of clusters for clustered errors, or N for heteroskedastic errors.\nThis follows the approach in R’s sandwich package, interpreting heteroskedastic errors as “singleton clusters.”\n\n\nTip: If G_adj = True for IID errors, G_df_val defaults to 1. For heteroskedastic erros, despite its name, G_adj=True will apply an adjustment of (N-1) / N, as there are \\(G = N\\) singleton clusters."
  },
  {
    "objectID": "ssc.html#g_df",
    "href": "ssc.html#g_df",
    "title": "On Small Sample Corrections",
    "section": "G_df",
    "text": "G_df\nRelevant only for multi-way clustering. Two-way clustering, for example, can be written as:\nvcov = ssc_A * vcov_A + ssc_B * vcov_B - ssc_AB * vcov_AB\nwhere A and B are clustering dimensions, with G_AB &gt; G_A &gt; G_B.\n\nIf G_df = \"min\", then G is set to the minimum value of G_A, G_B, and G_AB.\nIf G_df = \"conventional\", each clustering dimension uses its own cluster count (G_A, G_B, etc.) for its respective adjustment."
  },
  {
    "objectID": "compare-fixest-pyfixest.html",
    "href": "compare-fixest-pyfixest.html",
    "title": "Does PyFixest match fixest?",
    "section": "",
    "text": "This vignette compares estimation results from fixest with pyfixest via the rpy2 package."
  },
  {
    "objectID": "compare-fixest-pyfixest.html#setup",
    "href": "compare-fixest-pyfixest.html#setup",
    "title": "Does PyFixest match fixest?",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\nimport rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.packages import importr\n\nimport pyfixest as pf\n\n# Activate pandas2ri\npandas2ri.activate()\n\n# Import R packages\nfixest = importr(\"fixest\")\nstats = importr(\"stats\")\nbroom = importr(\"broom\")\n\n# IPython magic commands for autoreloading\n%load_ext autoreload\n%autoreload 2\n\n# Get data using pyfixest\ndata = pf.get_data(model=\"Feols\", N=10_000, seed=99292)"
  },
  {
    "objectID": "compare-fixest-pyfixest.html#ordinary-least-squares-ols",
    "href": "compare-fixest-pyfixest.html#ordinary-least-squares-ols",
    "title": "Does PyFixest match fixest?",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\nIID Inference\nFirst, we estimate a model via `pyfixest. We compute “iid” standard errors.\n\nfit = pf.feols(fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov=\"iid\")\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\nWe estimate the same model with weights:\n\nfit_weights = pf.feols(\n    fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, weights=\"weights\", vcov=\"iid\"\n)\n\nVia r-fixest and rpy2, we get\n\nr_fit = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=\"iid\",\n)\n\nr_fit_weights = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    weights=ro.Formula(\"~weights\"),\n    vcov=\"iid\",\n)\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\n\n\nR[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\nR[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\nLet’s compare how close the covariance matrices are:\n\nfit_vcov = fit._vcov\nr_vcov = stats.vcov(r_fit)\nfit_vcov - r_vcov\n\narray([[-6.50521303e-19, -1.23953015e-21],\n       [-1.23953015e-21, -1.32137140e-19]])\n\n\nAnd for WLS:\n\nfit_weights._vcov - stats.vcov(r_fit_weights)\n\narray([[ 1.68051337e-18, -1.27054942e-21],\n       [-1.27054942e-21, -1.52465931e-19]])\n\n\nWe conclude by comparing all estimation results via the tidy methods:\n\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.112019\n0.017042\n6.572948\n5.181855e-11\n0.078612\n0.145425\n\n\nX2\n0.732788\n0.004621\n158.578261\n0.000000e+00\n0.723730\n0.741846\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.112019\n0.017042\n6.572948\n0.0\n\n\n1\nX2\n0.732788\n0.004621\n158.578261\n0.0\n\n\n\n\n\n\n\n\nfit_weights.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.123687\n0.016975\n7.286200\n3.432810e-13\n0.090411\n0.156962\n\n\nX2\n0.732244\n0.004610\n158.844322\n0.000000e+00\n0.723207\n0.741280\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit_weights)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.123687\n0.016975\n7.2862\n0.0\n\n\n1\nX2\n0.732244\n0.00461\n158.844322\n0.0\n\n\n\n\n\n\n\n\n\nHeteroskedastic Errors\nWe repeat the same exercise with heteroskedastic (HC1) errors:\n\nfit = pf.feols(fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov=\"hetero\")\nfit_weights = pf.feols(\n    fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov=\"hetero\", weights=\"weights\"\n)\n\n\nr_fit = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=\"hetero\",\n)\n\nr_fit_weights = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    weights=ro.Formula(\"~weights\"),\n    vcov=\"hetero\",\n)\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\n\n\nAs before, we compare the variance covariance matrices:\n\nfit._vcov - stats.vcov(r_fit)\n\narray([[-1.08100378e-14, -1.51722114e-15],\n       [-1.51722119e-15, -3.55911386e-15]])\n\n\n\nfit_weights._vcov - stats.vcov(r_fit_weights)\n\narray([[-7.68563815e-15,  2.60362300e-15],\n       [ 2.60362300e-15, -1.52112210e-15]])\n\n\nWe conclude by comparing all estimation results via the tidy methods:\n\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.112019\n0.017105\n6.548962\n6.082002e-11\n0.078490\n0.145548\n\n\nX2\n0.732788\n0.004579\n160.036098\n0.000000e+00\n0.723812\n0.741763\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.112019\n0.017105\n6.548962\n0.0\n\n\n1\nX2\n0.732788\n0.004579\n160.036098\n0.0\n\n\n\n\n\n\n\n\nfit_weights.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.123687\n0.019470\n6.352618\n2.210045e-10\n0.085521\n0.161852\n\n\nX2\n0.732244\n0.005169\n141.653304\n0.000000e+00\n0.722111\n0.742376\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit_weights)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.123687\n0.01947\n6.352618\n0.0\n\n\n1\nX2\n0.732244\n0.005169\n141.653304\n0.0\n\n\n\n\n\n\n\n\n\nCluster-Robust Errors\nWe conclude with cluster robust errors.\n\nfit = pf.feols(fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov={\"CRV1\": \"f1\"})\nfit_weights = pf.feols(\n    fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov={\"CRV1\": \"f1\"}, weights=\"weights\"\n)\n\nr_fit = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=ro.Formula(\"~f1\"),\n)\nr_fit_weights = fixest.feols(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    weights=ro.Formula(\"~weights\"),\n    vcov=ro.Formula(\"~f1\"),\n)\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTE: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n\n\n\n\nfit._vcov - stats.vcov(r_fit)\n\narray([[ 2.86079212e-14, -4.95778972e-15],\n       [-4.95778972e-15, -9.43703123e-16]])\n\n\n\nfit_weights._vcov - stats.vcov(r_fit_weights)\n\narray([[ 7.00025975e-15,  1.26505760e-14],\n       [ 1.26505760e-14, -1.11283189e-16]])\n\n\nWe conclude by comparing all estimation results via the tidy methods:\n\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.112019\n0.015865\n7.060624\n4.823750e-09\n0.080152\n0.143885\n\n\nX2\n0.732788\n0.004490\n163.215618\n0.000000e+00\n0.723770\n0.741806\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.112019\n0.015865\n7.060624\n0.0\n\n\n1\nX2\n0.732788\n0.00449\n163.215618\n0.0\n\n\n\n\n\n\n\n\nfit_weights.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n0.123687\n0.018368\n6.733633\n1.566958e-08\n0.086792\n0.160581\n\n\nX2\n0.732244\n0.005266\n139.062210\n0.000000e+00\n0.721667\n0.742820\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(r_fit_weights)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n0.123687\n0.018368\n6.733633\n0.0\n\n\n1\nX2\n0.732244\n0.005266\n139.06221\n0.0"
  },
  {
    "objectID": "compare-fixest-pyfixest.html#poisson-regression",
    "href": "compare-fixest-pyfixest.html#poisson-regression",
    "title": "Does PyFixest match fixest?",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\ndata = pf.get_data(model=\"Fepois\")\n\n\nfit_iid = pf.fepois(fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov=\"iid\", iwls_tol=1e-10)\nfit_hetero = pf.fepois(\n    fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov=\"hetero\", iwls_tol=1e-10\n)\nfit_crv = pf.fepois(\n    fml=\"Y ~ X1 + X2 | f1 + f2\", data=data, vcov={\"CRV1\": \"f1\"}, iwls_tol=1e-10\n)\n\nfit_r_iid = fixest.fepois(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=\"iid\",\n)\n\nfit_r_hetero = fixest.fepois(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=\"hetero\",\n)\n\nfit_r_crv = fixest.fepois(\n    ro.Formula(\"Y ~ X1 + X2 | f1 + f2\"),\n    data=data,\n    vcov=ro.Formula(\"~f1\"),\n)\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 2 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 2 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 2 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTES: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n       1/1 fixed-effects (2 observations) removed because of only 0 outcomes or singletons.\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTES: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n       1/1 fixed-effects (2 observations) removed because of only 0 outcomes or singletons.\n\nWARNING:rpy2.rinterface_lib.callbacks:R[write to console]: NOTES: 3 observations removed because of NA values (LHS: 1, RHS: 1, Fixed-effects: 1).\n       1/1 fixed-effects (2 observations) removed because of only 0 outcomes or singletons.\n\n\n\n\nfit_iid._vcov - stats.vcov(fit_r_iid)\n\narray([[ 1.28462055e-08, -6.97520098e-10],\n       [-6.97520098e-10,  1.80739283e-09]])\n\n\n\nfit_hetero._vcov - stats.vcov(fit_r_hetero)\n\narray([[ 2.31981933e-08, -7.87424679e-10],\n       [-7.87424679e-10,  3.27120926e-09]])\n\n\n\nfit_crv._vcov - stats.vcov(fit_r_crv)\n\narray([[ 1.63339756e-08, -1.24809110e-10],\n       [-1.24809110e-10,  3.27895328e-09]])\n\n\nWe conclude by comparing all estimation results via the tidy methods:\n\nfit_iid.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n-0.006591\n0.042025\n-0.156836\n0.875374\n-0.088959\n0.075777\n\n\nX2\n-0.014924\n0.011336\n-1.316520\n0.188000\n-0.037142\n0.007294\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(fit_r_iid)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n-0.006591\n0.042025\n-0.156836\n0.875374\n\n\n1\nX2\n-0.014924\n0.011336\n-1.316529\n0.187996\n\n\n\n\n\n\n\n\nfit_hetero.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n-0.006591\n0.040363\n-0.163297\n0.870284\n-0.085700\n0.072518\n\n\nX2\n-0.014924\n0.010828\n-1.378277\n0.168118\n-0.036146\n0.006298\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(fit_r_hetero)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n-0.006591\n0.040362\n-0.163298\n0.870284\n\n\n1\nX2\n-0.014924\n0.010828\n-1.378296\n0.168112\n\n\n\n\n\n\n\n\nfit_crv.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nX1\n-0.006591\n0.035302\n-0.186705\n0.851892\n-0.075782\n0.062600\n\n\nX2\n-0.014924\n0.010468\n-1.425726\n0.153947\n-0.035440\n0.005592\n\n\n\n\n\n\n\n\npd.DataFrame(broom.tidy_fixest(fit_r_crv)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nX1\n-0.006591\n0.035302\n-0.186706\n0.851891\n\n\n1\nX2\n-0.014924\n0.010467\n-1.425747\n0.153941"
  },
  {
    "objectID": "table-layout.html",
    "href": "table-layout.html",
    "title": "Table Layout with PyFixest",
    "section": "",
    "text": "NoteMigration Notice\n\n\n\nStarting with pyfixest 0.41.0 (currently in development), the table functionality is powered by maketables. The pf.etable() API remains unchanged. pf.dtable() is deprecated (use DTable() directly) and pf.make_table() has been removed (use maketables.MTable() directly).\nPyfixest comes with functions to generate publication-ready tables. Regression tables are generated with pf.etable(), which can output different formats, for instance using the Great Tables package or generating formatted LaTex Tables using booktabs. Descriptive statistics tables can be created with DTable() and custom tables with maketables.MTable().\nTo begin, we load some libraries and fit a set of regression models.\nimport numpy as np\nimport pandas as pd\nimport pylatex as pl  # for the latex table; note: not a dependency of pyfixest - needs manual installation\nfrom maketables import DTable\nfrom great_tables import loc, style  # great_tables is used by maketables internally\nfrom IPython.display import FileLink, display\n\nimport pyfixest as pf\n\n%load_ext autoreload\n%autoreload 2\n\ndata = pf.get_data()\n\nfit1 = pf.feols(\"Y ~ X1 + X2 | f1\", data=data)\nfit2 = pf.feols(\"Y ~ X1 + X2 | f1 + f2\", data=data)\nfit3 = pf.feols(\"Y ~ X1 *X2 | f1 + f2\", data=data)\nfit4 = pf.feols(\"Y2 ~ X1 + X2 | f1\", data=data)\nfit5 = pf.feols(\"Y2 ~ X1 + X2 | f1 + f2\", data=data)\nfit6 = pf.feols(\"Y2 ~ X1 *X2 | f1 + f2\", data=data)"
  },
  {
    "objectID": "table-layout.html#basic-usage",
    "href": "table-layout.html#basic-usage",
    "title": "Table Layout with PyFixest",
    "section": "Basic Usage",
    "text": "Basic Usage\nWe can compare all regression models via the pyfixest-internal pf.etable() function:\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6])\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    X2\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    X1 × X2\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nYou can also estimate and display multiple regressions with one line of code using the (py)fixest stepwise notation:\n\npf.etable(pf.feols(\"Y+Y2~csw(X1,X2,X1:X2)\", data=data))\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -1.000***  (0.085)\n    -0.993***  (0.082)\n    -0.992***  (0.082)\n    -1.322***  (0.215)\n    -1.316***  (0.214)\n    -1.316***  (0.215)\n  \n  \n    X2\n    \n    -0.176***  (0.022)\n    -0.197***  (0.036)\n    \n    -0.133*  (0.057)\n    -0.132  (0.095)\n  \n  \n    X1 × X2\n    \n    \n    0.02  (0.027)\n    \n    \n    -0.000746  (0.071)\n  \n  \n    Intercept\n    0.919***  (0.112)\n    0.889***  (0.108)\n    0.888***  (0.108)\n    1.064***  (0.283)\n    1.042***  (0.283)\n    1.042***  (0.283)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n    998\n    999\n    999\n    999\n  \n  \n    R2\n    0.123\n    0.177\n    0.177\n    0.037\n    0.042\n    0.042\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#keep-and-drop-variables",
    "href": "table-layout.html#keep-and-drop-variables",
    "title": "Table Layout with PyFixest",
    "section": "Keep and drop variables",
    "text": "Keep and drop variables\netable allows us to do a few things out of the box. For example, we can only keep the variables that we’d like, which keeps all variables that fit the provided regex match.\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], keep=\"X1\")\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    X1 × X2\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nWe can use the exact_match argument to select a specific set of variables:\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], keep=[\"X1\", \"X2\"], exact_match=True)\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    X2\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nWe can also easily drop variables via the drop argument:\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], drop=[\"X1\"])\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X2\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#display-p-values-or-confidence-intervals",
    "href": "table-layout.html#display-p-values-or-confidence-intervals",
    "title": "Table Layout with PyFixest",
    "section": "Display p-values or confidence intervals",
    "text": "Display p-values or confidence intervals\nBy default, pf.etable() reports standard errors. But we can also ask to output p-values or confidence intervals via the coef_fmt function argument.\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], coef_fmt=\"b \\n (se) \\n [p]\")\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.95***  (0.066)  [0]\n    -0.924***  (0.056)  [0]\n    -0.924***  (0.056)  [0]\n    -1.267***  (0.211)  [0]\n    -1.232***  (0.211)  [0]\n    -1.231***  (0.211)  [0]\n  \n  \n    X2\n    -0.174***  (0.018)  [0]\n    -0.174***  (0.015)  [0]\n    -0.185***  (0.025)  [0]\n    -0.131*  (0.056)  [0.02]\n    -0.118*  (0.056)  [0.036]\n    -0.074  (0.094)  [0.436]\n  \n  \n    X1 × X2\n    \n    \n    0.011  (0.019)  [0.572]\n    \n    \n    -0.041  (0.071)  [0.563]\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)   [p-value]"
  },
  {
    "objectID": "table-layout.html#significance-levels-and-rounding",
    "href": "table-layout.html#significance-levels-and-rounding",
    "title": "Table Layout with PyFixest",
    "section": "Significance levels and rounding",
    "text": "Significance levels and rounding\nAdditionally, we can also overwrite the defaults for the reported significance levels and control the rounding of results via the signif_code and digits function arguments:\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], signif_code=[0.01, 0.05, 0.1], digits=5)\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.94953***  (0.06637)\n    -0.92405***  (0.05606)\n    -0.92417***  (0.05608)\n    -1.26655***  (0.21078)\n    -1.23153***  (0.21141)\n    -1.23100***  (0.21149)\n  \n  \n    X2\n    -0.17423***  (0.01760)\n    -0.17411***  (0.01486)\n    -0.18550***  (0.02502)\n    -0.13056**  (0.05592)\n    -0.11767**  (0.05610)\n    -0.07369  (0.09447)\n  \n  \n    X1 × X2\n    \n    \n    0.01057  (0.01868)\n    \n    \n    -0.04082  (0.07054)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#other-output-formats",
    "href": "table-layout.html#other-output-formats",
    "title": "Table Layout with PyFixest",
    "section": "Other output formats",
    "text": "Other output formats\nBy default, pf.etable() returns a GT object (see the Great Tables package), but you can also opt to dataframe, markdown, or latex output via the type argument.\n\n# Pandas styler output:\npf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    signif_code=[0.01, 0.05, 0.1],\n    digits=5,\n    coef_fmt=\"b (se)\",\n    type=\"df\",\n)\n\n\n\n\n  \n    \n      \n      \n      Y\n      Y2\n    \n    \n      \n      \n      (1)\n      (2)\n      (3)\n      (4)\n      (5)\n      (6)\n    \n  \n  \n    \n      coef\n      X1\n      -0.94953*** (0.06637)\n      -0.92405*** (0.05606)\n      -0.92417*** (0.05608)\n      -1.26655*** (0.21078)\n      -1.23153*** (0.21141)\n      -1.23100*** (0.21149)\n    \n    \n      X2\n      -0.17423*** (0.01760)\n      -0.17411*** (0.01486)\n      -0.18550*** (0.02502)\n      -0.13056** (0.05592)\n      -0.11767** (0.05610)\n      -0.07369 (0.09447)\n    \n    \n      X1 × X2\n      \n      \n      0.01057 (0.01868)\n      \n      \n      -0.04082 (0.07054)\n    \n    \n      fe\n      f1\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      f2\n      -\n      x\n      x\n      -\n      x\n      x\n    \n    \n      stats\n      Observations\n      997\n      997\n      997\n      998\n      998\n      998\n    \n    \n      R²\n      0.489\n      0.659\n      0.659\n      0.12\n      0.172\n      0.172\n    \n  \n\n\n\n\n\n# Markdown output:\npf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    signif_code=[0.01, 0.05, 0.1],\n    digits=5,\n    type=\"md\",\n)\n\n|                           | ('Y', '(1)')   | ('Y', '(2)')   | ('Y', '(3)')   | ('Y2', '(4)')   | ('Y2', '(5)')   | ('Y2', '(6)')   |\n|:--------------------------|:---------------|:---------------|:---------------|:----------------|:----------------|:----------------|\n| ('coef', 'X1')            | -0.94953***    | -0.92405***    | -0.92417***    | -1.26655***     | -1.23153***     | -1.23100***     |\n|                           |  (0.06637)     |  (0.05606)     |  (0.05608)     |  (0.21078)      |  (0.21141)      |  (0.21149)      |\n| ('coef', 'X2')            | -0.17423***    | -0.17411***    | -0.18550***    | -0.13056**      | -0.11767**      | -0.07369        |\n|                           |  (0.01760)     |  (0.01486)     |  (0.02502)     |  (0.05592)      |  (0.05610)      |  (0.09447)      |\n| ('coef', 'X1 × X2')       |                |                | 0.01057        |                 |                 | -0.04082        |\n|                           |                |                |  (0.01868)     |                 |                 |  (0.07054)      |\n| ('fe', 'f1')              | x              | x              | x              | x               | x               | x               |\n| ('fe', 'f2')              | -              | x              | x              | -               | x               | x               |\n| ('stats', 'Observations') | 997            | 997            | 997            | 998             | 998             | 998             |\n| ('stats', 'R²')           | 0.489          | 0.659          | 0.659          | 0.12            | 0.172           | 0.172           |\n\n\nTo obtain latex output use type = \"tex\". If you want to save the table as a tex file, you can use the file_name= argument to specify the respective path where it should be saved. Etable will use latex packages booktabs, threeparttable, makecell, and tabularx for the table layout, so don’t forget to include these packages in your latex document.\n\n# LaTex output (include latex packages booktabs, threeparttable, makecell, and tabularx in your document):\ntab = pf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    signif_code=[0.01, 0.05, 0.1],\n    digits=2,\n    type=\"tex\",\n)\n\nThe following code generates a pdf including the regression table which you can display clicking on the link below the cell:\n\n## Use pylatex to create a tex file with the table\n\n\ndef make_pdf(tab, file):\n    \"Create a PDF document with tex table.\"\n    doc = pl.Document()\n    doc.packages.append(pl.Package(\"booktabs\"))\n    doc.packages.append(pl.Package(\"threeparttable\"))\n    doc.packages.append(pl.Package(\"makecell\"))\n    doc.packages.append(pl.Package(\"tabularx\"))\n\n    with (\n        doc.create(pl.Section(\"A PyFixest LateX Table\")),\n        doc.create(pl.Table(position=\"htbp\")) as table,\n    ):\n        table.append(pl.NoEscape(tab))\n\n    doc.generate_pdf(file, clean_tex=False)\n\n\n# Compile latex to pdf & display a button with the hyperlink to the pdf\n# requires tex installation\nrun = False\nif run:\n    make_pdf(tab, \"latexdocs/SampleTableDoc\")\ndisplay(FileLink(\"latexdocs/SampleTableDoc.pdf\"))\n\nlatexdocs/SampleTableDoc.pdf"
  },
  {
    "objectID": "table-layout.html#rename-variables",
    "href": "table-layout.html#rename-variables",
    "title": "Table Layout with PyFixest",
    "section": "Rename variables",
    "text": "Rename variables\nYou can also rename variables if you want to have a more readable output. Just pass a dictionary to the labels argument. Note that interaction terms will also be relabeled using the specified labels for the interacted variables (if you want to manually relabel an interaction term differently, add it to the dictionary).\n\nlabels = {\n    \"Y\": \"Wage\",\n    \"Y2\": \"Wealth\",\n    \"X1\": \"Age\",\n    \"X2\": \"Years of Schooling\",\n    \"f1\": \"Industry\",\n    \"f2\": \"Year\",\n}\n\npf.etable([fit1, fit2, fit3, fit4, fit5, fit6], labels=labels)\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nIf you want to label the rows indicating the inclusion of fixed effects not with the variable label but with a custom label, you can pass on a separate dictionary to the felabels argument.\n\npf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    labels=labels,\n    felabels={\"f1\": \"Industry Fixed Effects\", \"f2\": \"Year Fixed Effects\"},\n)\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry Fixed Effects\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year Fixed Effects\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#rename-categorical-variables",
    "href": "table-layout.html#rename-categorical-variables",
    "title": "Table Layout with PyFixest",
    "section": "Rename categorical variables",
    "text": "Rename categorical variables\nBy default, categorical variables are returned using the formulaic “C(variable)[T.value]” notation. Via the cat_template argument, you can rename categorical variables via a specified template {variable}={value}. This works when either the variable is categorial in the DataFrame, or the C() or i() operators are used in the regresson formula. ´\n\n# Add a categorical variable\ndata['job'] = np.random.choice([\"Managerial\", \"Admin\", \"Blue collar\"], size=len(data), p=[1/3, 1/3, 1/3])\n# Add a label for this variable to the dictionary\nlabels['job']=\"Job Family\"\n\nfit7 = pf.feols(\"Y ~ X1 + X2 + job\", data = data)\n\npf.etable([fit7], labels=labels, cat_template = \"{variable}::{value}\")\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n\n\n  (1)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.994***  (0.082)\n  \n  \n    Years of Schooling\n    -0.175***  (0.022)\n  \n  \n    Job Family::Blue collar\n    -0.065  (0.161)\n  \n  \n    Job Family::Managerial\n    -0.254  (0.163)\n  \n  \n    Intercept\n    0.994***  (0.142)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n  \n  \n    R2\n    0.179\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nBut you can also remove the variable name and only keep the levels (categories) by specifying cat_template=“{value}”. Note that the labeling of categories also works in interaction terms:\n\nfit7 = pf.feols(\"Y ~ X1 + X2 + job\", data = data)\nfit8 = pf.feols(\"Y ~ X1 + X2 + job*X2\", data = data)\n\npf.etable([fit7, fit8], labels=labels, cat_template=\"{value}\")\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n\n\n  (1)\n  (2)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.994***  (0.082)\n    -0.997***  (0.082)\n  \n  \n    Years of Schooling\n    -0.175***  (0.022)\n    -0.152***  (0.038)\n  \n  \n    Blue collar\n    -0.065  (0.161)\n    -0.073  (0.161)\n  \n  \n    Managerial\n    -0.254  (0.163)\n    -0.263  (0.163)\n  \n  \n    Blue collar × Years of Schooling\n    \n    -0.014  (0.053)\n  \n  \n    Managerial × Years of Schooling\n    \n    -0.057  (0.054)\n  \n  \n    Intercept\n    0.994***  (0.142)\n    1.005***  (0.143)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n    998\n  \n  \n    R2\n    0.179\n    0.18\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#change-reference-category",
    "href": "table-layout.html#change-reference-category",
    "title": "Table Layout with PyFixest",
    "section": "Change reference category",
    "text": "Change reference category\nYou can also change the reference category of a categorical variable using the ref argument in the interaction i() operator. For example, repeating the last estimation but changing the reference category to “Managerial” instead of “Admin”:\n\nfit9 = pf.feols(\"Y ~ X1 + X2 + i(job,ref='Managerial') + i(job,X2,ref='Managerial')\", data = data)\n\npf.etable([fit9], labels=labels, cat_template=\"{value}\")\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n\n\n  (1)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.997***  (0.082)\n  \n  \n    Years of Schooling\n    -0.209***  (0.039)\n  \n  \n    Admin\n    0.263  (0.163)\n  \n  \n    Blue collar\n    0.191  (0.164)\n  \n  \n    Admin × Years of Schooling\n    0.057  (0.054)\n  \n  \n    Blue collar × Years of Schooling\n    0.042  (0.054)\n  \n  \n    Intercept\n    0.741***  (0.144)\n  \n  \n    stats\n  \n  \n    Observations\n    998\n  \n  \n    R2\n    0.18\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nNotice that this process will change the _coefnames. In this example, the new _coefnames are:\n\nfit9._coefnames\n\n[np.str_('Intercept'),\n np.str_('X1'),\n np.str_('X2'),\n np.str_(\"C(job, contr.treatment(base='Managerial'))[T.Admin]\"),\n np.str_(\"C(job, contr.treatment(base='Managerial'))[T.Blue collar]\"),\n np.str_(\"C(job, contr.treatment(base='Managerial'))[T.Admin]:X2\"),\n np.str_(\"C(job, contr.treatment(base='Managerial'))[T.Blue collar]:X2\")]"
  },
  {
    "objectID": "table-layout.html#custom-model-headlines",
    "href": "table-layout.html#custom-model-headlines",
    "title": "Table Layout with PyFixest",
    "section": "Custom model headlines",
    "text": "Custom model headlines\nYou can also add custom headers for each model by passing a list of strings to the model_headers argument.\n\npf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    labels=labels,\n    model_heads=[\"US\", \"China\", \"EU\", \"US\", \"China\", \"EU\"],\n)\n\n\n\n\n\n\n\n  \n     \n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  \n  \n    US\n  \n  \n    China\n  \n  \n    EU\n  \n  \n    US\n  \n  \n    China\n  \n  \n    EU\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nOr change the ordering of headlines having headlines first and then dependent variables using the head_order argument. “hd” stands for headlines then dependent variables, “dh” for dependent variables then headlines. Assigning “d” or “h” can be used to only show dependent variables or only headlines. When head_order=“” only model numbers are shown.\n\npf.etable(\n    [fit1, fit4, fit2, fit5, fit3, fit6],\n    labels=labels,\n    model_heads=[\"US\", \"US\", \"China\", \"China\", \"EU\", \"EU\"],\n    head_order=\"hd\",\n)\n\n\n\n\n\n\n\n  \n     \n  \n  \n    US\n  \n  \n    China\n  \n  \n    EU\n  \n\n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -1.267***  (0.211)\n    -0.924***  (0.056)\n    -1.232***  (0.211)\n    -0.924***  (0.056)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.131*  (0.056)\n    -0.174***  (0.015)\n    -0.118*  (0.056)\n    -0.185***  (0.025)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    \n    \n    0.011  (0.019)\n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    -\n    x\n    x\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    998\n    997\n    998\n    997\n    998\n  \n  \n    R2\n    0.489\n    0.12\n    0.659\n    0.172\n    0.659\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\nRemove the dependent variables from the headers:\n\npf.etable(\n    [fit1, fit4, fit2, fit5, fit3, fit6],\n    labels=labels,\n    model_heads=[\"US\", \"US\", \"China\", \"China\", \"EU\", \"EU\"],\n    head_order=\"\",\n)\n\n\n\n\n\n\n\n  \n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -1.267***  (0.211)\n    -0.924***  (0.056)\n    -1.232***  (0.211)\n    -0.924***  (0.056)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.131*  (0.056)\n    -0.174***  (0.015)\n    -0.118*  (0.056)\n    -0.185***  (0.025)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    \n    \n    0.011  (0.019)\n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    -\n    x\n    x\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    998\n    997\n    998\n    997\n    998\n  \n  \n    R2\n    0.489\n    0.12\n    0.659\n    0.172\n    0.659\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#further-custom-model-information",
    "href": "table-layout.html#further-custom-model-information",
    "title": "Table Layout with PyFixest",
    "section": "Further custom model information",
    "text": "Further custom model information\nYou can add further custom model statistics/information to the bottom of the table by using the custom_stats argument to which you pass a dictionary with the name of the row and lists of values. The length of the lists must be equal to the number of models.\n\npf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    labels=labels,\n    custom_model_stats={\n        \"Number of Clusters\": [42, 42, 42, 37, 37, 37],\n        \"Additional Info\": [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"],\n    },\n)\n\n\n\n\n\n\n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Number of Clusters\n    42\n    42\n    42\n    37\n    37\n    37\n  \n  \n    Additional Info\n    A\n    A\n    B\n    B\n    C\n    C\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#custom-table-notes",
    "href": "table-layout.html#custom-table-notes",
    "title": "Table Layout with PyFixest",
    "section": "Custom table notes",
    "text": "Custom table notes\nYou can replace the default table notes with your own notes using the notes argument.\n\nmynotes = \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\"\npf.etable(\n    [fit1, fit4, fit2, fit5, fit3, fit6],\n    labels=labels,\n    model_heads=[\"US\", \"US\", \"China\", \"China\", \"EU\", \"EU\"],\n    head_order=\"hd\",\n    notes=mynotes,\n)\n\n\n\n\n\n\n\n  \n     \n  \n  \n    US\n  \n  \n    China\n  \n  \n    EU\n  \n\n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -1.267***  (0.211)\n    -0.924***  (0.056)\n    -1.232***  (0.211)\n    -0.924***  (0.056)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.131*  (0.056)\n    -0.174***  (0.015)\n    -0.118*  (0.056)\n    -0.185***  (0.025)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    \n    \n    0.011  (0.019)\n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year\n    -\n    -\n    x\n    x\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    998\n    997\n    998\n    997\n    998\n  \n  \n    R2\n    0.489\n    0.12\n    0.659\n    0.172\n    0.659\n    0.172\n  \n\n  \n  \n  \n    Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
  },
  {
    "objectID": "table-layout.html#publication-ready-latex-tables",
    "href": "table-layout.html#publication-ready-latex-tables",
    "title": "Table Layout with PyFixest",
    "section": "Publication-ready LaTex tables",
    "text": "Publication-ready LaTex tables\nWith few lines of code you thus obtain a publication-ready latex table:\n\ntab = pf.etable(\n    [fit1, fit4, fit2, fit5, fit3, fit6],\n    labels=labels,\n    model_heads=[\"US\", \"US\", \"China\", \"China\", \"EU\", \"EU\"],\n    head_order=\"hd\",\n    type=\"tex\",\n    notes=mynotes,\n    show_fe=True,\n    show_se_type=False,\n    custom_model_stats={\n        \"Number of Clusters\": [42, 42, 42, 37, 37, 37],\n    },\n)\n\n# Compile latex to pdf & display a button with the hyperlink to the pdf\nrun = False\nif run:\n    make_pdf(tab, \"latexdocs/SampleTableDoc2\")\ndisplay(FileLink(\"latexdocs/SampleTableDoc2.pdf\"))\n\nlatexdocs/SampleTableDoc2.pdf"
  },
  {
    "objectID": "table-layout.html#basic-usage-of-dtable",
    "href": "table-layout.html#basic-usage-of-dtable",
    "title": "Table Layout with PyFixest",
    "section": "Basic Usage of DTable",
    "text": "Basic Usage of DTable\nSpecify the variables you want to display the descriptive statistics for. You can also use a dictionary to rename the variables and add a caption.\n\nDTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    labels=labels,\n    caption=\"Descriptive statistics\",\n    digits=2,\n)\n\n\n        \n        \n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  N\n  Mean\n  Std. Dev.\n\n\n\n  \n    Wage\n    999.00\n    -0.13\n    2.30\n  \n  \n    Wealth\n    1,000\n    -0.31\n    5.58\n  \n  \n    Age\n    999.00\n    1.04\n    0.81\n  \n  \n    Years of Schooling\n    1,000\n    -0.13\n    3.05\n  \n\n  \n  \n  \n    \n  \n\n\n\n\n\n\n        \n\n\n\n\n\nChoose the set of statistics to be displayed with stats. You can use any pandas aggregation functions.\n\nDTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    stats=[\"count\", \"mean\", \"std\", \"min\", \"max\"],\n    labels=labels,\n    caption=\"Descriptive statistics\",\n)\n\n\n        \n        \n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  N\n  Mean\n  Std. Dev.\n  Min\n  Max\n\n\n\n  \n    Wage\n    999.00\n    -0.13\n    2.30\n    -6.54\n    6.91\n  \n  \n    Wealth\n    1,000\n    -0.31\n    5.58\n    -16.97\n    17.16\n  \n  \n    Age\n    999.00\n    1.04\n    0.81\n    0.00\n    2.00\n  \n  \n    Years of Schooling\n    1,000\n    -0.13\n    3.05\n    -9.67\n    10.99"
  },
  {
    "objectID": "table-layout.html#summarize-by-characteristics-in-columns-and-rows",
    "href": "table-layout.html#summarize-by-characteristics-in-columns-and-rows",
    "title": "Table Layout with PyFixest",
    "section": "Summarize by characteristics in columns and rows",
    "text": "Summarize by characteristics in columns and rows\nYou can summarize by characteristics using the bycol argument when groups are to be displayed in columns. When the number of observations is the same for all variables in a group, you can also opt to display the number of observations only once for each group byin a separate line at the bottom of the table with counts_row_below==True.\n\n# Generate some categorial data\ndata[\"country\"] = np.random.choice([\"US\", \"EU\"], data.shape[0])\ndata[\"occupation\"] = np.random.choice([\"Blue collar\", \"White collar\"], data.shape[0])\n\n# Drop nan values to have balanced data\ndata.dropna(inplace=True)\n\nDTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    labels=labels,\n    bycol=[\"country\", \"occupation\"],\n    stats=[\"count\", \"mean\", \"std\"],\n    caption=\"Descriptive statistics\",\n    stats_labels={\"count\": \"Number of observations\"},\n    counts_row_below=True,\n)\n\n\n        \n        \n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n     \n  \n  \n    EU\n  \n  \n    US\n  \n\n\n  \n  \n    Blue collar\n  \n  \n    White collar\n  \n  \n    Blue collar\n  \n  \n    White collar\n  \n\n\n  Mean\n  Std. Dev.\n  Mean\n  Std. Dev.\n  Mean\n  Std. Dev.\n  Mean\n  Std. Dev.\n\n\n\n  \n    stats\n  \n  \n    Wage\n    -0.12\n    2.52\n    -0.09\n    2.30\n    -0.12\n    2.18\n    -0.17\n    2.24\n  \n  \n    Wealth\n    -0.13\n    5.74\n    -0.60\n    5.21\n    -0.30\n    5.96\n    -0.24\n    5.42\n  \n  \n    Age\n    1.01\n    0.79\n    1.09\n    0.82\n    1.04\n    0.83\n    1.03\n    0.79\n  \n  \n    Years of Schooling\n    0.16\n    3.07\n    -0.15\n    3.02\n    -0.10\n    3.03\n    -0.42\n    3.06\n  \n  \n    nobs\n  \n  \n    Number of observations\n    247.00\n    \n    247.00\n    \n    259.00\n    \n    244.00\n    \n  \n\n  \n  \n  \n    \n  \n\n\n\n\n\n\n        \n\n\n\n\n\nYou can also use custom aggregation functions to compute further statistics or affect how statistics are presented. Pyfixest provides two such functions mean_std and mean_newline_std which compute the mean and standard deviation and display both the same cell (either with line break between them or not). This allows to have more compact tables when you want to show statistics for many characteristcs in the columns.\nYou can also hide the display of the statistics labels in the header with hide_stats_labels=True. In that case a table note will be added naming the statistics displayed using its label (if you have not provided a custom note).\n\nDTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    labels=labels,\n    bycol=[\"country\", \"occupation\"],\n    stats=[\"mean_newline_std\", \"count\"],\n    caption=\"Descriptive statistics\",\n    stats_labels={\"count\": \"Number of observations\"},\n    counts_row_below=True,\n    hide_stats=True,\n)\n\n\n        \n        \n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  \n    EU\n  \n  \n    US\n  \n\n\n  Blue collar\n  White collar\n  Blue collar\n  White collar\n\n\n\n  \n    stats\n  \n  \n    Wage\n    -0.12(2.52)\n    -0.09(2.30)\n    -0.12(2.18)\n    -0.17(2.24)\n  \n  \n    Wealth\n    -0.13(5.74)\n    -0.60(5.21)\n    -0.30(5.96)\n    -0.24(5.42)\n  \n  \n    Age\n    1.01(0.79)\n    1.09(0.82)\n    1.04(0.83)\n    1.03(0.79)\n  \n  \n    Years of Schooling\n    0.16(3.07)\n    -0.15(3.02)\n    -0.10(3.03)\n    -0.42(3.06)\n  \n  \n    nobs\n  \n  \n    Number of observations\n    247\n    247\n    259\n    244\n  \n\n  \n  \n  \n    Note: Displayed statistics are Mean (Std. Dev.).\n  \n\n\n\n\n\n\n        \n\n\n\n\n\nYou can also split by characteristics in both columns and rows. Note that you can only use one grouping variable in rows, but several in columns (as shown above).\n\nDTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    labels=labels,\n    bycol=[\"country\"],\n    byrow=\"occupation\",\n    stats=[\"count\", \"mean\", \"std\"],\n    caption=\"Descriptive statistics\",\n)\n\n\n        \n        \n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  \n    EU\n  \n  \n    US\n  \n\n\n  N\n  Mean\n  Std. Dev.\n  N\n  Mean\n  Std. Dev.\n\n\n\n  \n    Blue collar\n  \n  \n    Wage\n    247.00\n    -0.12\n    2.52\n    259.00\n    -0.12\n    2.18\n  \n  \n    Wealth\n    247.00\n    -0.13\n    5.74\n    259.00\n    -0.30\n    5.96\n  \n  \n    Age\n    247.00\n    1.01\n    0.79\n    259.00\n    1.04\n    0.83\n  \n  \n    Years of Schooling\n    247.00\n    0.16\n    3.07\n    259.00\n    -0.10\n    3.03\n  \n  \n    White collar\n  \n  \n    Wage\n    247.00\n    -0.09\n    2.30\n    244.00\n    -0.17\n    2.24\n  \n  \n    Wealth\n    247.00\n    -0.60\n    5.21\n    244.00\n    -0.24\n    5.42\n  \n  \n    Age\n    247.00\n    1.09\n    0.82\n    244.00\n    1.03\n    0.79\n  \n  \n    Years of Schooling\n    247.00\n    -0.15\n    3.02\n    244.00\n    -0.42\n    3.06\n  \n\n  \n  \n  \n    \n  \n\n\n\n\n\n\n        \n\n\n\n\n\nAnd you can again export descriptive statistics tables also to LaTex:\n\ndtab = DTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    labels=labels,\n    bycol=[\"country\"],\n    byrow=\"occupation\",\n    stats=[\"count\", \"mean\", \"std\"],\n    type=\"tex\",\n)\n\nrun = False\nif run:\n    make_pdf(dtab, \"latexdocs/SampleTableDoc3\")\ndisplay(FileLink(\"latexdocs/SampleTableDoc3.pdf\"))\n\nlatexdocs/SampleTableDoc3.pdf"
  },
  {
    "objectID": "table-layout.html#example-styling",
    "href": "table-layout.html#example-styling",
    "title": "Table Layout with PyFixest",
    "section": "Example Styling",
    "text": "Example Styling\n\n(\n    pf.etable([fit1, fit2, fit3, fit4, fit5, fit6])\n    .tab_options(\n        column_labels_background_color=\"cornsilk\",\n        stub_background_color=\"whitesmoke\",\n    )\n    .tab_style(\n        style=style.fill(color=\"mistyrose\"),\n        locations=loc.body(columns=\"(3)\", rows=[\"X2\"]),\n    )\n)\n\n\n\n\n\n\n\n  \n  \n    Y\n  \n  \n    Y2\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    X1\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    X2\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    X1 × X2\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    f1\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    f2\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "table-layout.html#defining-table-styles-some-examples",
    "href": "table-layout.html#defining-table-styles-some-examples",
    "title": "Table Layout with PyFixest",
    "section": "Defining Table Styles: Some Examples",
    "text": "Defining Table Styles: Some Examples\nYou can easily define table styles that you can apply to all tables in your project. Just define a dictionary with the respective values for the tab options (see the Great Tables documentation) and use the style with .tab_options(**style_dict).\n\nstyle_print = {\n    \"table_font_size\": \"12px\",\n    \"heading_title_font_size\": \"12px\",\n    \"source_notes_font_size\": \"8px\",\n    \"data_row_padding\": \"3px\",\n    \"column_labels_padding\": \"3px\",\n    \"row_group_border_top_style\": \"hidden\",\n    \"table_body_border_top_style\": \"None\",\n    \"table_body_border_bottom_width\": \"1px\",\n    \"column_labels_border_top_width\": \"1px\",\n    \"table_width\": \"14cm\",\n}\n\n\nstyle_presentation = {\n    \"table_font_size\": \"16px\",\n    \"table_font_color_light\": \"white\",\n    \"table_body_border_top_style\": \"hidden\",\n    \"table_body_border_bottom_style\": \"hidden\",\n    \"heading_title_font_size\": \"18px\",\n    \"source_notes_font_size\": \"12px\",\n    \"data_row_padding\": \"3px\",\n    \"column_labels_padding\": \"6px\",\n    \"column_labels_background_color\": \"midnightblue\",\n    \"stub_background_color\": \"whitesmoke\",\n    \"row_group_background_color\": \"whitesmoke\",\n    \"table_background_color\": \"whitesmoke\",\n    \"heading_background_color\": \"white\",\n    \"source_notes_background_color\": \"white\",\n    \"column_labels_border_bottom_color\": \"white\",\n    \"column_labels_font_weight\": \"bold\",\n    \"row_group_font_weight\": \"bold\",\n    \"table_width\": \"18cm\",\n}\n\n\nt1 = DTable(\n    data,\n    vars=[\"Y\", \"Y2\", \"X1\", \"X2\"],\n    stats=[\"count\", \"mean\", \"std\", \"min\", \"max\"],\n    labels=labels,\n    caption=\"Descriptive statistics\",\n)\n\nt2 = pf.etable(\n    [fit1, fit2, fit3, fit4, fit5, fit6],\n    labels=labels,\n    show_se=False,\n    felabels={\"f1\": \"Industry Fixed Effects\", \"f2\": \"Year Fixed Effects\"},\n    caption=\"Regression results\",\n)\n\n\ndisplay(t1.make(type=\"gt\", gt_style=style_print))\ndisplay(t2.tab_options(**style_print))\n\n\n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  N\n  Mean\n  Std. Dev.\n  Min\n  Max\n\n\n\n  \n    Wage\n    997.00\n    -0.13\n    2.31\n    -6.54\n    6.91\n  \n  \n    Wealth\n    997.00\n    -0.32\n    5.59\n    -16.97\n    17.16\n  \n  \n    Age\n    997.00\n    1.04\n    0.81\n    0.00\n    2.00\n  \n  \n    Years of Schooling\n    997.00\n    -0.13\n    3.05\n    -9.67\n    10.99\n  \n\n  \n  \n  \n    \n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n  \n    Regression results\n  \n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry Fixed Effects\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year Fixed Effects\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)\n  \n\n\n\n\n\n\n        \n\n\n\nstyle_printDouble = {\n    \"table_font_size\": \"12px\",\n    \"heading_title_font_size\": \"12px\",\n    \"source_notes_font_size\": \"8px\",\n    \"data_row_padding\": \"3px\",\n    \"column_labels_padding\": \"3px\",\n    \"table_body_border_bottom_style\": \"double\",\n    \"column_labels_border_top_style\": \"double\",\n    \"column_labels_border_bottom_width\": \"0.5px\",\n    \"row_group_border_top_style\": \"hidden\",\n    \"table_body_border_top_style\": \"None\",\n    \"table_width\": \"14cm\",\n}\ndisplay(t1.make(type=\"gt\", gt_style=style_printDouble))\ndisplay(t2.tab_options(**style_printDouble))\n\n\n\n\n\n\n  \n    Descriptive statistics\n  \n\n  \n  N\n  Mean\n  Std. Dev.\n  Min\n  Max\n\n\n\n  \n    Wage\n    997.00\n    -0.13\n    2.31\n    -6.54\n    6.91\n  \n  \n    Wealth\n    997.00\n    -0.32\n    5.59\n    -16.97\n    17.16\n  \n  \n    Age\n    997.00\n    1.04\n    0.81\n    0.00\n    2.00\n  \n  \n    Years of Schooling\n    997.00\n    -0.13\n    3.05\n    -9.67\n    10.99\n  \n\n  \n  \n  \n    \n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n  \n    Regression results\n  \n\n  \n  \n    Wage\n  \n  \n    Wealth\n  \n\n\n  (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n\n\n\n  \n    coef\n  \n  \n    Age\n    -0.95***  (0.066)\n    -0.924***  (0.056)\n    -0.924***  (0.056)\n    -1.267***  (0.211)\n    -1.232***  (0.211)\n    -1.231***  (0.211)\n  \n  \n    Years of Schooling\n    -0.174***  (0.018)\n    -0.174***  (0.015)\n    -0.185***  (0.025)\n    -0.131*  (0.056)\n    -0.118*  (0.056)\n    -0.074  (0.094)\n  \n  \n    Age × Years of Schooling\n    \n    \n    0.011  (0.019)\n    \n    \n    -0.041  (0.071)\n  \n  \n    fe\n  \n  \n    Industry Fixed Effects\n    x\n    x\n    x\n    x\n    x\n    x\n  \n  \n    Year Fixed Effects\n    -\n    x\n    x\n    -\n    x\n    x\n  \n  \n    stats\n  \n  \n    Observations\n    997\n    997\n    997\n    998\n    998\n    998\n  \n  \n    R2\n    0.489\n    0.659\n    0.659\n    0.12\n    0.172\n    0.172\n  \n\n  \n  \n  \n    Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient   (Std. Error)"
  },
  {
    "objectID": "brave_true.html",
    "href": "brave_true.html",
    "title": "Causal Inference for the Brave and True",
    "section": "",
    "text": "import pandas as pd\n\nimport pyfixest as pf\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\nChapter 14: Panel Data and Fixed Effects\nIn this example we replicate the results of the great (freely available reference!) Causal Inference for the Brave and True - Chapter 14. Please refer to the original text for a detailed explanation of the data.\n\ndata_path = \"https://raw.githubusercontent.com/bashtage/linearmodels/main/linearmodels/datasets/wage_panel/wage_panel.csv.bz2\"\ndata_df = pd.read_csv(data_path)\n\ndata_df.head()\n\n\n\n\n\n\n\n\nnr\nyear\nblack\nexper\nhisp\nhours\nmarried\neduc\nunion\nlwage\nexpersq\noccupation\n\n\n\n\n0\n13\n1980\n0\n1\n0\n2672\n0\n14\n0\n1.197540\n1\n9\n\n\n1\n13\n1981\n0\n2\n0\n2320\n0\n14\n1\n1.853060\n4\n9\n\n\n2\n13\n1982\n0\n3\n0\n2940\n0\n14\n0\n1.344462\n9\n9\n\n\n3\n13\n1983\n0\n4\n0\n2960\n0\n14\n0\n1.433213\n16\n9\n\n\n4\n13\n1984\n0\n5\n0\n3071\n0\n14\n0\n1.568125\n25\n5\n\n\n\n\n\n\n\nWe have a classical panel data set with units (nr) and time (year).\nWe are interested in estimating the effect of marriage status on log wage, using a set of controls (union, hours) and individual (nr) and year fixed effects.\n\npanel_fit = pf.feols(\n    fml=\"lwage ~ married + expersq + union + hours | nr + year\",\n    data=data_df,\n    vcov={\"CRV1\": \"nr + year\"},\n    demeaner_backend=\"rust\",\n)\n\n\npf.etable(panel_fit)\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n(1)\n\n\n\n\ncoef\n\n\nmarried\n0.048*\n(0.018)\n\n\nexpersq\n-0.006***\n(0.001)\n\n\nunion\n0.073*\n(0.023)\n\n\nhours\n-0.000**\n(0.000)\n\n\nfe\n\n\nnr\nx\n\n\nyear\nx\n\n\nstats\n\n\nObservations\n4360\n\n\nS.E. type\nby: nr+year\n\n\nR2\n0.631\n\n\nR2 Within\n0.047\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nWe obtain the same results as in the book!"
  },
  {
    "objectID": "pyfixest-gpu-cupy.html",
    "href": "pyfixest-gpu-cupy.html",
    "title": "PyFixest on the GPU via CuPy",
    "section": "",
    "text": "Besides JAX, it is possible to run PyFixest on the GPU via CuPy (linux and windows). Instead of applying the alternating projections algorithm to demean fixed effects, CuPy works with sparse matrices and the sparse LSMR solver (as is e.g. available in scipy).This strategy is amenable for GPU acceleration, and for problems where the standard demeaner struggles to converge, this strategy can lead to significant speedups if paired with a GPU.Note that for smaller and more well-behaved problems, running the alternating projections algorithm on the CPU via numba or rust usually seems to work better: Benchmark Hardware Specifications:- CPU: x86_64, 8 physical cores @ 3.2 GHz, 44 GB RAM- GPU: NVIDIA RTX A6000, 48 GB memory, Compute Capability 8.6"
  },
  {
    "objectID": "pyfixest-gpu-cupy.html#installation",
    "href": "pyfixest-gpu-cupy.html#installation",
    "title": "PyFixest on the GPU via CuPy",
    "section": "Installation",
    "text": "Installation\nTo run pyfixest via cup on the GPU, you need to install the required dependency:\n# For CUDA 11.x, 12.x, 13.x\npip install cupy-cuda11x\npip install cupy-cuda12x\npip install cupy-cuda13x"
  },
  {
    "objectID": "mixtape.html",
    "href": "mixtape.html",
    "title": "The Mixtape with PyFixest",
    "section": "",
    "text": "In this notebook, we translate some of the Python code in Scott Cunningham’s mixtape to PyFixest.\nimport numpy as np\nimport pandas as pd\n\nimport pyfixest as pf\n\n%config InlineBackend.figure_format = \"retina\""
  },
  {
    "objectID": "mixtape.html#chapter-8-panel-data",
    "href": "mixtape.html#chapter-8-panel-data",
    "title": "The Mixtape with PyFixest",
    "section": "Chapter 8: Panel Data",
    "text": "Chapter 8: Panel Data\nInstead of demeaning by hand and then fitting the model via statsmodels, we just let PyFixest do all the work for us.\n\n# read the data from github & load into pandas\nurl = \"https://raw.githubusercontent.com/scunning1975/mixtape/master/sasp_panel.dta\"\nsasp = pd.read_stata(url)\nsasp.head()\n\n\n\n\n\n\n\n\nid\nsession\nage\nage_cl\nappearance_cl\nbmi\nschooling\nasq_cl\nprovider_second\nasian_cl\n...\nhispanic\nother\nwhite\nasq\ncohab\nmarried\ndivorced\nseparated\nnevermarried\nwidowed\n\n\n\n\n0\n243.0\n2.0\n27.0\n30.0\n5.0\nNaN\n11.0\n900.0\n1. No\n0.0\n...\n0.0\n0.0\n0.0\n729.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n397.0\n4.0\n28.0\n56.0\n5.0\n28.971931\n16.0\n3136.0\n1. No\n0.0\n...\n0.0\n0.0\n1.0\n784.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n598.0\n4.0\n50.0\n52.0\n6.0\n21.453857\n16.0\n2704.0\n1. No\n0.0\n...\n0.0\n0.0\n1.0\n2500.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n3\n28.0\n1.0\n41.0\n72.0\n5.0\n24.028320\n12.0\n5184.0\n1. No\n0.0\n...\n0.0\n0.0\n1.0\n1681.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n28.0\n4.0\n41.0\n46.0\n8.0\n24.028320\n12.0\n2116.0\n1. No\n0.0\n...\n0.0\n0.0\n1.0\n1681.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n# some initial data cleaning\nsasp = sasp.dropna()\n# order by id and session\nsasp.sort_values(\"id\", inplace=True)\n\n# create balanced panel\ntimes = len(sasp.session.unique())\nin_all_times = (\n    sasp.groupby(\"id\")[\"session\"].apply(lambda x: len(x) == times).reset_index()\n)\nin_all_times.rename(columns={\"session\": \"in_all_times\"}, inplace=True)\nbalanced_sasp = pd.merge(in_all_times, sasp, how=\"left\", on=\"id\")\nbalanced_sasp = balanced_sasp[balanced_sasp.in_all_times]\n\nprovider_second = np.zeros(balanced_sasp.shape[0])\nprovider_second[balanced_sasp.provider_second == \"2. Yes\"] = 1\nbalanced_sasp.provider_second = provider_second\n\n\n# define formulas\n\ncovars = \"\"\"\n    age + asq + bmi + hispanic + black + other + asian + schooling + cohab +\n            married + divorced + separated + age_cl + unsafe + llength + reg + asq_cl +\n            appearance_cl + provider_second + asian_cl + black_cl + hispanic_cl +\n           othrace_cl + hot + massage_cl\n    \"\"\"\n\n# we fit on all covariates\nfml_pooled = f\"lnw ~ {covars}\"\n# we fit on all covariates and add one-hot encoded id fixed effects\nfml_onehot = f\"lnw ~  {covars} + C(id)\"\n# we fit on all covariates and swipe out the fixed effects (i.e. we apply the within transformation via pyfixest.feols)\nfml_fe = f\"lnw ~ {covars} | id\"\n\n\n%%capture\nfit_pooled = pf.feols(fml=fml_pooled, data=balanced_sasp, vcov={\"CRV1\": \"id\"})\nfit_fe = pf.feols(fml=fml_fe, data=balanced_sasp, vcov={\"CRV1\": \"id\"})\n\n\npf.etable(\n    [fit_pooled, fit_fe],\n    model_heads=[\"POLS\", \"FE\"],\n    keep=[\"unsafe\", \"llength\", \"reg\"],\n    labels={\n        \"unsafe\": \"Unprotected sex with client of any kind\",\n        \"llength\": \"Ln(Length)\",\n        \"reg\": \"Client was a Regular\",\n    },\n    digits=6,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n \nlnw\n\n\n\nPOLS\nFE\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nUnprotected sex with client of any kind\n0.013407\n(0.042455)\n0.051034\n(0.028283)\n\n\nLn(Length)\n-0.308251***\n(0.040905)\n-0.434506***\n(0.024323)\n\n\nClient was a Regular\n-0.047007\n(0.033282)\n-0.037341*\n(0.018761)\n\n\nfe\n\n\nid\n-\nx\n\n\nstats\n\n\nObservations\n1028\n1028\n\n\nS.E. type\nby: id\nby: id\n\n\nR2\n0.302643\n0.832214\n\n\nR2 Within\n-\n0.515959\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nOur point estimates match the Stata results that Scott reports in the mixtape exactly. The standard errors differ slightly due to differences in small sample adjustments in Stata and Pyfixest. See here for an overview of how pyfixest handles small sample adjustments (tldr - exactly like r-fixest)."
  },
  {
    "objectID": "mixtape.html#chapter-9-difference-in-differences",
    "href": "mixtape.html#chapter-9-difference-in-differences",
    "title": "The Mixtape with PyFixest",
    "section": "Chapter 9: Difference-in-Differences",
    "text": "Chapter 9: Difference-in-Differences\n\nCode Example 1\n\nabortion = pd.read_stata(\n    \"https://raw.githubusercontent.com/scunning1975/mixtape/master/abortion.dta\"\n)\nabortion = abortion[~pd.isnull(abortion.lnr)]\nabortion_bf15 = abortion[abortion.bf15 == 1]\n# pf throws error when weights are 0\nabortion_bf15 = abortion_bf15[abortion_bf15.totpop &gt; 0]\nabortion_bf15[\"year\"] = abortion_bf15[\"year\"].astype(int)\nabortion_bf15.head()\n\n\n\n\n\n\n\n\nfip\nage\nrace\nyear\nsex\ntotcase\ntotpop\nrate\ntotrate\nid\n...\nfemale\nlnr\nt\nyounger\nfa\npi\nwm15\nwf15\nbm15\nbf15\n\n\n\n\n19\n1.0\n15.0\n2.0\n1985\n2\n5683.0\n106187\n6527.500000\n5351.899902\n14.0\n...\n1.0\n8.783779\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n39\n1.0\n15.0\n2.0\n1986\n2\n5344.0\n106831\n6351.200195\n5002.299805\n14.0\n...\n1.0\n8.756399\n2.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n71\n1.0\n15.0\n2.0\n1987\n2\n4983.0\n106496\n5759.100098\n4679.000000\n14.0\n...\n1.0\n8.658537\n3.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n89\n1.0\n15.0\n2.0\n1988\n2\n5276.0\n105238\n6139.600098\n5013.399902\n14.0\n...\n1.0\n8.722515\n4.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n106\n1.0\n15.0\n2.0\n1989\n2\n5692.0\n102956\n5951.500000\n5528.600098\n14.0\n...\n1.0\n8.691399\n5.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 39 columns\n\n\n\n\n# we use the i() operator pyfixest provides, as it allows us to easily set the\n# reference year, and works smoothly with the iplot() method\n\nfml = \"\"\"lnr ~ i(year, repeal, ref = 1985) + C(repeal) + C(year) + C(fip)\n        + acc + ir + pi + alcohol + crack + poverty + income + ur\n\"\"\"\n\nfit = pf.feols(fml=fml, data=abortion_bf15, weights=\"totpop\", vcov={\"CRV1\": \"fip\"})\n\npf.iplot(\n    fit,\n    coord_flip=False,\n    plot_backend=\"matplotlib\",\n    title=\"Event Study Estimate\",\n    cat_template=\"{value}\",\n)\n\nC:\\Users\\alexa\\Documents\\pyfixest\\pyfixest\\estimation\\feols_.py:2759: UserWarning: \n            1 variables dropped due to multicollinearity.\n            The following variables are dropped: ['C(fip)[T.53.0]'].\n            \n  warnings.warn("
  },
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "import pyfixest as pf\nfrom pyfixest.report.utils import rename_categoricals\n\ndf = pf.get_data()\n\nfit1 = pf.feols(\"Y ~ X1\", data = df)\nfit2 = pf.feols(\"Y ~ X1 + X2\", data = df)\nfit3 = pf.feols(\"Y ~ X1 + X2 | f1\", data = df)\n\n\n            \n            \n            \n\n\n\n            \n            \n            \n\n\n\n\nThe table functionality in pyfixest now uses maketables. maketables is a spin-off of pyfixest internal functions, but supports more packages in the Python eco-system (e.g. statsmodels and linearmorels). Due to it’s close connection to pyfixest, the API of pf.etable() remains unchanged. Changes:\n\npf.etable() now uses maketables.ETable internally. The API remains unchanged for backward compatibility.\nBecause the function is not at the core of pyixest functionality, we will deprecate pf.dtable(). A FutureWarning is now emitted. The function has been moved to maketables and can be used by calling maketables.DTable() directly.\nThe same applies for pf.make_table(), which has been an internal utility function to create tables. An equivalent function now lives in maketables.MTable().\nThe great_tables dependency has been replaced with maketables.\n\nMigration guide:\n# dtable migration\n# Before:\npf.dtable(df, vars=[\"Y\", \"X1\"])\n# After:\nimport maketables\nmaketables.DTable(df, vars=[\"Y\", \"X1\"])\n\n# make_table migration\n# Before:\npf.make_table(df, type=\"gt\", caption=\"My Table\")\n# After:\nimport maketables\nmaketables.MTable(df, caption=\"My Table\").make(type=\"gt\")\n\n\n\n\nAdds the following statistics to the Fepois class: _loglik, _loglik_null, _pseudo_r2 for regression without weights.\nSet the default parameters for the MAP algorithm to a tolerance of 1e-06 and maximum number of iterations of 10_000.\nAdds support for weights for poisson regression via feopis().\n\n\n\n\nfeglm() now supports high-dimensional fixed effects for logit, probit, and gaussian families! Fixed effects are specified after the | symbol, just like in feols() and fepois().\n\nimport numpy as np\n\ndata_glm = pf.get_data()\ndata_glm[\"Y\"] = np.where(data_glm[\"Y\"] &gt; 0, 1, 0)\n\n# Logit with fixed effects\nfit_logit = pf.feglm(\"Y ~ X1 + X2 | f1\", data=data_glm, family=\"logit\")\nfit_logit.summary()\n\n###\n\nEstimation:  Logit\nDep. var.: Y, Fixed effects: f1\nInference:  iid\nObservations:  998\n\n| Coefficient   |   Estimate |   Std. Error |   t value |   Pr(&gt;|t|) |   2.5% |   97.5% |\n|:--------------|-----------:|-------------:|----------:|-----------:|-------:|--------:|\n| X1            |     -1.016 |        0.109 |    -9.297 |      0.000 | -1.231 |  -0.802 |\n| X2            |     -0.166 |        0.028 |    -5.831 |      0.000 | -0.222 |  -0.110 |\n---\nDeviance: 961.445 \n\n\nFixed effects estimation via demeaning produces identical point estimates as one-hot encoding the fixed effects via C():\n\n# Compare FE demeaning vs one-hot encoding\nfit_logit_onehot = pf.feglm(\"Y ~ X1 + X2 + C(f1)\", data=data_glm, family=\"logit\")\n\npf.etable([fit_logit, fit_logit_onehot])\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n-1.016***\n(0.109)\n-1.016***\n(0.109)\n\n\nX2\n-0.166***\n(0.028)\n-0.166***\n(0.028)\n\n\nf1=1.0\n\n2.653***\n(0.61)\n\n\nf1=2.0\n\n-2.673**\n(0.843)\n\n\nf1=3.0\n\n-0.236\n(0.569)\n\n\nf1=4.0\n\n-2.385**\n(0.745)\n\n\nf1=5.0\n\n1.653**\n(0.628)\n\n\nf1=6.0\n\n-1.587*\n(0.691)\n\n\nf1=7.0\n\n0.319\n(0.542)\n\n\nf1=8.0\n\n-0.061\n(0.531)\n\n\nf1=9.0\n\n0.448\n(0.529)\n\n\nf1=10.0\n\n0.505\n(0.528)\n\n\nf1=11.0\n\n1.375*\n(0.568)\n\n\nf1=12.0\n\n0.05\n(0.562)\n\n\nf1=13.0\n\n0.979\n(0.567)\n\n\nf1=14.0\n\n-0.602\n(0.58)\n\n\nf1=15.0\n\n0.942\n(0.561)\n\n\nf1=16.0\n\n2.678***\n(0.644)\n\n\nf1=17.0\n\n-0.862\n(0.621)\n\n\nf1=18.0\n\n1.514**\n(0.561)\n\n\nf1=19.0\n\n0.453\n(0.56)\n\n\nf1=20.0\n\n2.225***\n(0.579)\n\n\nf1=21.0\n\n-2.048**\n(0.682)\n\n\nf1=22.0\n\n-1.482*\n(0.607)\n\n\nf1=23.0\n\n-1.125\n(0.624)\n\n\nf1=24.0\n\n-0.843\n(0.573)\n\n\nf1=25.0\n\n-1.485\n(0.762)\n\n\nf1=26.0\n\n0.785\n(0.578)\n\n\nf1=27.0\n\n-0.488\n(0.578)\n\n\nf1=28.0\n\n1.815***\n(0.54)\n\n\nf1=29.0\n\n0.825\n(0.573)\n\n\nIntercept\n\n0.631\n(0.393)\n\n\nfe\n\n\nf1\nx\n-\n\n\nstats\n\n\nObservations\n998\n998\n\n\nR2\n-\n-\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\nNote that standard errors may differ between the two approaches due to different degrees of freedom adjustments.\nAll three GLM families (gaussian, logit, probit) support fixed effects:\n\nfit_gaussian = pf.feglm(\"Y ~ X1 + X2 | f1\", data=data_glm, family=\"gaussian\")\nfit_probit = pf.feglm(\"Y ~ X1 + X2 | f1\", data=data_glm, family=\"probit\")\n\npf.etable([fit_gaussian, fit_logit, fit_probit])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n(3)\n\n\n\n\ncoef\n\n\nX1\n-0.166***\n(0.04)\n-1.016***\n(0.109)\n-0.588***\n(0.062)\n\n\nX2\n-0.025*\n(0.011)\n-0.166***\n(0.028)\n-0.097***\n(0.016)\n\n\nfe\n\n\nf1\nx\nx\nx\n\n\nstats\n\n\nObservations\n998\n998\n998\n\n\nR2\n-\n-\n-\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixest 0.13 has recently been released to CRAN, with a range of breaking changes. We are following these in pyfixet 0.40.0.\n\nThe default inference method is now always “iid”. Before, both fixest and pyfixest would default to cluster by the first fixest effect in the presence of fixed effect.\nThe arguments of the pf.ssc() functions have been renamed: adj becomes k_adj, fixef_k becomes k_fixef, cluster_df becomes G_df, and cluster_adj becomes G_adj. Backwards compatibility is ensured.\nfixest no longer applies the G/(G-1) small sample correction for heteroskedastic errors. The argument is now only relevant for cluster robust errors.\nIf the k_adj arg is set to True with heteroskedastic errors, the applied small sample correction now is N / (N-k) and no longer (N-1) / (N-df_k), as was previously the case and is still the case for iid and cluster robust errors.\nThe k_fixef option \"nested\" has been renamed to \"nonnested\".\nThe fixef_rm argument is changed from default \"none\" to \"singleton\" for all estimation functions. This has no impact on point estimates, but might change inference due to a smaller number of observations in the degree of freedom correction.\nThe multicollinearity default tolerance has been reduced from 1e-10 to 1e-09.\nThe attribute _dof_k has been renamed to _df_k.\n\nOther related changes: - The arguments to adjust the small samples for the wildboottest methods have been renamed to k_adj and G_adj.\n\n\n\nWe have been using pixi as our package manager for a while and are moving from a pyproject.toml to using a pixi.toml for specifying dependencies. We also install all package by default from the free and open source conda-forge. Because of some environment resolution challenges around an old version of rpy2that we need for testing pyfixest against fixest, we temporarily have dropped support for the dev and docs environments for windows. A fix is work in progress.\n\n\n\n\n\n\nWe add CuPy and SciPy Backend to run the demeaning algorithm on the GPU via the sparse LSMR solver. For problems where the standard demeaner struggles to converge, this strategy can lead to significant speedups if paired with a GPU.\n\n\n\nComplex Fixed Effects Benchmark\n\n\n\n\n\nWe now support HAC standard errors! Thanks for Daman (https://github.com/damandhaliwal) for all his work (and coping with me with the PR, which took forever to get through). We now support:\n\ntime series HAC\npanel HAC\npanel DK\n\nvariance-covariance matrices.\nFor now, we assume that the time series column is consecutive and each entry has “time delta 1”. Relaxation of this requirement are work in progress.\n\n\n\nFor easier encoding of categorical variables in relation to the iplot() and coefplot() functions, we add a new function argument, cat_template:\n\nfit_c = pf.feols(fml = \"Y ~ i(X1, f1)\", data = df)\nfit_c.iplot(cat_template = \"{variable}::{value}\")\n\n   \n   \n\n\nThis is particularly useful for Difference-in-Differences and event studies. You can find an example use case in the DiD vignette.\nGoing forward, we will deprecate the rename_event_study_coefs function, which is no longer needed (and did not work anyways).\n\n\n\nWe have reworked the Gelbach Decomposition method, with some breaking changes: by default, calling Feols.decompose() now returns a GelbachDecomposition instance - before, we’d return a pd.DataFrame. The param argument has been renamed to decomp_var, but is not yet deprecated.\nThe class now comes with new tidy() and etable() methods.\n\nfrom pyfixest.utils.dgps import gelbach_data\nimport numpy as np\nimport pyfixest as pf\n\n# Generate test data using gelbach_data\ndata = gelbach_data(nobs=500)\ndata[\"w\"] = np.random.rand(500)\nfit = pf.feols(\"y ~ x1 + x21 + x22 + x23\", data=data)\ngb = fit.decompose(decomp_var = \"x1\", x1_vars = [\"x21\"],reps = 10, nthreads = 1)\nprint(type(gb))\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]100%|██████████| 10/10 [00:00&lt;00:00, 378.43it/s]\n\n\n&lt;class 'pyfixest.estimation.decomposition.GelbachDecomposition'&gt;\n\n\n\n\n\nIt is now also possible to add background variables that are included in both the long and short regressions via the x1_vars function argument.\nWe can inspect results as a pd.DataFrame\n\ngb.tidy()\n\n\n\n\n\n\n\n\ncoefficients\nci_lower\nci_upper\npanels\n\n\n\n\ndirect_effect\n1.412165\n0.543347\n1.935669\nLevels (units)\n\n\nfull_effect\n1.041753\n0.988224\n1.094174\nLevels (units)\n\n\nexplained_effect\n0.370412\n-0.507537\n0.880713\nLevels (units)\n\n\nunexplained_effect\n1.041753\n0.988224\n1.094174\nLevels (units)\n\n\nx22\n0.061930\n-0.505729\n0.358138\nLevels (units)\n\n\nx23\n0.308483\n-0.001808\n0.522575\nLevels (units)\n\n\ndirect_effect\n1.000000\n1.000000\n1.000000\nShare of Full Effect\n\n\nfull_effect\n0.737699\n0.545886\n1.898845\nShare of Full Effect\n\n\nexplained_effect\n0.262301\n-0.898845\n0.454114\nShare of Full Effect\n\n\nunexplained_effect\n0.737699\n0.545886\n1.898845\nShare of Full Effect\n\n\nx22\n0.043855\n-0.902483\n0.184401\nShare of Full Effect\n\n\nx23\n0.218446\n-0.002201\n0.269713\nShare of Full Effect\n\n\ndirect_effect\n3.812414\n-1.167910\n39.174170\nShare of Explained Effect\n\n\nfull_effect\n2.812414\n-2.167910\n38.174170\nShare of Explained Effect\n\n\nexplained_effect\n1.000000\n1.000000\n1.000000\nShare of Explained Effect\n\n\nunexplained_effect\n2.812414\n-2.167910\n38.174170\nShare of Explained Effect\n\n\nx22\n0.167192\n-6.509153\n1.054200\nShare of Explained Effect\n\n\nx23\n0.832808\n-0.054200\n7.509153\nShare of Explained Effect\n\n\n\n\n\n\n\nor produce a GT table:\n\ngb.etable(\n    stats = \"all\",\n    caption = \"Gelbach Decomposition\"\n)\n\n\n\n\n\n\n\nGelbach Decomposition\n\n\n\nInitial Difference\nAdjusted Difference\nExplained Difference\n\n\n\n\nLevels (units)\n\n\nx1\n1.412\n1.042\n0.370\n\n\n\n[0.543, 1.936]\n[0.988, 1.094]\n[-0.508, 0.881]\n\n\nx22\n-\n-\n0.062\n\n\n\n-\n-\n[-0.506, 0.358]\n\n\nx23\n-\n-\n0.308\n\n\n\n-\n-\n[-0.002, 0.523]\n\n\nShare of Full Effect\n\n\nx1\n1.000\n0.738\n0.262\n\n\n\n-\n[0.546, 1.899]\n[-0.899, 0.454]\n\n\nx22\n-\n-\n0.044\n\n\n\n-\n-\n[-0.902, 0.184]\n\n\nx23\n-\n-\n0.218\n\n\n\n-\n-\n[-0.002, 0.270]\n\n\nShare of Explained Effect\n\n\nx1\n-\n-\n1.000\n\n\nx22\n-\n-\n0.167\n\n\n\n-\n-\n[-6.509, 1.054]\n\n\nx23\n-\n-\n0.833\n\n\n\n-\n-\n[-0.054, 7.509]\n\n\n\nDecomposition variable: x1. Control Variables: x21. CIs are computed using B = 10 bootstrap replications using iid sampling.Col 1: Adjusted Difference (by x21) - Coefficient on x1 in short regression. Col 2: Adjusted Difference - Coefficient on x1 in long regression. Col 3: Explained Difference - Difference in coefficients of x1 in short and long regression. Panel 1: Levels (units). Panel 2: Share of Full Effect: Levels normalized by coefficient of the short regression. Panel 3: Share of Explained Effect: Levels normalized by coefficient of the long regression.\n\n\n\n\n\n\n\n\n\nAs can be seen, we by default now return normalized (and not just absolute) effects.\nWe are now also supporting frequency weights for the decomposition (currently without inference).\nAdditionally, some house keeping and interal refactoring of the GelbachDecomposition class.\n\n\n\n\n\n\n\nWe have created a Rust backend for all performance critical algorithms, with pretty great performance improvements! You can use the Rust backend by setting demeaner_options = \"rust\".\n\nWe find pretty great performance improvements and want to make the Rust backend the default in PyFixest 0.31.0.\nTo back up the performance claim, here is a benchmark:\n\nimport pyfixest as pf\nimport numpy as np\nimport pandas as pd\nimport time\n\nrng = np.random.default_rng(737)\n\nN = 10_000_000\nbenchmark_data = pd.DataFrame({\n  \"Y\": rng.normal(0, 1, N),\n  \"X1\": rng.normal(0, 1, N),\n  \"X2\": rng.normal(0, 1, N),\n  \"X3\": rng.normal(0, 1, N),\n  \"f1\": rng.integers(0, 10_000, N),\n  \"f2\": rng.integers(0, 1_000, N),\n  \"f3\": rng.integers(0, 10, N),\n})\n\n# burn-in for numba\nfit_nb_warmup = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data[:100_000]\n)\n\n# benchmark for numba backend\ntic = time.time()\nfit_nb = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data\n)\ntoc = time.time()\nprint(f\"Numba backend took {toc-tic}.\")\n\n# benchmark for rust backend\ntic = time.time()\nfit_rust = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data,\n  demeaner_backend = \"rust\"\n)\ntoc = time.time()\nprint(f\"Rust backend took {toc-tic}.\")\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\nNumba backend took 4.966489553451538.\nRust backend took 6.356808662414551.\n\n\nResults are also matching =)\n\npf.etable([fit_nb, fit_rust], digits = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n0.00022257\n(0.00031662)\n0.00022257\n(0.00031662)\n\n\nX2\n0.00020980\n(0.00031647)\n0.00020980\n(0.00031647)\n\n\nX3\n0.00062145*\n(0.00031651)\n0.00062145*\n(0.00031651)\n\n\nfe\n\n\nf1\nx\nx\n\n\nf2\nx\nx\n\n\nf3\nx\nx\n\n\nstats\n\n\nObservations\n10,000,000\n10,000,000\n\n\nR2\n0.001\n0.001\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\n\nWe now support quantile regression, including a Frisch-Newton Interior Point Solver with and without preprocessing, iid, heteroskedastic and cluster robust standard errors, fast algorithms for the entire quantile regression process, and some visualisations. In particular the algorithms for the quantile regression process show excellent performance. You can learn more about all features and take a look at more systematic benchmarks in the quantreg vignette.\n\n\nN_qr = 10_000\nrng = np.random.default_rng(929)\n\ndf_qr = pd.DataFrame({\n  \"X1\": rng.normal(0, 1, N_qr),\n  \"X2\": rng.normal(0, 1, N_qr)\n})\ndf_qr[\"Y\"] = -0.5 + -2 * df_qr[\"X1\"] + 1.9 * df_qr[\"X1\"] ** 4 + df_qr[\"X2\"] - 0.4 * df_qr[\"X2\"] **7 + rng.normal(0, 1, N_qr)\n\nfit_qr = pf.quantreg(\n  fml = \"Y ~ X1 + X2\",\n  data = df_qr,\n  quantile = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n  method = \"pfn\",\n  multi_method = \"cfm2\"\n)\n\npf.qplot(fit_qr, figsize = [7,3])\n\n\nWe have switched the default solver to scipy.linalg.solve(): link\nYou can now set the maximum number of iterations for the demeaning algo via a fixef_maxiter argument: link\n\n\n\n\n\nWe fixed a bug in internal renaming of categoricals: link\nWe fixed a bug in etable arguments link\nWe stopped casting dependent variable to integer to void Information Loss in Poisson Regression: link\n\n\n\n\n\nWe have added a guide on how to replicate Stata results with pyfixest: link\nWe improved the documentation on how to relabel variable names in the plotting and etable functions: link\n\n\n\n\n\n\nWe have reorganized our tests and rely more on conda environments for making R package test dependencies available: link\n\n\n\n\nWe have added a Code of Conduct.\nWe have opened our discord community. Please join us there to discuss pyfixest and other py-econometrics projects! Link here.\n\n\n\n\n\n\n@FuZhiyu made their first contribution in https://github.com/py-econometrics/pyfixest/pull/886\n@mortizm1988 made their first contribution in https://github.com/py-econometrics/pyfixest/pull/895\n@jestover made their first contribution in https://github.com/py-econometrics/pyfixest/pull/897\n@JaapCTJ made their first contribution in https://github.com/py-econometrics/pyfixest/pull/900\n@shapiromh made their first contribution in https://github.com/py-econometrics/pyfixest/pull/906\n@schroedk made their first contribution in https://github.com/py-econometrics/pyfixest/pull/905\n@WiktorTheScriptor made their first contribution in https://github.com/py-econometrics/pyfixest/pull/938\n@damandhaliwal made their first contribution in https://github.com/py-econometrics/pyfixest/pull/944\n\nFull Changelog: https://github.com/py-econometrics/pyfixest/compare/v0.29.0…v0.30.0\n\n\n\n\nWe add options fixef_k = \"nested\" and fixef_k = \"full\" for computing small sample corrections via pf.ssc(). We set the defaults for pf.feols() and other estimation functions to fixef_k = \"nested\" to 100% mimic the defaults of r-fixest. This is a “breaking change” in the sense that it might (slightly) impact the standard errors of your estimations.\nWe add support for fully saturated event study estimation via the SaturatedEventStudy class, which can be called via pf.event_study().\nWe add support for difference-in-differences specification tests following Lal (2025).\nWe add R2-within values to the default etable() output.\nWe fix a small bug in the Gelbach decompose() method, which would fail if a user selected only_coef = True.\nThe decompose() method runs fully on sparse matrices, which leads to large performance improvements on big data sets.\nWe fix a small bug in the predict() method with newdata, see here for details.\nWe add a function argument rename_models to help rename model names in the coefplot() and iplot() functions and methods:\n\npf.coefplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        fit1._model_name_plot: \"Model 1\",\n        fit2._model_name_plot: \"Model 2\",\n        fit3._model_name_plot: \"Model 3\"\n    },\n)\n\n   \n   \n\n\nMade lets-plot an optional dependency. The package will now fall back to matplotlib for plotting if lets-plot is not installed. Users can install lets-plot with pip install pyfixest[plots].\nPyFixest now supports R2, adjusted R2, and within-R2 values for WLS (it previously only did for OLS, if at all).\nWe add support for standard error of predictions for OLS models without fixed effects. As a default, the predict model still returns a np.ndarray. If the argument se_fit is set to True, we report the prediction standard errors. If argument interval = \"prediction\", we return a pd.DataFrame with predictions, their standard errors, and confidence intervals.\n\n\n\n\n\n\n\nAdds a function argument context, that allows to pass information / context to the formulaic.Formulaic.get_model_matrix() call that creates the model matrix.\nFix a bug that caused reindexing of LPDID._coeftable when calling LPDID.iplot(). As a result, a second call of LPDID.iplot() would fail.\nBumps the required formulaic version to 1.1.0 and fixes errors that arose when a) the ref argument was used for i() syntax, which led to a silent failure under formulaic &gt;= 1.1.0, and fixef() / predict() with fixed effects, which led to a loud error.\n\n\n\n\n\nAdds a pf.feglm() function that supports GLMs with normal and binomial families (gaussian, logit, probit) without fixed effects. Fixed effects support is work in progress.\nAdds options to run the demean function via JAX. This might speed up the model fit if GPU is available.\n\n\n\n\n\n\nAdds support for Gelbach’s (JoLe 2016) Regression Decomposition method using a decompose() method for Feols.\nAdds support for the multiple hypothesis correction by Westfall & Young via the pf.wyoung() function.\nInput data frames to pf.feols() and pf.fepois() are now converted to pandas via narwhals. As a result, users can not provide duckdb or ibis tables as inputs, as well as pandas and polars data frames. polars and pyarrow are dropped as a dependencies.\nFixes a bug in the wildboottest method, which incorrectly used to run a regression on the demeaned dependend variable in case it was applied after a fixed effects regression. My apologies for that!\nFixes a bug in the ritest method, which would use randomization inference coefficients instead of t-statistics, leading to incorrect results. This has consequences for the rwolf() function, which, in case of running ri-inference, would default to run the randomization-t. My apolgies!\nAdds a vignette on multiple testing corrections.\nAdds a vignette on Gelbach’s regression decomposition.\n\n\n\n\nSee the github changelog for details: link.\n\n\n\n\n\n\nFix bug in wildboottest method @s3alfisc (#506)\ndocs: add sanskriti2005 as a contributor for infra @allcontributors (#503)\nInfra: added the release-drafter for automation of release notes @sanskriti2005 (#502)\nFix broken link in contributing.md @s3alfisc (#499)\ndocs: add leostimpfle as a contributor for bug @allcontributors (#495)\nUpdate justfile @leostimpfle (#494)\ndocs: add baggiponte as a contributor for doc @allcontributors (#490)\ndocs: improve installation section @baggiponte (#489)\nBump tornado from 6.4 to 6.4.1 @dependabot (#487)\ndocs: add leostimpfle as a contributor for code @allcontributors (#478)\nFeols: speed up the creation of interacted fixed effects via fe1^fe2 syntax @leostimpfle (#475)\nrename resampling iterations to ‘reps’ in all methods @s3alfisc (#474)\nfix a lot of broken links throught the repo @s3alfisc (#472)\nMultiple readme fixes required after package was moved to py-econometrics project @s3alfisc (#450)\n\n\n\n\n\ninfrastructure: fix minor release drafter bugs @s3alfisc (#504)\n\n\n\n\n\n\nAdd support for randomization inference via the ritest() method:\n\n\nimport pyfixest as pf\ndata = pf.get_data()\n\nfit = pf.feols(\"Y ~ X1\", data = data)\nfit.ritest(resampvar=\"X1=0\", reps = 1000)\n\n\n\n\n\nThis version introduces MyPy type checks to the entire pyfixest codebase. Thanks to @juanitorduz for nudging me to get started with this =). It also fixes a handful of smaller bugs.\n\n\n\n\n\nFixes multiple smaller and larger performance regressions. The NYC-Taxi example regression now takes approximately 22 seconds to run (… if my laptopt is connected to a power charger)!\n\n\n%load_ext autoreload\n%autoreload 2\n\nimport duckdb\nimport time\nimport numpy as np\nimport pyfixest as pf\n\n# %%\nnyc = duckdb.sql(\n    '''\n    FROM 'C:/Users/alexa/Documents/nyc-taxi/**/*.parquet'\n    SELECT\n        tip_amount, trip_distance, passenger_count,\n        vendor_id, payment_type, dropoff_at,\n        dayofweek(dropoff_at) AS dofw\n    WHERE year = 2012 AND month &lt;= 3\n    '''\n    ).df()\n\n# convert dowf, vendor_id, payment_type to categorical\ntic = time.time()\nnyc[\"dofw\"] = nyc[\"dofw\"].astype(int)\nnyc[\"vendor_id\"] = nyc[\"vendor_id\"].astype(\"category\")\nnyc[\"payment_type\"] = nyc[\"payment_type\"].astype(\"category\")\nprint(f\"\"\"\n    I am convering columns of type 'objects' to 'categories' and 'int'data types outside\n    of the regression, hence I am cheating a bit. This saves {np.round(time.time() - tic)} seconds.\n    \"\"\"\n)\n#    I am convering columns of type 'objects' to 'categories' and 'int'data types outside\n#    of the regression, hence I am cheating a bit. This saves 7.0 seconds.\n\nrun = True\nif run:\n\n    # mock regression for JIT compilation\n    fit = pf.feols(\n        fml = \"tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw\",\n        data = nyc.iloc[1:10_000],\n        copy_data = False,\n        store_data = False\n        )\n\n    import time\n    tic = time.time()\n    fit = pf.feols(\n        fml = \"tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw\",\n        data = nyc,\n        copy_data = False, # saves a few seconds\n        store_data = False # saves a few second\n        )\n    passed = time.time() - tic\n    print(f\"Passed time is {np.round(passed)}.\")\n    # Passed time is 22.\n\n\nAdds three new function arguments to feols() and fepois(): copy_data, store_data, and fixef_tol.\nAdds support for frequency weights with the weights_type function argument.\n\n\nimport pyfixest as pf\n\ndata = pf.get_data(N = 10000, model = \"Fepois\")\ndf_weighted = data[[\"Y\", \"X1\", \"f1\"]].groupby([\"Y\", \"X1\", \"f1\"]).size().reset_index().rename(columns={0: \"count\"})\ndf_weighted[\"id\"] = list(range(df_weighted.shape[0]))\n\nprint(\"Dimension of the aggregated df:\", df_weighted.shape)\nprint(df_weighted.head())\n\nfit = pf.feols(\n    \"Y ~ X1 | f1\",\n    data = data\n)\nfit_weighted = pf.feols(\n    \"Y ~ X1 | f1\",\n    data = df_weighted,\n    weights = \"count\",\n    weights_type = \"fweights\"\n)\npf.etable([fit, fit_weighted], coef_fmt = \"b(se) \\n (t) \\n (p)\")\n\nDimension of the aggregated df: (1278, 5)\n     Y   X1   f1  count  id\n0  0.0  0.0  0.0     17   0\n1  0.0  0.0  1.0     11   1\n2  0.0  0.0  2.0     10   2\n3  0.0  0.0  3.0     17   3\n4  0.0  0.0  4.0     14   4\n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n0.001(0.012)\n(0.088)\n(0.93)\n0.001(0.012)\n(0.088)\n(0.93)\n\n\nfe\n\n\nf1\nx\nx\n\n\nstats\n\n\nObservations\n9,996\n9,996\n\n\nR2\n0.011\n0.011\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient(Std. Error) (t-stats) (p-value)\n\n\n\n\n\n\n\n\n\n\nBugfix: Wild Cluster Bootstrap Inference with Weights would compute unweighted standard errors. Sorry about that! WLS is not supported for the WCB.\nAdds support for CRV3 inference with weights.\n\n\n\n\n\nLarge Refactoring of Interal Processing of Model Formulas, in particular FixestFormulaParser and model_matrix_fixest. As a results, the code should be cleaner and more robust.\nThanks to the refactoring, we can now bump the required formulaic version to the stable 1.0.0 release.\nThe fml argument of model_matrix_fixest is deprecated. Instead, model_matrix_fixest now asks for a FixestFormula, which is essentially a dictionary with information on model specifications like a first stage formula (if applicable), dependent variables, fixed effects, etc.\nAdditionally, model_matrix_fixest now returns a dictionary instead of a tuple.\nBrings back fixed effects reference setting via i(var1, var2, ref) syntax. Deprecates the i_ref1, i_ref2 function arguments. I.e. it is again possible to e.g. run\n\n\nimport pyfixest as pf\ndata = pf.get_data()\n\nfit1 = pf.feols(\"Y ~ i(f1, X2)\", data=data)\nfit1.coef()[0:8]\n\nVia the ref syntax, via can set the reference level:\n\nfit2 = pf.feols(\"Y ~ i(f1, X2, ref = 1)\", data=data)\nfit2.coef()[0:8]\n\n\n\n\n\nRestructures the codebase and reorganizes how users can interact with the pyfixest API. It is now recommended to use pyfixest in the following way:\n\nimport numpy as np\nimport pyfixest as pf\ndata = pf.get_data()\ndata[\"D\"] = data[\"X1\"] &gt; 0\nfit = pf.feols(\"Y ~ D + f1\", data = data)\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nIntercept\n0.778849\n0.170261\n4.574437\n0.000005\n0.444737\n1.112961\n\n\nD\n-1.402617\n0.152224\n-9.214140\n0.000000\n-1.701335\n-1.103899\n\n\nf1\n0.004774\n0.008058\n0.592508\n0.553645\n-0.011038\n0.020587\n\n\n\n\n\n\n\nThe update should not inroduce any breaking changes. Thanks to @Wenzhi-Ding for the PR!\nAdds support for simultaneous confidence intervals via a multiplier bootstrap. Thanks to @apoorvalal for the contribution!\n\nfit.confint(joint = True)\n\n\n\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nIntercept\n0.381047\n1.176651\n\n\nD\n-1.758278\n-1.046956\n\n\nf1\n-0.014052\n0.023601\n\n\n\n\n\n\n\nAdds support for the causal cluster variance estimator by Abadie et al. (QJE, 2023) for OLS via the .ccv() method.\n\nfit.ccv(treatment = \"D\", cluster = \"group_id\")\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1588: UserWarning: The initial model was not clustered. CRV1 inference is computed and stored in the model object.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\nCCV\n-1.4026168622179929\n0.215624\n-6.504907\n0.000004\n-1.855627\n-0.949607\n\n\nCRV1\n-1.402617\n0.205132\n-6.837621\n0.000002\n-1.833584\n-0.97165\n\n\n\n\n\n\n\n\n\n\n\n\nAdds multiple quality of life improvements for developers, thanks to NKeleher.\nAdds more options to customize etable() output thanks to Wenzhi-Ding.\nImplements Romano-Wolf and Bonferroni corrections for multiple testing in the multcomp module.\n\n\n\n\n\nAdds support for weighted least squares for feols().\nReduces testing time drastically by running tests on fewer random data samples. Qualitatively, the set of test remains identical.\nSome updates for future pandas compatibility.\n\n\n\n\n\nMoves the documentation to quartodoc.\nChanges all docstrings to numpy format.\nDifference-in-differences estimation functions now need to be imported via the pyfixest.did.estimation module:\n\n\nfrom pyfixest.did.estimation import did2s, lpdid, event_study\n\n\n\n\n\nFixes a bug that lead to incorrect results when the dependent variable and all covariates (excluding the fixed effects) where integers.\n\n\n\n\n\nFixes a bug in etable() with IV’s that occurred because feols() does not report R2 statistics for IVs.\n\n\n\n\n\nFixes a bug in etable() and a warning in fixest_model_matrix that arose with higher pandas versions. Thanks to @aeturrell for reporting!\n\n\n\n\n\n\n\nIntroduces a new pyfixest.did module which contains routines for Difference-in-Differences estimation.\nIntroduces support for basic versions of the local projections DiD estimator following Dube et al (2023)\nAdds a new vignette for Difference-in-Differences estimation.\nReports R2 values in etable().\n\n\n\n\n\n\n\n\nGood performance improvements for singleton fixed effects detection. Thanks to @styfenschaer for the PR! See #229.\nUses the r2u project for installing R and R packages on github actions, with great performance improvements.\nAllows to pass polars data frames to feols(), fepois() and predict(). #232. Thanks to @vincentarelbundock for the suggestion!\n\n\n\n\n\nMissing variables in features were not always handled correctly in predict() with newdata not None in the presence of missing data, which would lead to an error. See #246 for details.\nCategorical variables were not always handled correctly in predict() with newdata not None, because the number of fixed effects levels in newdata might be smaller than in data. In consequence, some levels were not found, which lead to an error. See #245 for details. Thanks to @jiafengkevinchen for the pointer!\nMulticollinearity checks for over-identified IV was not implemented correctly, which lead to a dimension error. See #236 for details. Thanks to @jiafengkevinchen for the pointer!\nThe number of degrees of freedom k was computed incorrectly if columns were dropped from the design matrix X in the presence of multicollinearity. See #235 for details. Thanks to @jiafengkevinchen for the pointer!\nIf all variables were dropped due to multicollinearity, an unclear and imprecise error message was produced. See #228 for details. Thanks to @manferdinig for the pointer!\nIf selection fixef_rm = 'singleton', feols() and fepois() would fail, which has been fixed. #192\n\n\n\n\n\nFor now, sets formulaic versions to be 0.6.6 or lower as version 1.0.0 seems to have introduced a problem with the i() operator, See #244 for details.\nDrops dependency on pyhdfe.\n\n\n\n\n\n\nFixes some bugs around the computation of R-squared values (see issue #103).\nReports R-squared values again when calling .summary().\n\n\n\n\n\nSignificant speedups for CRV1 inference.\n\n\n\n\nFixes a small bug with the separation check for poisson regression #138.\n\n\n\nFixes bugs with i(var1, var2) syntax introduced with PyFixest 0.10.10.\n\n\n\nFixes a bug with variable interactions via i(var) syntax. See issue #221.\n\n\n\nMakes etable() prettier and more informative.\n\n\n\n\n\nReference levels for the i() formula syntax can no longer be set within the formula, but need to be added via the i_ref1 function argument to either feols() and fepois().\n\n\n\nA dids2() function is added, which implements the 2-stage difference-in-differences procedure à la Gardner and follows the syntax of @kylebutts did2s R package.\nfrom pyfixest.did.did import did2s\nfrom pyfixest.estimation import feols\nfrom pyfixest.visualize import iplot\nimport pandas as pd\nimport numpy as np\n\ndf_het = pd.read_csv(\"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\")\n\nfit = did2s(\n    df_het,\n    yname = \"dep_var\",\n    first_stage = \"~ 0 | state + year\",\n    second_stage = \"~i(rel_year)\",\n    treatment = \"treat\",\n    cluster = \"state\",\n    i_ref1 = [-1.0, np.inf],\n)\n\nfit_twfe = feols(\n    \"dep_var ~ i(rel_year) | state + year\",\n    df_het,\n    i_ref1 = [-1.0, np.inf]\n)\n\niplot([fit, fit_twfe], coord_flip=False, figsize = (900, 400), title = \"TWFE vs DID2S\")\n\n\n\n\n\n\nAdds basic support for event study estimation via two-way fixed effects and Gardner’s two-stage “Did2s” approach. This is a beta version and experimental. Further updates (i.e. proper event studies vs “only” ATTs) and a more flexible did2s front end will follow in future releases.\n\n%load_ext autoreload\n%autoreload 2\n\nfrom pyfixest.did.did import event_study\nimport pyfixest as pf\nimport pandas as pd\ndf_het = pd.read_csv(\"pyfixest/did/data/df_het.csv\")\n\nfit_twfe = event_study(\n    data = df_het,\n    yname = \"dep_var\",\n    idname= \"state\",\n    tname = \"year\",\n    gname = \"g\",\n    estimator = \"twfe\"\n)\n\nfit_did2s = event_study(\n    data = df_het,\n    yname = \"dep_var\",\n    idname= \"state\",\n    tname = \"year\",\n    gname = \"g\",\n    estimator = \"did2s\"\n)\n\npf.etable([fit_twfe, fit_did2s])\n# | Coefficient   | est1             | est2             |\n# |:--------------|:-----------------|:-----------------|\n# | ATT           | 2.135*** (0.044) | 2.152*** (0.048) |\n# Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\nAdds an etable() function that outputs markdown, latex or a pd.DataFrame.\n\n\n\n\n\nFixes a big in IV estimation that would trigger an error. See here for details. Thanks to @aeturrell for reporting!\n\n\n\n\n\nImplements a custom function to drop singleton fixed effects.\nAdditional small performance improvements.\n\n\n\n\n\nAllows for white space in the multiway clustering formula.\nAdds documentation for multiway clustering.\n\n\n\n\n\nAdds support for two-way clustering.\nAdds support for CRV3 inference for Poisson regression.\n\n\n\n\n\nAdapts the internal fixed effects demeaning criteron to match `PyHDFE’s default.\nAdds Styfen as coauthor.\n\n\n\n\n\nMultiple performance improvements.\nMost importantly, implements a custom demeaning algorithm in numba - thanks to Styfen Schaer (@styfenschaer), which leads to performance improvements of 5x or more:\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport time\nimport pyhdfe\nfrom pyfixest.demean import demean\n\nnp.random.seed(1238)\nN = 10_000_000\nx = np.random.normal(0, 1, 10*N).reshape((N,10))\nf1 = np.random.choice(list(range(1000)), N).reshape((N,1))\nf2 = np.random.choice(list(range(1000)), N).reshape((N,1))\n\nflist = np.concatenate((f1, f2), axis = 1)\nweights = np.ones(N)\n\nalgorithm = pyhdfe.create(flist)\n\nstart_time = time.time()\nres_pyhdfe = algorithm.residualize(x)\nend_time = time.time()\nprint(end_time - start_time)\n# 26.04527711868286\n\n\nstart_time = time.time()\nres_pyfixest, success = demean(x, flist, weights, tol = 1e-10)\n# Calculate the execution time\nend_time = time.time()\nprint(end_time - start_time)\n#4.334428071975708\n\nnp.allclose(res_pyhdfe , res_pyfixest)\n# True\n\n\n\n\nBump required formulaic version to 0.6.5.\nStop copying the data frame in fixef().\n\n\n\n\n\nFixes a big in the wildboottest method (see #158).\nAllows to run a wild bootstrap after fixed effect estimation.\n\n\n\n\n\nAdds support for wildboottest for Python 3.11.\n\n\n\n\n\nFixes a couple more bugs in the predict() and fixef() methods.\nThe predict() argument data is renamed to newdata.\n\n\n\n\nFixes a bug in predict() produced when multicollinear variables are dropped.\n\n\n\nImproved Collinearity handling. See #145\n\n\n\n\nMoves plotting from matplotlib to lets-plot.\nFixes a few minor bugs in plotting and the fixef() method.\n\n\n\n\n\n\nIt is no longer required to initiate an object of type Fixest prior to running Feols or fepois. Instead, you can now simply use feols() and fepois() as functions, just as in fixest. Both function can be found in an estimation module and need to obtain a pd.DataFrame as a function argument:\nfrom pyfixest.estimation import fixest, fepois\nfrom pyfixest.utils import get_data\n\ndata = get_data()\nfit = feols(\"Y ~ X1 | f1\", data = data, vcov = \"iid\")\nCalling feols() will return an instance of class Feols, while calling fepois() will return an instance of class Fepois. Multiple estimation syntax will return an instance of class FixestMulti.\nPost processing works as before via .summary(), .tidy() and other methods.\n\n\n\nA summary function allows to compare multiple models:\nfrom pyfixest.summarize import summary\nfit2 = feols(\"Y ~ X1 + X2| f1\", data = data, vcov = \"iid\")\nsummary([fit, fit2])\nVisualization is possible via custom methods (.iplot() & .coefplot()), but a new module allows to visualize a list of Feols and/or Fepois instances:\nfrom pyfixest.visualize import coefplot, iplot\ncoefplot([fit, fit2])\nThe documentation has been improved (though there is still room for progress), and the code has been cleaned up a bit (also lots of room for improvements)."
  },
  {
    "objectID": "changelog.html#pyfixest-0.40.1",
    "href": "changelog.html#pyfixest-0.40.1",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "fixest 0.13 has recently been released to CRAN, with a range of breaking changes. We are following these in pyfixet 0.40.0.\n\nThe default inference method is now always “iid”. Before, both fixest and pyfixest would default to cluster by the first fixest effect in the presence of fixed effect.\nThe arguments of the pf.ssc() functions have been renamed: adj becomes k_adj, fixef_k becomes k_fixef, cluster_df becomes G_df, and cluster_adj becomes G_adj. Backwards compatibility is ensured.\nfixest no longer applies the G/(G-1) small sample correction for heteroskedastic errors. The argument is now only relevant for cluster robust errors.\nIf the k_adj arg is set to True with heteroskedastic errors, the applied small sample correction now is N / (N-k) and no longer (N-1) / (N-df_k), as was previously the case and is still the case for iid and cluster robust errors.\nThe k_fixef option \"nested\" has been renamed to \"nonnested\".\nThe fixef_rm argument is changed from default \"none\" to \"singleton\" for all estimation functions. This has no impact on point estimates, but might change inference due to a smaller number of observations in the degree of freedom correction.\nThe multicollinearity default tolerance has been reduced from 1e-10 to 1e-09.\nThe attribute _dof_k has been renamed to _df_k.\n\nOther related changes: - The arguments to adjust the small samples for the wildboottest methods have been renamed to k_adj and G_adj.\n\n\n\nWe have been using pixi as our package manager for a while and are moving from a pyproject.toml to using a pixi.toml for specifying dependencies. We also install all package by default from the free and open source conda-forge. Because of some environment resolution challenges around an old version of rpy2that we need for testing pyfixest against fixest, we temporarily have dropped support for the dev and docs environments for windows. A fix is work in progress."
  },
  {
    "objectID": "changelog.html#new-features",
    "href": "changelog.html#new-features",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "We add CuPy and SciPy Backend to run the demeaning algorithm on the GPU via the sparse LSMR solver. For problems where the standard demeaner struggles to converge, this strategy can lead to significant speedups if paired with a GPU.\n\n\n\nComplex Fixed Effects Benchmark\n\n\n\n\n\nWe now support HAC standard errors! Thanks for Daman (https://github.com/damandhaliwal) for all his work (and coping with me with the PR, which took forever to get through). We now support:\n\ntime series HAC\npanel HAC\npanel DK\n\nvariance-covariance matrices.\nFor now, we assume that the time series column is consecutive and each entry has “time delta 1”. Relaxation of this requirement are work in progress.\n\n\n\nFor easier encoding of categorical variables in relation to the iplot() and coefplot() functions, we add a new function argument, cat_template:\n\nfit_c = pf.feols(fml = \"Y ~ i(X1, f1)\", data = df)\nfit_c.iplot(cat_template = \"{variable}::{value}\")\n\n   \n   \n\n\nThis is particularly useful for Difference-in-Differences and event studies. You can find an example use case in the DiD vignette.\nGoing forward, we will deprecate the rename_event_study_coefs function, which is no longer needed (and did not work anyways).\n\n\n\nWe have reworked the Gelbach Decomposition method, with some breaking changes: by default, calling Feols.decompose() now returns a GelbachDecomposition instance - before, we’d return a pd.DataFrame. The param argument has been renamed to decomp_var, but is not yet deprecated.\nThe class now comes with new tidy() and etable() methods.\n\nfrom pyfixest.utils.dgps import gelbach_data\nimport numpy as np\nimport pyfixest as pf\n\n# Generate test data using gelbach_data\ndata = gelbach_data(nobs=500)\ndata[\"w\"] = np.random.rand(500)\nfit = pf.feols(\"y ~ x1 + x21 + x22 + x23\", data=data)\ngb = fit.decompose(decomp_var = \"x1\", x1_vars = [\"x21\"],reps = 10, nthreads = 1)\nprint(type(gb))\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]100%|██████████| 10/10 [00:00&lt;00:00, 378.43it/s]\n\n\n&lt;class 'pyfixest.estimation.decomposition.GelbachDecomposition'&gt;\n\n\n\n\n\nIt is now also possible to add background variables that are included in both the long and short regressions via the x1_vars function argument.\nWe can inspect results as a pd.DataFrame\n\ngb.tidy()\n\n\n\n\n\n\n\n\ncoefficients\nci_lower\nci_upper\npanels\n\n\n\n\ndirect_effect\n1.412165\n0.543347\n1.935669\nLevels (units)\n\n\nfull_effect\n1.041753\n0.988224\n1.094174\nLevels (units)\n\n\nexplained_effect\n0.370412\n-0.507537\n0.880713\nLevels (units)\n\n\nunexplained_effect\n1.041753\n0.988224\n1.094174\nLevels (units)\n\n\nx22\n0.061930\n-0.505729\n0.358138\nLevels (units)\n\n\nx23\n0.308483\n-0.001808\n0.522575\nLevels (units)\n\n\ndirect_effect\n1.000000\n1.000000\n1.000000\nShare of Full Effect\n\n\nfull_effect\n0.737699\n0.545886\n1.898845\nShare of Full Effect\n\n\nexplained_effect\n0.262301\n-0.898845\n0.454114\nShare of Full Effect\n\n\nunexplained_effect\n0.737699\n0.545886\n1.898845\nShare of Full Effect\n\n\nx22\n0.043855\n-0.902483\n0.184401\nShare of Full Effect\n\n\nx23\n0.218446\n-0.002201\n0.269713\nShare of Full Effect\n\n\ndirect_effect\n3.812414\n-1.167910\n39.174170\nShare of Explained Effect\n\n\nfull_effect\n2.812414\n-2.167910\n38.174170\nShare of Explained Effect\n\n\nexplained_effect\n1.000000\n1.000000\n1.000000\nShare of Explained Effect\n\n\nunexplained_effect\n2.812414\n-2.167910\n38.174170\nShare of Explained Effect\n\n\nx22\n0.167192\n-6.509153\n1.054200\nShare of Explained Effect\n\n\nx23\n0.832808\n-0.054200\n7.509153\nShare of Explained Effect\n\n\n\n\n\n\n\nor produce a GT table:\n\ngb.etable(\n    stats = \"all\",\n    caption = \"Gelbach Decomposition\"\n)\n\n\n\n\n\n\n\nGelbach Decomposition\n\n\n\nInitial Difference\nAdjusted Difference\nExplained Difference\n\n\n\n\nLevels (units)\n\n\nx1\n1.412\n1.042\n0.370\n\n\n\n[0.543, 1.936]\n[0.988, 1.094]\n[-0.508, 0.881]\n\n\nx22\n-\n-\n0.062\n\n\n\n-\n-\n[-0.506, 0.358]\n\n\nx23\n-\n-\n0.308\n\n\n\n-\n-\n[-0.002, 0.523]\n\n\nShare of Full Effect\n\n\nx1\n1.000\n0.738\n0.262\n\n\n\n-\n[0.546, 1.899]\n[-0.899, 0.454]\n\n\nx22\n-\n-\n0.044\n\n\n\n-\n-\n[-0.902, 0.184]\n\n\nx23\n-\n-\n0.218\n\n\n\n-\n-\n[-0.002, 0.270]\n\n\nShare of Explained Effect\n\n\nx1\n-\n-\n1.000\n\n\nx22\n-\n-\n0.167\n\n\n\n-\n-\n[-6.509, 1.054]\n\n\nx23\n-\n-\n0.833\n\n\n\n-\n-\n[-0.054, 7.509]\n\n\n\nDecomposition variable: x1. Control Variables: x21. CIs are computed using B = 10 bootstrap replications using iid sampling.Col 1: Adjusted Difference (by x21) - Coefficient on x1 in short regression. Col 2: Adjusted Difference - Coefficient on x1 in long regression. Col 3: Explained Difference - Difference in coefficients of x1 in short and long regression. Panel 1: Levels (units). Panel 2: Share of Full Effect: Levels normalized by coefficient of the short regression. Panel 3: Share of Explained Effect: Levels normalized by coefficient of the long regression.\n\n\n\n\n\n\n\n\n\nAs can be seen, we by default now return normalized (and not just absolute) effects.\nWe are now also supporting frequency weights for the decomposition (currently without inference).\nAdditionally, some house keeping and interal refactoring of the GelbachDecomposition class."
  },
  {
    "objectID": "changelog.html#pyfixest-0.30.0",
    "href": "changelog.html#pyfixest-0.30.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "We have created a Rust backend for all performance critical algorithms, with pretty great performance improvements! You can use the Rust backend by setting demeaner_options = \"rust\".\n\nWe find pretty great performance improvements and want to make the Rust backend the default in PyFixest 0.31.0.\nTo back up the performance claim, here is a benchmark:\n\nimport pyfixest as pf\nimport numpy as np\nimport pandas as pd\nimport time\n\nrng = np.random.default_rng(737)\n\nN = 10_000_000\nbenchmark_data = pd.DataFrame({\n  \"Y\": rng.normal(0, 1, N),\n  \"X1\": rng.normal(0, 1, N),\n  \"X2\": rng.normal(0, 1, N),\n  \"X3\": rng.normal(0, 1, N),\n  \"f1\": rng.integers(0, 10_000, N),\n  \"f2\": rng.integers(0, 1_000, N),\n  \"f3\": rng.integers(0, 10, N),\n})\n\n# burn-in for numba\nfit_nb_warmup = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data[:100_000]\n)\n\n# benchmark for numba backend\ntic = time.time()\nfit_nb = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data\n)\ntoc = time.time()\nprint(f\"Numba backend took {toc-tic}.\")\n\n# benchmark for rust backend\ntic = time.time()\nfit_rust = pf.feols(\n  fml = \"Y ~ X1 + X2 + X3 | f1 + f2 + f3\", data = benchmark_data,\n  demeaner_backend = \"rust\"\n)\ntoc = time.time()\nprint(f\"Rust backend took {toc-tic}.\")\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\nNumba backend took 4.966489553451538.\nRust backend took 6.356808662414551.\n\n\nResults are also matching =)\n\npf.etable([fit_nb, fit_rust], digits = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n0.00022257\n(0.00031662)\n0.00022257\n(0.00031662)\n\n\nX2\n0.00020980\n(0.00031647)\n0.00020980\n(0.00031647)\n\n\nX3\n0.00062145*\n(0.00031651)\n0.00062145*\n(0.00031651)\n\n\nfe\n\n\nf1\nx\nx\n\n\nf2\nx\nx\n\n\nf3\nx\nx\n\n\nstats\n\n\nObservations\n10,000,000\n10,000,000\n\n\nR2\n0.001\n0.001\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient (Std. Error)\n\n\n\n\n\n\n\n\n\n\nWe now support quantile regression, including a Frisch-Newton Interior Point Solver with and without preprocessing, iid, heteroskedastic and cluster robust standard errors, fast algorithms for the entire quantile regression process, and some visualisations. In particular the algorithms for the quantile regression process show excellent performance. You can learn more about all features and take a look at more systematic benchmarks in the quantreg vignette.\n\n\nN_qr = 10_000\nrng = np.random.default_rng(929)\n\ndf_qr = pd.DataFrame({\n  \"X1\": rng.normal(0, 1, N_qr),\n  \"X2\": rng.normal(0, 1, N_qr)\n})\ndf_qr[\"Y\"] = -0.5 + -2 * df_qr[\"X1\"] + 1.9 * df_qr[\"X1\"] ** 4 + df_qr[\"X2\"] - 0.4 * df_qr[\"X2\"] **7 + rng.normal(0, 1, N_qr)\n\nfit_qr = pf.quantreg(\n  fml = \"Y ~ X1 + X2\",\n  data = df_qr,\n  quantile = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n  method = \"pfn\",\n  multi_method = \"cfm2\"\n)\n\npf.qplot(fit_qr, figsize = [7,3])\n\n\nWe have switched the default solver to scipy.linalg.solve(): link\nYou can now set the maximum number of iterations for the demeaning algo via a fixef_maxiter argument: link\n\n\n\n\n\nWe fixed a bug in internal renaming of categoricals: link\nWe fixed a bug in etable arguments link\nWe stopped casting dependent variable to integer to void Information Loss in Poisson Regression: link\n\n\n\n\n\nWe have added a guide on how to replicate Stata results with pyfixest: link\nWe improved the documentation on how to relabel variable names in the plotting and etable functions: link"
  },
  {
    "objectID": "changelog.html#infrastructure",
    "href": "changelog.html#infrastructure",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "We have reorganized our tests and rely more on conda environments for making R package test dependencies available: link\n\n\n\n\nWe have added a Code of Conduct.\nWe have opened our discord community. Please join us there to discuss pyfixest and other py-econometrics projects! Link here."
  },
  {
    "objectID": "changelog.html#new-contributors",
    "href": "changelog.html#new-contributors",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "@FuZhiyu made their first contribution in https://github.com/py-econometrics/pyfixest/pull/886\n@mortizm1988 made their first contribution in https://github.com/py-econometrics/pyfixest/pull/895\n@jestover made their first contribution in https://github.com/py-econometrics/pyfixest/pull/897\n@JaapCTJ made their first contribution in https://github.com/py-econometrics/pyfixest/pull/900\n@shapiromh made their first contribution in https://github.com/py-econometrics/pyfixest/pull/906\n@schroedk made their first contribution in https://github.com/py-econometrics/pyfixest/pull/905\n@WiktorTheScriptor made their first contribution in https://github.com/py-econometrics/pyfixest/pull/938\n@damandhaliwal made their first contribution in https://github.com/py-econometrics/pyfixest/pull/944\n\nFull Changelog: https://github.com/py-econometrics/pyfixest/compare/v0.29.0…v0.30.0"
  },
  {
    "objectID": "changelog.html#pyfixest-0.29.0",
    "href": "changelog.html#pyfixest-0.29.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "We add options fixef_k = \"nested\" and fixef_k = \"full\" for computing small sample corrections via pf.ssc(). We set the defaults for pf.feols() and other estimation functions to fixef_k = \"nested\" to 100% mimic the defaults of r-fixest. This is a “breaking change” in the sense that it might (slightly) impact the standard errors of your estimations.\nWe add support for fully saturated event study estimation via the SaturatedEventStudy class, which can be called via pf.event_study().\nWe add support for difference-in-differences specification tests following Lal (2025).\nWe add R2-within values to the default etable() output.\nWe fix a small bug in the Gelbach decompose() method, which would fail if a user selected only_coef = True.\nThe decompose() method runs fully on sparse matrices, which leads to large performance improvements on big data sets.\nWe fix a small bug in the predict() method with newdata, see here for details.\nWe add a function argument rename_models to help rename model names in the coefplot() and iplot() functions and methods:\n\npf.coefplot(\n    models = [fit1, fit2, fit3],\n    rename_models = {\n        fit1._model_name_plot: \"Model 1\",\n        fit2._model_name_plot: \"Model 2\",\n        fit3._model_name_plot: \"Model 3\"\n    },\n)\n\n   \n   \n\n\nMade lets-plot an optional dependency. The package will now fall back to matplotlib for plotting if lets-plot is not installed. Users can install lets-plot with pip install pyfixest[plots].\nPyFixest now supports R2, adjusted R2, and within-R2 values for WLS (it previously only did for OLS, if at all).\nWe add support for standard error of predictions for OLS models without fixed effects. As a default, the predict model still returns a np.ndarray. If the argument se_fit is set to True, we report the prediction standard errors. If argument interval = \"prediction\", we return a pd.DataFrame with predictions, their standard errors, and confidence intervals."
  },
  {
    "objectID": "changelog.html#pyfixest-0.28.0",
    "href": "changelog.html#pyfixest-0.28.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds a function argument context, that allows to pass information / context to the formulaic.Formulaic.get_model_matrix() call that creates the model matrix.\nFix a bug that caused reindexing of LPDID._coeftable when calling LPDID.iplot(). As a result, a second call of LPDID.iplot() would fail.\nBumps the required formulaic version to 1.1.0 and fixes errors that arose when a) the ref argument was used for i() syntax, which led to a silent failure under formulaic &gt;= 1.1.0, and fixef() / predict() with fixed effects, which led to a loud error.\n\n\n\n\n\nAdds a pf.feglm() function that supports GLMs with normal and binomial families (gaussian, logit, probit) without fixed effects. Fixed effects support is work in progress.\nAdds options to run the demean function via JAX. This might speed up the model fit if GPU is available."
  },
  {
    "objectID": "changelog.html#pyfixest-0.27.0",
    "href": "changelog.html#pyfixest-0.27.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds support for Gelbach’s (JoLe 2016) Regression Decomposition method using a decompose() method for Feols.\nAdds support for the multiple hypothesis correction by Westfall & Young via the pf.wyoung() function.\nInput data frames to pf.feols() and pf.fepois() are now converted to pandas via narwhals. As a result, users can not provide duckdb or ibis tables as inputs, as well as pandas and polars data frames. polars and pyarrow are dropped as a dependencies.\nFixes a bug in the wildboottest method, which incorrectly used to run a regression on the demeaned dependend variable in case it was applied after a fixed effects regression. My apologies for that!\nFixes a bug in the ritest method, which would use randomization inference coefficients instead of t-statistics, leading to incorrect results. This has consequences for the rwolf() function, which, in case of running ri-inference, would default to run the randomization-t. My apolgies!\nAdds a vignette on multiple testing corrections.\nAdds a vignette on Gelbach’s regression decomposition."
  },
  {
    "objectID": "changelog.html#pyfixest-0.22.0---0.25.4",
    "href": "changelog.html#pyfixest-0.22.0---0.25.4",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "See the github changelog for details: link."
  },
  {
    "objectID": "changelog.html#pyfixest-0.22.0",
    "href": "changelog.html#pyfixest-0.22.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fix bug in wildboottest method @s3alfisc (#506)\ndocs: add sanskriti2005 as a contributor for infra @allcontributors (#503)\nInfra: added the release-drafter for automation of release notes @sanskriti2005 (#502)\nFix broken link in contributing.md @s3alfisc (#499)\ndocs: add leostimpfle as a contributor for bug @allcontributors (#495)\nUpdate justfile @leostimpfle (#494)\ndocs: add baggiponte as a contributor for doc @allcontributors (#490)\ndocs: improve installation section @baggiponte (#489)\nBump tornado from 6.4 to 6.4.1 @dependabot (#487)\ndocs: add leostimpfle as a contributor for code @allcontributors (#478)\nFeols: speed up the creation of interacted fixed effects via fe1^fe2 syntax @leostimpfle (#475)\nrename resampling iterations to ‘reps’ in all methods @s3alfisc (#474)\nfix a lot of broken links throught the repo @s3alfisc (#472)\nMultiple readme fixes required after package was moved to py-econometrics project @s3alfisc (#450)\n\n\n\n\n\ninfrastructure: fix minor release drafter bugs @s3alfisc (#504)"
  },
  {
    "objectID": "changelog.html#pyfixest-0.21.0",
    "href": "changelog.html#pyfixest-0.21.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Add support for randomization inference via the ritest() method:\n\n\nimport pyfixest as pf\ndata = pf.get_data()\n\nfit = pf.feols(\"Y ~ X1\", data = data)\nfit.ritest(resampvar=\"X1=0\", reps = 1000)"
  },
  {
    "objectID": "changelog.html#pyfixest-0.20.0",
    "href": "changelog.html#pyfixest-0.20.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "This version introduces MyPy type checks to the entire pyfixest codebase. Thanks to @juanitorduz for nudging me to get started with this =). It also fixes a handful of smaller bugs."
  },
  {
    "objectID": "changelog.html#pyfixest-0.19.0",
    "href": "changelog.html#pyfixest-0.19.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes multiple smaller and larger performance regressions. The NYC-Taxi example regression now takes approximately 22 seconds to run (… if my laptopt is connected to a power charger)!\n\n\n%load_ext autoreload\n%autoreload 2\n\nimport duckdb\nimport time\nimport numpy as np\nimport pyfixest as pf\n\n# %%\nnyc = duckdb.sql(\n    '''\n    FROM 'C:/Users/alexa/Documents/nyc-taxi/**/*.parquet'\n    SELECT\n        tip_amount, trip_distance, passenger_count,\n        vendor_id, payment_type, dropoff_at,\n        dayofweek(dropoff_at) AS dofw\n    WHERE year = 2012 AND month &lt;= 3\n    '''\n    ).df()\n\n# convert dowf, vendor_id, payment_type to categorical\ntic = time.time()\nnyc[\"dofw\"] = nyc[\"dofw\"].astype(int)\nnyc[\"vendor_id\"] = nyc[\"vendor_id\"].astype(\"category\")\nnyc[\"payment_type\"] = nyc[\"payment_type\"].astype(\"category\")\nprint(f\"\"\"\n    I am convering columns of type 'objects' to 'categories' and 'int'data types outside\n    of the regression, hence I am cheating a bit. This saves {np.round(time.time() - tic)} seconds.\n    \"\"\"\n)\n#    I am convering columns of type 'objects' to 'categories' and 'int'data types outside\n#    of the regression, hence I am cheating a bit. This saves 7.0 seconds.\n\nrun = True\nif run:\n\n    # mock regression for JIT compilation\n    fit = pf.feols(\n        fml = \"tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw\",\n        data = nyc.iloc[1:10_000],\n        copy_data = False,\n        store_data = False\n        )\n\n    import time\n    tic = time.time()\n    fit = pf.feols(\n        fml = \"tip_amount ~ trip_distance + passenger_count | vendor_id + payment_type + dofw\",\n        data = nyc,\n        copy_data = False, # saves a few seconds\n        store_data = False # saves a few second\n        )\n    passed = time.time() - tic\n    print(f\"Passed time is {np.round(passed)}.\")\n    # Passed time is 22.\n\n\nAdds three new function arguments to feols() and fepois(): copy_data, store_data, and fixef_tol.\nAdds support for frequency weights with the weights_type function argument.\n\n\nimport pyfixest as pf\n\ndata = pf.get_data(N = 10000, model = \"Fepois\")\ndf_weighted = data[[\"Y\", \"X1\", \"f1\"]].groupby([\"Y\", \"X1\", \"f1\"]).size().reset_index().rename(columns={0: \"count\"})\ndf_weighted[\"id\"] = list(range(df_weighted.shape[0]))\n\nprint(\"Dimension of the aggregated df:\", df_weighted.shape)\nprint(df_weighted.head())\n\nfit = pf.feols(\n    \"Y ~ X1 | f1\",\n    data = data\n)\nfit_weighted = pf.feols(\n    \"Y ~ X1 | f1\",\n    data = df_weighted,\n    weights = \"count\",\n    weights_type = \"fweights\"\n)\npf.etable([fit, fit_weighted], coef_fmt = \"b(se) \\n (t) \\n (p)\")\n\nDimension of the aggregated df: (1278, 5)\n     Y   X1   f1  count  id\n0  0.0  0.0  0.0     17   0\n1  0.0  0.0  1.0     11   1\n2  0.0  0.0  2.0     10   2\n3  0.0  0.0  3.0     17   3\n4  0.0  0.0  4.0     14   4\n\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/model_matrix_fixest_.py:221: UserWarning: 1 singleton fixed effect(s) detected. These observations are dropped from the model.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY\n\n\n(1)\n(2)\n\n\n\n\ncoef\n\n\nX1\n0.001(0.012)\n(0.088)\n(0.93)\n0.001(0.012)\n(0.088)\n(0.93)\n\n\nfe\n\n\nf1\nx\nx\n\n\nstats\n\n\nObservations\n9,996\n9,996\n\n\nR2\n0.011\n0.011\n\n\n\nSignificance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001. Format of coefficient cell: Coefficient(Std. Error) (t-stats) (p-value)\n\n\n\n\n\n\n\n\n\n\nBugfix: Wild Cluster Bootstrap Inference with Weights would compute unweighted standard errors. Sorry about that! WLS is not supported for the WCB.\nAdds support for CRV3 inference with weights."
  },
  {
    "objectID": "changelog.html#pyfixest-0.18.0",
    "href": "changelog.html#pyfixest-0.18.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Large Refactoring of Interal Processing of Model Formulas, in particular FixestFormulaParser and model_matrix_fixest. As a results, the code should be cleaner and more robust.\nThanks to the refactoring, we can now bump the required formulaic version to the stable 1.0.0 release.\nThe fml argument of model_matrix_fixest is deprecated. Instead, model_matrix_fixest now asks for a FixestFormula, which is essentially a dictionary with information on model specifications like a first stage formula (if applicable), dependent variables, fixed effects, etc.\nAdditionally, model_matrix_fixest now returns a dictionary instead of a tuple.\nBrings back fixed effects reference setting via i(var1, var2, ref) syntax. Deprecates the i_ref1, i_ref2 function arguments. I.e. it is again possible to e.g. run\n\n\nimport pyfixest as pf\ndata = pf.get_data()\n\nfit1 = pf.feols(\"Y ~ i(f1, X2)\", data=data)\nfit1.coef()[0:8]\n\nVia the ref syntax, via can set the reference level:\n\nfit2 = pf.feols(\"Y ~ i(f1, X2, ref = 1)\", data=data)\nfit2.coef()[0:8]"
  },
  {
    "objectID": "changelog.html#pyfixest-0.17.0",
    "href": "changelog.html#pyfixest-0.17.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Restructures the codebase and reorganizes how users can interact with the pyfixest API. It is now recommended to use pyfixest in the following way:\n\nimport numpy as np\nimport pyfixest as pf\ndata = pf.get_data()\ndata[\"D\"] = data[\"X1\"] &gt; 0\nfit = pf.feols(\"Y ~ D + f1\", data = data)\nfit.tidy()\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\nCoefficient\n\n\n\n\n\n\n\n\n\n\nIntercept\n0.778849\n0.170261\n4.574437\n0.000005\n0.444737\n1.112961\n\n\nD\n-1.402617\n0.152224\n-9.214140\n0.000000\n-1.701335\n-1.103899\n\n\nf1\n0.004774\n0.008058\n0.592508\n0.553645\n-0.011038\n0.020587\n\n\n\n\n\n\n\nThe update should not inroduce any breaking changes. Thanks to @Wenzhi-Ding for the PR!\nAdds support for simultaneous confidence intervals via a multiplier bootstrap. Thanks to @apoorvalal for the contribution!\n\nfit.confint(joint = True)\n\n\n\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nIntercept\n0.381047\n1.176651\n\n\nD\n-1.758278\n-1.046956\n\n\nf1\n-0.014052\n0.023601\n\n\n\n\n\n\n\nAdds support for the causal cluster variance estimator by Abadie et al. (QJE, 2023) for OLS via the .ccv() method.\n\nfit.ccv(treatment = \"D\", cluster = \"group_id\")\n\n/home/runner/work/pyfixest/pyfixest/pyfixest/estimation/feols_.py:1588: UserWarning: The initial model was not clustered. CRV1 inference is computed and stored in the model object.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\nCCV\n-1.4026168622179929\n0.215624\n-6.504907\n0.000004\n-1.855627\n-0.949607\n\n\nCRV1\n-1.402617\n0.205132\n-6.837621\n0.000002\n-1.833584\n-0.97165"
  },
  {
    "objectID": "changelog.html#pyfixest-0.16.0",
    "href": "changelog.html#pyfixest-0.16.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds multiple quality of life improvements for developers, thanks to NKeleher.\nAdds more options to customize etable() output thanks to Wenzhi-Ding.\nImplements Romano-Wolf and Bonferroni corrections for multiple testing in the multcomp module."
  },
  {
    "objectID": "changelog.html#pyfixest-0.15.",
    "href": "changelog.html#pyfixest-0.15.",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds support for weighted least squares for feols().\nReduces testing time drastically by running tests on fewer random data samples. Qualitatively, the set of test remains identical.\nSome updates for future pandas compatibility."
  },
  {
    "objectID": "changelog.html#pyfixest-0.14.0",
    "href": "changelog.html#pyfixest-0.14.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Moves the documentation to quartodoc.\nChanges all docstrings to numpy format.\nDifference-in-differences estimation functions now need to be imported via the pyfixest.did.estimation module:\n\n\nfrom pyfixest.did.estimation import did2s, lpdid, event_study"
  },
  {
    "objectID": "changelog.html#pyfixest-0.13.5",
    "href": "changelog.html#pyfixest-0.13.5",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a bug that lead to incorrect results when the dependent variable and all covariates (excluding the fixed effects) where integers."
  },
  {
    "objectID": "changelog.html#pyfixest-0.13.4",
    "href": "changelog.html#pyfixest-0.13.4",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a bug in etable() with IV’s that occurred because feols() does not report R2 statistics for IVs."
  },
  {
    "objectID": "changelog.html#pyfixest-0.13.2",
    "href": "changelog.html#pyfixest-0.13.2",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a bug in etable() and a warning in fixest_model_matrix that arose with higher pandas versions. Thanks to @aeturrell for reporting!"
  },
  {
    "objectID": "changelog.html#pyfixest-0.13.0",
    "href": "changelog.html#pyfixest-0.13.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Introduces a new pyfixest.did module which contains routines for Difference-in-Differences estimation.\nIntroduces support for basic versions of the local projections DiD estimator following Dube et al (2023)\nAdds a new vignette for Difference-in-Differences estimation.\nReports R2 values in etable()."
  },
  {
    "objectID": "changelog.html#pyfixest-0.12.0",
    "href": "changelog.html#pyfixest-0.12.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Good performance improvements for singleton fixed effects detection. Thanks to @styfenschaer for the PR! See #229.\nUses the r2u project for installing R and R packages on github actions, with great performance improvements.\nAllows to pass polars data frames to feols(), fepois() and predict(). #232. Thanks to @vincentarelbundock for the suggestion!\n\n\n\n\n\nMissing variables in features were not always handled correctly in predict() with newdata not None in the presence of missing data, which would lead to an error. See #246 for details.\nCategorical variables were not always handled correctly in predict() with newdata not None, because the number of fixed effects levels in newdata might be smaller than in data. In consequence, some levels were not found, which lead to an error. See #245 for details. Thanks to @jiafengkevinchen for the pointer!\nMulticollinearity checks for over-identified IV was not implemented correctly, which lead to a dimension error. See #236 for details. Thanks to @jiafengkevinchen for the pointer!\nThe number of degrees of freedom k was computed incorrectly if columns were dropped from the design matrix X in the presence of multicollinearity. See #235 for details. Thanks to @jiafengkevinchen for the pointer!\nIf all variables were dropped due to multicollinearity, an unclear and imprecise error message was produced. See #228 for details. Thanks to @manferdinig for the pointer!\nIf selection fixef_rm = 'singleton', feols() and fepois() would fail, which has been fixed. #192\n\n\n\n\n\nFor now, sets formulaic versions to be 0.6.6 or lower as version 1.0.0 seems to have introduced a problem with the i() operator, See #244 for details.\nDrops dependency on pyhdfe."
  },
  {
    "objectID": "changelog.html#pyfixest-0.11.1",
    "href": "changelog.html#pyfixest-0.11.1",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes some bugs around the computation of R-squared values (see issue #103).\nReports R-squared values again when calling .summary()."
  },
  {
    "objectID": "changelog.html#pyfixest-0.11.0",
    "href": "changelog.html#pyfixest-0.11.0",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Significant speedups for CRV1 inference."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.12",
    "href": "changelog.html#pyfixest-0.10.12",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a small bug with the separation check for poisson regression #138."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.11",
    "href": "changelog.html#pyfixest-0.10.11",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes bugs with i(var1, var2) syntax introduced with PyFixest 0.10.10."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.10",
    "href": "changelog.html#pyfixest-0.10.10",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a bug with variable interactions via i(var) syntax. See issue #221."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.9",
    "href": "changelog.html#pyfixest-0.10.9",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Makes etable() prettier and more informative."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.8",
    "href": "changelog.html#pyfixest-0.10.8",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Reference levels for the i() formula syntax can no longer be set within the formula, but need to be added via the i_ref1 function argument to either feols() and fepois().\n\n\n\nA dids2() function is added, which implements the 2-stage difference-in-differences procedure à la Gardner and follows the syntax of @kylebutts did2s R package.\nfrom pyfixest.did.did import did2s\nfrom pyfixest.estimation import feols\nfrom pyfixest.visualize import iplot\nimport pandas as pd\nimport numpy as np\n\ndf_het = pd.read_csv(\"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\")\n\nfit = did2s(\n    df_het,\n    yname = \"dep_var\",\n    first_stage = \"~ 0 | state + year\",\n    second_stage = \"~i(rel_year)\",\n    treatment = \"treat\",\n    cluster = \"state\",\n    i_ref1 = [-1.0, np.inf],\n)\n\nfit_twfe = feols(\n    \"dep_var ~ i(rel_year) | state + year\",\n    df_het,\n    i_ref1 = [-1.0, np.inf]\n)\n\niplot([fit, fit_twfe], coord_flip=False, figsize = (900, 400), title = \"TWFE vs DID2S\")"
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.7",
    "href": "changelog.html#pyfixest-0.10.7",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds basic support for event study estimation via two-way fixed effects and Gardner’s two-stage “Did2s” approach. This is a beta version and experimental. Further updates (i.e. proper event studies vs “only” ATTs) and a more flexible did2s front end will follow in future releases.\n\n%load_ext autoreload\n%autoreload 2\n\nfrom pyfixest.did.did import event_study\nimport pyfixest as pf\nimport pandas as pd\ndf_het = pd.read_csv(\"pyfixest/did/data/df_het.csv\")\n\nfit_twfe = event_study(\n    data = df_het,\n    yname = \"dep_var\",\n    idname= \"state\",\n    tname = \"year\",\n    gname = \"g\",\n    estimator = \"twfe\"\n)\n\nfit_did2s = event_study(\n    data = df_het,\n    yname = \"dep_var\",\n    idname= \"state\",\n    tname = \"year\",\n    gname = \"g\",\n    estimator = \"did2s\"\n)\n\npf.etable([fit_twfe, fit_did2s])\n# | Coefficient   | est1             | est2             |\n# |:--------------|:-----------------|:-----------------|\n# | ATT           | 2.135*** (0.044) | 2.152*** (0.048) |\n# Significance levels: * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.6",
    "href": "changelog.html#pyfixest-0.10.6",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds an etable() function that outputs markdown, latex or a pd.DataFrame."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.5",
    "href": "changelog.html#pyfixest-0.10.5",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a big in IV estimation that would trigger an error. See here for details. Thanks to @aeturrell for reporting!"
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.4",
    "href": "changelog.html#pyfixest-0.10.4",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Implements a custom function to drop singleton fixed effects.\nAdditional small performance improvements."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.3",
    "href": "changelog.html#pyfixest-0.10.3",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Allows for white space in the multiway clustering formula.\nAdds documentation for multiway clustering."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.2",
    "href": "changelog.html#pyfixest-0.10.2",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds support for two-way clustering.\nAdds support for CRV3 inference for Poisson regression."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10.1",
    "href": "changelog.html#pyfixest-0.10.1",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adapts the internal fixed effects demeaning criteron to match `PyHDFE’s default.\nAdds Styfen as coauthor."
  },
  {
    "objectID": "changelog.html#pyfixest-0.10",
    "href": "changelog.html#pyfixest-0.10",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Multiple performance improvements.\nMost importantly, implements a custom demeaning algorithm in numba - thanks to Styfen Schaer (@styfenschaer), which leads to performance improvements of 5x or more:\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport time\nimport pyhdfe\nfrom pyfixest.demean import demean\n\nnp.random.seed(1238)\nN = 10_000_000\nx = np.random.normal(0, 1, 10*N).reshape((N,10))\nf1 = np.random.choice(list(range(1000)), N).reshape((N,1))\nf2 = np.random.choice(list(range(1000)), N).reshape((N,1))\n\nflist = np.concatenate((f1, f2), axis = 1)\nweights = np.ones(N)\n\nalgorithm = pyhdfe.create(flist)\n\nstart_time = time.time()\nres_pyhdfe = algorithm.residualize(x)\nend_time = time.time()\nprint(end_time - start_time)\n# 26.04527711868286\n\n\nstart_time = time.time()\nres_pyfixest, success = demean(x, flist, weights, tol = 1e-10)\n# Calculate the execution time\nend_time = time.time()\nprint(end_time - start_time)\n#4.334428071975708\n\nnp.allclose(res_pyhdfe , res_pyfixest)\n# True"
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.11",
    "href": "changelog.html#pyfixest-0.9.11",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Bump required formulaic version to 0.6.5.\nStop copying the data frame in fixef()."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.10",
    "href": "changelog.html#pyfixest-0.9.10",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a big in the wildboottest method (see #158).\nAllows to run a wild bootstrap after fixed effect estimation."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.9",
    "href": "changelog.html#pyfixest-0.9.9",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Adds support for wildboottest for Python 3.11."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.8",
    "href": "changelog.html#pyfixest-0.9.8",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a couple more bugs in the predict() and fixef() methods.\nThe predict() argument data is renamed to newdata."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.7",
    "href": "changelog.html#pyfixest-0.9.7",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Fixes a bug in predict() produced when multicollinear variables are dropped."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.6",
    "href": "changelog.html#pyfixest-0.9.6",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Improved Collinearity handling. See #145"
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.5",
    "href": "changelog.html#pyfixest-0.9.5",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "Moves plotting from matplotlib to lets-plot.\nFixes a few minor bugs in plotting and the fixef() method."
  },
  {
    "objectID": "changelog.html#pyfixest-0.9.1",
    "href": "changelog.html#pyfixest-0.9.1",
    "title": "PyFixest 0.41.0 (In Development)",
    "section": "",
    "text": "It is no longer required to initiate an object of type Fixest prior to running Feols or fepois. Instead, you can now simply use feols() and fepois() as functions, just as in fixest. Both function can be found in an estimation module and need to obtain a pd.DataFrame as a function argument:\nfrom pyfixest.estimation import fixest, fepois\nfrom pyfixest.utils import get_data\n\ndata = get_data()\nfit = feols(\"Y ~ X1 | f1\", data = data, vcov = \"iid\")\nCalling feols() will return an instance of class Feols, while calling fepois() will return an instance of class Fepois. Multiple estimation syntax will return an instance of class FixestMulti.\nPost processing works as before via .summary(), .tidy() and other methods.\n\n\n\nA summary function allows to compare multiple models:\nfrom pyfixest.summarize import summary\nfit2 = feols(\"Y ~ X1 + X2| f1\", data = data, vcov = \"iid\")\nsummary([fit, fit2])\nVisualization is possible via custom methods (.iplot() & .coefplot()), but a new module allows to visualize a list of Feols and/or Fepois instances:\nfrom pyfixest.visualize import coefplot, iplot\ncoefplot([fit, fit2])\nThe documentation has been improved (though there is still room for progress), and the code has been cleaned up a bit (also lots of room for improvements)."
  }
]