---
title: Multiple Hypothesis Testing Corrections
aliases:
  - /multiple_testing.html
execute:
  eval: false
jupyter: python3
---



When conducting online A/B tests or large-scale experiments, we often analyze multiple dependent variables simultaneously. Analyzing multiple KPIs introduces a significant statistical challenge: the multiple testing problem.

In classical hypothesis testing, the significance level $\alpha$ controls the probability of a false positive (Type I error), typically $\alpha=0.05$. While this ensures a 5% chance of a false positive for a single test, the likelihood of detecting at least one false positive grows rapidly when conducting multiple tests. For example, testing 20 KPIs independently at $\alpha = 0.05$ results in a 64% chance of at least one false positive—calculated as $1 - (1 - 0.05)^{20} \approx 0.64$. This issue, known as the multiple testing problem, can lead to false claims of significant effects when none exist.

To address this, the concept of controlling the **Familywise Error Rate (FWER)** has been widely adopted. FWER controls the probability of at least one Type I error across a family of hypotheses. Several correction methods exist to mitigate the multiple testing problem, including:

- **Bonferroni Correction**: A simple and conservative method that adjusts the significance level for each test by dividing $\alpha$ by the number of tests.
- **Romano-Wolf & Westfall-Young Stepwise Procedures**: Two more powerful methods that use resampling techniques to control the FWER.

This vignette demonstrates how these methods effectively control the FWER in a variety of scenarios. We will compare their performance and highlight the trade-offs between simplicity and statistical power. Specifically, we show that while Bonferroni provides strong error control, it is conservative in many practical applications. In contrast, Romano-Wolf and Westfall-Young methods are more powerful, offering greater sensitivity to detect true effects while maintaining robust control of the FWER.

## What is a Family-Wise Error Rate (FWER)? 

Suppose that we are running an experiment and want to test if our treatment impacts 20 different dependent variables (KPIs). For any given independent test, the chance of a false positive is given by the (significance) **level** of the individual test, which is most commonly set to $\alpha = 0.05$. Formally, we can define the false positive rate for a single hypothesis $H$ as:

$$
P(\text{reject } H \mid H \text{ is true}) = \alpha
$$

For a **family of tests** $S = {s \in \{1, \dots, P\} }$ hypotheses $\{H_{s}\}_{s \in S}$, we can analogously define the **family-wise error rate (FWER)** as:

$$
P(\text{reject at least one } H_{s} \text{ with } s \in I) = \alpha
$$

where $I$ is the set of **true hypotheses**. In other words, the FWER is the probability of making at least one false positive across all tests in the family.

## Setup

In a first step, we create a data set with multiple (potentially correlated) dependent variables that share a common set of covariates. 
All simulations in this notebook are greatly inspired by Clarke, Romano & Wolf (Stata Journal, 2020).

```{python}
%load_ext autoreload
%autoreload 2

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from great_tables import loc, style
from joblib import Parallel, delayed
from tqdm import tqdm

import pyfixest as pf
```

```{python}
N = 100
n_covariates = 5
n_depvars = 20
```

```{python}
def get_data(N, n_covariates, n_depvars, rho, seed):
    "Simulate data with true nulls."
    rng = np.random.default_rng(seed)
    Omega = np.eye(n_depvars)
    X = rng.standard_normal((N, n_covariates))
    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)
    beta = np.zeros((n_covariates, n_depvars))
    y = X @ beta + u_joint

    data = pd.DataFrame(X, columns=[f"X{i}" for i in range(n_covariates)])
    data = data.assign(**{f"y{i}": y[:, i] for i in range(n_depvars)})

    return data
```

```{python}
data = get_data(
    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345
)
data.head()
```

Now that we have our data set at hand, we can fit 20 regression models - one for each dependent variable. To do so, we will use 
pyfixest's multiple estimation syntax.

```{python}
dependent_vars = " + ".join([f"y{i}" for i in range(20)])
independent_vars = " + ".join([f"X{i}" for i in range(5)])
fml = f"{dependent_vars} ~ {independent_vars}"
```

```{python}
fit = pf.feols(fml, data=data, vcov="hetero")
```

```{python}
(pf.etable(fit).tab_style(style=style.fill(color="yellow"), locations=loc.body(rows=1)))
```

We see that our estimation produces multiple false positives for the effect of X1 on multiple dependent variables. Recall that 
we had simulated X1 to have **no effect** on any of the dependent variables. Still, the estimation procedure produces a significant
effect for X1 on multiple dependent variables. 

`PyFixest` provides three functions to adjust inference for multiple testing: `pf.bonferroni()`, `pf.rwolf()`, and `pf.wyoung()`.
All three share a common API.

```{python}
pval_bonferroni = (
    pf.bonferroni(fit.to_list(), param="X1").xs("Bonferroni Pr(>|t|)").values
)

pval_rwolf = (
    pf.rwolf(fit.to_list(), param="X1", reps=1000, seed=22).xs("RW Pr(>|t|)").values
)

pval_wyoung = (
    pf.wyoung(fit.to_list(), param="X1", reps=1000, seed=22).xs("WY Pr(>|t|)").values
)
```

```{python}
(
    pf.etable(
        fit,
        custom_model_stats={
            "Bonferroni: pval(X1)": pval_bonferroni.round(4).tolist(),
            "RW: pval(X1)": pval_rwolf.round(4).tolist(),
            "WY: pval(X1)": pval_wyoung.round(4).tolist(),
        },
    ).tab_style(style=style.fill(color="yellow"), locations=loc.body(rows=[6, 7, 8]))
)
```

We quickly see that the corrected p-values do not flag any false positives. 

## Controlling for the Familiy-Wise Error Rate (FWER)

We now show by means of simulation that the three methods control the family-wise error rate (FWER). To do so, we simulate 1000 
data sets imposing true nulls for the effect of X1 on all of 20 created dependent variables. 
For each simulation, we then count if the methods flag more than one false positive and report our results. 

```{python}
def compute_family_rejection(seed, rho):
    "Simulate data, estimate models, and compute family rejection rates."
    data = get_data(N, n_covariates, n_depvars, rho, seed=seed)
    fit = pf.feols(fml=fml, data=data, vcov="hetero")
    df = fit.tidy().reset_index().set_index("Coefficient").xs("X1")

    df["Pr(>|t|) reject"] = df["Pr(>|t|)"] < 0.05
    df["Bonferroni reject"] = (
        pf.bonferroni(fit, param="X1").xs("Bonferroni Pr(>|t|)").values < 0.05
    )
    df["rwolf reject"] = (
        pf.rwolf(fit, param="X1", reps=1000, seed=seed * 11).xs("RW Pr(>|t|)").values
        < 0.05
    )
    df["wyoung reject"] = (
        pf.wyoung(fit, param="X1", reps=1000, seed=seed * 11).xs("WY Pr(>|t|)").values
        < 0.05
    )

    # Compute family rejection means
    family_rejection = {
        "Pr(>|t|) reject family": df["Pr(>|t|) reject"].sum() > 0,
        "Bonferroni reject family": df["Bonferroni reject"].sum() > 0,
        "rwolf reject family": df["rwolf reject"].sum() > 0,
        "wyoung reject family": df["wyoung reject"].sum() > 0,
    }

    return pd.Series(family_rejection)
```

```{python}
def run_fwer_simulation(n_iter, rho):
    "Run simulation for family-wise error rate."
    results = Parallel(n_jobs=-1)(
        delayed(compute_family_rejection)(seed, rho=rho) for seed in tqdm(range(n_iter))
    )
    return pd.concat(results).reset_index().groupby("index").mean()
```

```{python}
run_fwer_simulation(n_iter=1000, rho=0.5)
```

We see that all three correction methods get close to the desired 5% level. In contrast, the uncorrected method produces the expected much higher family-wise error rate. 

## Power 

Now that we've seen that all three methods effectively handle false positives, let's see how well they avoid **false negatives**. 
In other words, we will study how powerful all three methods are in detecting true effects. 

To do so, we slightly have to adjust the data generating process. Instead of simulating the impact of X1 on all dependent variables 
to be zero (a true null effect), we will now simulate the impact of X1 to be $0.5$ for all dependent variables. Hence 
we simulate **true positives** and count how often we correctly detect the true effect, or, equivalently stated, how often we correctly reject 
the null of no treatment effect. 

```{python}
def get_data_true_effect(N, n_covariates, n_depvars, rho=0.5, seed=12345, effect=0.1):
    "Generate data with true positives."
    rng = np.random.default_rng(seed)
    Omega = np.eye(n_depvars)
    Omega[Omega != 1] = rho
    X = rng.standard_normal((N, n_covariates))
    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)
    beta = np.zeros((n_covariates, n_depvars))
    beta[1, :] = effect
    y = X @ beta + u_joint

    data = pd.DataFrame(X, columns=[f"X{i}" for i in range(n_covariates)])
    data = data.assign(**{f"y{i}": y[:, i] for i in range(n_depvars)})

    return data
```

```{python}
data_true = get_data_true_effect(
    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345, effect=0.5
)
fit = pf.feols(fml, data=data_true)
```

```{python}
(
    pf.etable(fit).tab_style(
        style=style.fill(color="yellow"), locations=loc.body(rows=[1])
    )
)
```

We will now study power more systematically via a simulation. More concretely, we will compute how often 
we detect the **true effect** of **X1 on Y1, Y2, ..., etc** given a fixed sample size $N$ using "uncorrected" p-values, 
the Bonferroni, Romano-Wolf and Westfall-Young methods. 

```{python}
def compute_power(seed, rho, effect):
    "Simulate data, estimate models, and compute power."
    data = get_data_true_effect(
        N, n_covariates, n_depvars, rho, seed=seed, effect=effect
    )
    fit = pf.feols(
        fml=fml, data=data, vcov="hetero"
    )  # model '1' regresses on Y1 - we're only interested in the power of this specific test
    df = fit.tidy().reset_index().set_index("Coefficient").xs("X1")

    df["Pr(>|t|) detect"] = df["Pr(>|t|)"] < 0.05
    df["Bonferroni detect"] = (
        pf.bonferroni(fit, param="X1").xs("Bonferroni Pr(>|t|)").values < 0.05
    )
    df["rwolf detect"] = (
        pf.rwolf(fit, param="X1", reps=200, seed=seed * 11).xs("RW Pr(>|t|)").values
        < 0.05
    )
    df["wyoung detect"] = (
        pf.wyoung(fit, param="X1", reps=200, seed=seed * 11).xs("WY Pr(>|t|)").values
        < 0.05
    )

    # Compute family rejection means
    detect_effect = {
        "Pr(>|t|) detect effect": df["Pr(>|t|) detect"].mean(),
        "Bonferroni detect effect": df["Bonferroni detect"].mean(),
        "rwolf detect effect": df["rwolf detect"].mean(),
        "wyoung detect effect": df["wyoung detect"].mean(),
    }

    detect_effect_df = pd.DataFrame(detect_effect, index=[effect])
    return detect_effect_df
```

```{python}
def run_power_simulation(n_iter, rho, effect, nthreads=-1):
    "Run simulation for power."
    seeds = list(range(n_iter))
    results = Parallel(n_jobs=nthreads)(
        delayed(compute_power)(seed, rho=rho, effect=effect) for seed in tqdm(seeds)
    )

    return pd.concat(results).mean()
```

```{python}
run_power_simulation(n_iter=1000, rho=0.5, effect=0.4)
```

We see that the "unadjusted" method detects the "true effects" at the highest frequency with on average 97% correctly detected effects. Does this mean that we should use uncorrected tests then? Well, maybe, but these do not control the family-wise error rate. While we have a better chance to detect a true effect, we also have a higher risk of flagging false positives. 

Additionally, it looks as if the rwolf and wyoung methods detect the true positives at a slightly higher rate than the Bonferroni method. 

Do these findings generalize to other effect sizes? We can check this by simply imposing 
different effects on the data generating process and repeating the previous exercise 
multiple times. 

```{python}
def run_power_simulation_vary_effect():
    "Run power simulations with varying effect sizes."
    n_points = 10
    max_val = 0.7

    effects = (
        np.sign(np.linspace(-1, 1, n_points))
        * max_val
        * (np.linspace(-1, 1, n_points) ** 2)
    )

    res_list = []
    for effect_size in tqdm(effects):
        res = run_power_simulation(n_iter=1000, rho=0.5, effect=effect_size)
        res["effect"] = effect_size
        res_list.append(res)
    return pd.concat(res_list, axis=1).T.set_index("effect")


res = run_power_simulation_vary_effect()
```

```{python}
column_to_label_dict = {
    "Pr(>|t|) detect effect": "Unadjusted",
    "Bonferroni detect effect": "Bonferroni",
    "rwolf detect effect": "RW",
    "wyoung detect effect": "WY",
}

plt.figure(figsize=(10, 6))
line_styles = ["-", "--", "-.", ":"]
for column, line_style in zip(res.columns, line_styles):
    plt.plot(
        res.index, res[column], linestyle=line_style, label=column_to_label_dict[column]
    )

plt.title("Power of Multiple Testing Correction Procedures")
plt.xlabel("Effect")
plt.ylabel("Proportion of correctly detected effects")
plt.legend()
plt.grid()
plt.show()
```

We see that for any simulated effect size, the Romano-Wolf and Westfall-Young methods detect a higher share of true positives than the Bonferroni method: they have **higher power**.

## Literature 

- Clarke, Damian, Joseph P. Romano, and Michael Wolf. "The Romano–Wolf multiple-hypothesis correction in Stata." The Stata Journal 20.4 (2020): 812-843.
- Romano, Joseph P., and Michael Wolf. "Stepwise multiple testing as formalized data snooping." Econometrica 73.4 (2005): 1237-1282.
- Westfall, Peter H., and S. Stanley Young. Resampling-based multiple testing: Examples and methods for p-value adjustment. Vol. 279. John Wiley & Sons, 1993.

