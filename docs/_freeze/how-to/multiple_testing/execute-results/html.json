{
  "hash": "191ab427b7e94d4bf0e75631780bdbf1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Multiple Hypothesis Testing Corrections\naliases:\n  - /multiple_testing.html\nexecute:\n  eval: false\njupyter: python3\n---\n\n\n\nWhen conducting online A/B tests or large-scale experiments, we often analyze multiple dependent variables simultaneously. Analyzing multiple KPIs introduces a significant statistical challenge: the multiple testing problem.\n\nIn classical hypothesis testing, the significance level $\\alpha$ controls the probability of a false positive (Type I error), typically $\\alpha=0.05$. While this ensures a 5% chance of a false positive for a single test, the likelihood of detecting at least one false positive grows rapidly when conducting multiple tests. For example, testing 20 KPIs independently at $\\alpha = 0.05$ results in a 64% chance of at least one false positive—calculated as $1 - (1 - 0.05)^{20} \\approx 0.64$. This issue, known as the multiple testing problem, can lead to false claims of significant effects when none exist.\n\nTo address this, the concept of controlling the **Familywise Error Rate (FWER)** has been widely adopted. FWER controls the probability of at least one Type I error across a family of hypotheses. Several correction methods exist to mitigate the multiple testing problem, including:\n\n- **Bonferroni Correction**: A simple and conservative method that adjusts the significance level for each test by dividing $\\alpha$ by the number of tests.\n- **Romano-Wolf & Westfall-Young Stepwise Procedures**: Two more powerful methods that use resampling techniques to control the FWER.\n\nThis vignette demonstrates how these methods effectively control the FWER in a variety of scenarios. We will compare their performance and highlight the trade-offs between simplicity and statistical power. Specifically, we show that while Bonferroni provides strong error control, it is conservative in many practical applications. In contrast, Romano-Wolf and Westfall-Young methods are more powerful, offering greater sensitivity to detect true effects while maintaining robust control of the FWER.\n\n## What is a Family-Wise Error Rate (FWER)? \n\nSuppose that we are running an experiment and want to test if our treatment impacts 20 different dependent variables (KPIs). For any given independent test, the chance of a false positive is given by the (significance) **level** of the individual test, which is most commonly set to $\\alpha = 0.05$. Formally, we can define the false positive rate for a single hypothesis $H$ as:\n\n$$\nP(\\text{reject } H \\mid H \\text{ is true}) = \\alpha\n$$\n\nFor a **family of tests** $S = {s \\in \\{1, \\dots, P\\} }$ hypotheses $\\{H_{s}\\}_{s \\in S}$, we can analogously define the **family-wise error rate (FWER)** as:\n\n$$\nP(\\text{reject at least one } H_{s} \\text{ with } s \\in I) = \\alpha\n$$\n\nwhere $I$ is the set of **true hypotheses**. In other words, the FWER is the probability of making at least one false positive across all tests in the family.\n\n## Setup\n\nIn a first step, we create a data set with multiple (potentially correlated) dependent variables that share a common set of covariates. \nAll simulations in this notebook are greatly inspired by Clarke, Romano & Wolf (Stata Journal, 2020).\n\n::: {#28a18556 .cell execution_count=1}\n``` {.python .cell-code}\n%load_ext autoreload\n%autoreload 2\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom great_tables import loc, style\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\n\nimport pyfixest as pf\n```\n:::\n\n\n::: {#28aaf2cd .cell execution_count=2}\n``` {.python .cell-code}\nN = 100\nn_covariates = 5\nn_depvars = 20\n```\n:::\n\n\n::: {#6a778097 .cell execution_count=3}\n``` {.python .cell-code}\ndef get_data(N, n_covariates, n_depvars, rho, seed):\n    \"Simulate data with true nulls.\"\n    rng = np.random.default_rng(seed)\n    Omega = np.eye(n_depvars)\n    X = rng.standard_normal((N, n_covariates))\n    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)\n    beta = np.zeros((n_covariates, n_depvars))\n    y = X @ beta + u_joint\n\n    data = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(n_covariates)])\n    data = data.assign(**{f\"y{i}\": y[:, i] for i in range(n_depvars)})\n\n    return data\n```\n:::\n\n\n::: {#f7de40e8 .cell execution_count=4}\n``` {.python .cell-code}\ndata = get_data(\n    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345\n)\ndata.head()\n```\n:::\n\n\nNow that we have our data set at hand, we can fit 20 regression models - one for each dependent variable. To do so, we will use \npyfixest's multiple estimation syntax.\n\n::: {#a9e87201 .cell execution_count=5}\n``` {.python .cell-code}\ndependent_vars = \" + \".join([f\"y{i}\" for i in range(20)])\nindependent_vars = \" + \".join([f\"X{i}\" for i in range(5)])\nfml = f\"{dependent_vars} ~ {independent_vars}\"\n```\n:::\n\n\n::: {#e1817fe4 .cell execution_count=6}\n``` {.python .cell-code}\nfit = pf.feols(fml, data=data, vcov=\"hetero\")\n```\n:::\n\n\n::: {#793c5882 .cell execution_count=7}\n``` {.python .cell-code}\n(pf.etable(fit).tab_style(style=style.fill(color=\"yellow\"), locations=loc.body(rows=1)))\n```\n:::\n\n\nWe see that our estimation produces multiple false positives for the effect of X1 on multiple dependent variables. Recall that \nwe had simulated X1 to have **no effect** on any of the dependent variables. Still, the estimation procedure produces a significant\neffect for X1 on multiple dependent variables. \n\n`PyFixest` provides three functions to adjust inference for multiple testing: `pf.bonferroni()`, `pf.rwolf()`, and `pf.wyoung()`.\nAll three share a common API.\n\n::: {#5aee3634 .cell execution_count=8}\n``` {.python .cell-code}\npval_bonferroni = (\n    pf.bonferroni(fit.to_list(), param=\"X1\").xs(\"Bonferroni Pr(>|t|)\").values\n)\n\npval_rwolf = (\n    pf.rwolf(fit.to_list(), param=\"X1\", reps=1000, seed=22).xs(\"RW Pr(>|t|)\").values\n)\n\npval_wyoung = (\n    pf.wyoung(fit.to_list(), param=\"X1\", reps=1000, seed=22).xs(\"WY Pr(>|t|)\").values\n)\n```\n:::\n\n\n::: {#f217b22b .cell execution_count=9}\n``` {.python .cell-code}\n(\n    pf.etable(\n        fit,\n        custom_model_stats={\n            \"Bonferroni: pval(X1)\": pval_bonferroni.round(4).tolist(),\n            \"RW: pval(X1)\": pval_rwolf.round(4).tolist(),\n            \"WY: pval(X1)\": pval_wyoung.round(4).tolist(),\n        },\n    ).tab_style(style=style.fill(color=\"yellow\"), locations=loc.body(rows=[6, 7, 8]))\n)\n```\n:::\n\n\nWe quickly see that the corrected p-values do not flag any false positives. \n\n## Controlling for the Familiy-Wise Error Rate (FWER)\n\nWe now show by means of simulation that the three methods control the family-wise error rate (FWER). To do so, we simulate 1000 \ndata sets imposing true nulls for the effect of X1 on all of 20 created dependent variables. \nFor each simulation, we then count if the methods flag more than one false positive and report our results. \n\n::: {#a57841e0 .cell execution_count=10}\n``` {.python .cell-code}\ndef compute_family_rejection(seed, rho):\n    \"Simulate data, estimate models, and compute family rejection rates.\"\n    data = get_data(N, n_covariates, n_depvars, rho, seed=seed)\n    fit = pf.feols(fml=fml, data=data, vcov=\"hetero\")\n    df = fit.tidy().reset_index().set_index(\"Coefficient\").xs(\"X1\")\n\n    df[\"Pr(>|t|) reject\"] = df[\"Pr(>|t|)\"] < 0.05\n    df[\"Bonferroni reject\"] = (\n        pf.bonferroni(fit, param=\"X1\").xs(\"Bonferroni Pr(>|t|)\").values < 0.05\n    )\n    df[\"rwolf reject\"] = (\n        pf.rwolf(fit, param=\"X1\", reps=1000, seed=seed * 11).xs(\"RW Pr(>|t|)\").values\n        < 0.05\n    )\n    df[\"wyoung reject\"] = (\n        pf.wyoung(fit, param=\"X1\", reps=1000, seed=seed * 11).xs(\"WY Pr(>|t|)\").values\n        < 0.05\n    )\n\n    # Compute family rejection means\n    family_rejection = {\n        \"Pr(>|t|) reject family\": df[\"Pr(>|t|) reject\"].sum() > 0,\n        \"Bonferroni reject family\": df[\"Bonferroni reject\"].sum() > 0,\n        \"rwolf reject family\": df[\"rwolf reject\"].sum() > 0,\n        \"wyoung reject family\": df[\"wyoung reject\"].sum() > 0,\n    }\n\n    return pd.Series(family_rejection)\n```\n:::\n\n\n::: {#38b42fce .cell execution_count=11}\n``` {.python .cell-code}\ndef run_fwer_simulation(n_iter, rho):\n    \"Run simulation for family-wise error rate.\"\n    results = Parallel(n_jobs=-1)(\n        delayed(compute_family_rejection)(seed, rho=rho) for seed in tqdm(range(n_iter))\n    )\n    return pd.concat(results).reset_index().groupby(\"index\").mean()\n```\n:::\n\n\n::: {#b17a1847 .cell execution_count=12}\n``` {.python .cell-code}\nrun_fwer_simulation(n_iter=1000, rho=0.5)\n```\n:::\n\n\nWe see that all three correction methods get close to the desired 5% level. In contrast, the uncorrected method produces the expected much higher family-wise error rate. \n\n## Power \n\nNow that we've seen that all three methods effectively handle false positives, let's see how well they avoid **false negatives**. \nIn other words, we will study how powerful all three methods are in detecting true effects. \n\nTo do so, we slightly have to adjust the data generating process. Instead of simulating the impact of X1 on all dependent variables \nto be zero (a true null effect), we will now simulate the impact of X1 to be $0.5$ for all dependent variables. Hence \nwe simulate **true positives** and count how often we correctly detect the true effect, or, equivalently stated, how often we correctly reject \nthe null of no treatment effect. \n\n::: {#2e6d4f8c .cell execution_count=13}\n``` {.python .cell-code}\ndef get_data_true_effect(N, n_covariates, n_depvars, rho=0.5, seed=12345, effect=0.1):\n    \"Generate data with true positives.\"\n    rng = np.random.default_rng(seed)\n    Omega = np.eye(n_depvars)\n    Omega[Omega != 1] = rho\n    X = rng.standard_normal((N, n_covariates))\n    u_joint = np.random.multivariate_normal(np.zeros(n_depvars), Omega, N)\n    beta = np.zeros((n_covariates, n_depvars))\n    beta[1, :] = effect\n    y = X @ beta + u_joint\n\n    data = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(n_covariates)])\n    data = data.assign(**{f\"y{i}\": y[:, i] for i in range(n_depvars)})\n\n    return data\n```\n:::\n\n\n::: {#2cb8feea .cell execution_count=14}\n``` {.python .cell-code}\ndata_true = get_data_true_effect(\n    N=N, n_covariates=n_covariates, n_depvars=n_depvars, rho=0.5, seed=12345, effect=0.5\n)\nfit = pf.feols(fml, data=data_true)\n```\n:::\n\n\n::: {#77083708 .cell execution_count=15}\n``` {.python .cell-code}\n(\n    pf.etable(fit).tab_style(\n        style=style.fill(color=\"yellow\"), locations=loc.body(rows=[1])\n    )\n)\n```\n:::\n\n\nWe will now study power more systematically via a simulation. More concretely, we will compute how often \nwe detect the **true effect** of **X1 on Y1, Y2, ..., etc** given a fixed sample size $N$ using \"uncorrected\" p-values, \nthe Bonferroni, Romano-Wolf and Westfall-Young methods. \n\n::: {#7c2538ff .cell execution_count=16}\n``` {.python .cell-code}\ndef compute_power(seed, rho, effect):\n    \"Simulate data, estimate models, and compute power.\"\n    data = get_data_true_effect(\n        N, n_covariates, n_depvars, rho, seed=seed, effect=effect\n    )\n    fit = pf.feols(\n        fml=fml, data=data, vcov=\"hetero\"\n    )  # model '1' regresses on Y1 - we're only interested in the power of this specific test\n    df = fit.tidy().reset_index().set_index(\"Coefficient\").xs(\"X1\")\n\n    df[\"Pr(>|t|) detect\"] = df[\"Pr(>|t|)\"] < 0.05\n    df[\"Bonferroni detect\"] = (\n        pf.bonferroni(fit, param=\"X1\").xs(\"Bonferroni Pr(>|t|)\").values < 0.05\n    )\n    df[\"rwolf detect\"] = (\n        pf.rwolf(fit, param=\"X1\", reps=200, seed=seed * 11).xs(\"RW Pr(>|t|)\").values\n        < 0.05\n    )\n    df[\"wyoung detect\"] = (\n        pf.wyoung(fit, param=\"X1\", reps=200, seed=seed * 11).xs(\"WY Pr(>|t|)\").values\n        < 0.05\n    )\n\n    # Compute family rejection means\n    detect_effect = {\n        \"Pr(>|t|) detect effect\": df[\"Pr(>|t|) detect\"].mean(),\n        \"Bonferroni detect effect\": df[\"Bonferroni detect\"].mean(),\n        \"rwolf detect effect\": df[\"rwolf detect\"].mean(),\n        \"wyoung detect effect\": df[\"wyoung detect\"].mean(),\n    }\n\n    detect_effect_df = pd.DataFrame(detect_effect, index=[effect])\n    return detect_effect_df\n```\n:::\n\n\n::: {#5e2e85e8 .cell execution_count=17}\n``` {.python .cell-code}\ndef run_power_simulation(n_iter, rho, effect, nthreads=-1):\n    \"Run simulation for power.\"\n    seeds = list(range(n_iter))\n    results = Parallel(n_jobs=nthreads)(\n        delayed(compute_power)(seed, rho=rho, effect=effect) for seed in tqdm(seeds)\n    )\n\n    return pd.concat(results).mean()\n```\n:::\n\n\n::: {#62bb7ac3 .cell execution_count=18}\n``` {.python .cell-code}\nrun_power_simulation(n_iter=1000, rho=0.5, effect=0.4)\n```\n:::\n\n\nWe see that the \"unadjusted\" method detects the \"true effects\" at the highest frequency with on average 97% correctly detected effects. Does this mean that we should use uncorrected tests then? Well, maybe, but these do not control the family-wise error rate. While we have a better chance to detect a true effect, we also have a higher risk of flagging false positives. \n\nAdditionally, it looks as if the rwolf and wyoung methods detect the true positives at a slightly higher rate than the Bonferroni method. \n\nDo these findings generalize to other effect sizes? We can check this by simply imposing \ndifferent effects on the data generating process and repeating the previous exercise \nmultiple times. \n\n::: {#4f64e821 .cell execution_count=19}\n``` {.python .cell-code}\ndef run_power_simulation_vary_effect():\n    \"Run power simulations with varying effect sizes.\"\n    n_points = 10\n    max_val = 0.7\n\n    effects = (\n        np.sign(np.linspace(-1, 1, n_points))\n        * max_val\n        * (np.linspace(-1, 1, n_points) ** 2)\n    )\n\n    res_list = []\n    for effect_size in tqdm(effects):\n        res = run_power_simulation(n_iter=1000, rho=0.5, effect=effect_size)\n        res[\"effect\"] = effect_size\n        res_list.append(res)\n    return pd.concat(res_list, axis=1).T.set_index(\"effect\")\n\n\nres = run_power_simulation_vary_effect()\n```\n:::\n\n\n::: {#1b416072 .cell execution_count=20}\n``` {.python .cell-code}\ncolumn_to_label_dict = {\n    \"Pr(>|t|) detect effect\": \"Unadjusted\",\n    \"Bonferroni detect effect\": \"Bonferroni\",\n    \"rwolf detect effect\": \"RW\",\n    \"wyoung detect effect\": \"WY\",\n}\n\nplt.figure(figsize=(10, 6))\nline_styles = [\"-\", \"--\", \"-.\", \":\"]\nfor column, line_style in zip(res.columns, line_styles):\n    plt.plot(\n        res.index, res[column], linestyle=line_style, label=column_to_label_dict[column]\n    )\n\nplt.title(\"Power of Multiple Testing Correction Procedures\")\nplt.xlabel(\"Effect\")\nplt.ylabel(\"Proportion of correctly detected effects\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n:::\n\n\nWe see that for any simulated effect size, the Romano-Wolf and Westfall-Young methods detect a higher share of true positives than the Bonferroni method: they have **higher power**.\n\n## Literature \n\n- Clarke, Damian, Joseph P. Romano, and Michael Wolf. \"The Romano–Wolf multiple-hypothesis correction in Stata.\" The Stata Journal 20.4 (2020): 812-843.\n- Romano, Joseph P., and Michael Wolf. \"Stepwise multiple testing as formalized data snooping.\" Econometrica 73.4 (2005): 1237-1282.\n- Westfall, Peter H., and S. Stanley Young. Resampling-based multiple testing: Examples and methods for p-value adjustment. Vol. 279. John Wiley & Sons, 1993.\n\n",
    "supporting": [
      "multiple_testing_files"
    ],
    "filters": [],
    "includes": {}
  }
}